[I 2024-11-18 13:36:35,793] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2024-11-18 13:55:42,673] Trial 368 finished with value: 0.6061814259500057 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9492564136669568, 'batch_size': 37, 'attention_heads': 9, 'hidden_dimension': 94, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3389475562967513, 'global_pooling': 'max', 'learning_rate': 0.00025195366791465345, 'weight_decay': 0.00023813884550060046, 'beta_0': 0.8152437055052744, 'beta_1': 0.9831878817273227, 'epsilon': 2.1576576930776406e-06, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 15, 'plateau_patience': 15, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
The selected strides are greater or equal to the total chunk size.
[I 2024-11-18 13:55:44,700] Trial 369 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9563996950562059, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 138, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3301794797422353, 'global_pooling': 'max', 'learning_rate': 0.00033157083072868967, 'weight_decay': 0.00020307114292943875, 'beta_0': 0.8104911871788484, 'beta_1': 0.9821094518512538, 'epsilon': 5.971069126230565e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 850.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 400.69 MiB is free. Including non-PyTorch memory, this process has 44.16 GiB memory in use. Of the allocated memory 42.83 GiB is allocated by PyTorch, and 181.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 14:08:24,681] Trial 370 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9357928690784449, 'batch_size': 35, 'attention_heads': 10, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30571912405080887, 'global_pooling': 'max', 'learning_rate': 0.00019868423446608484, 'weight_decay': 0.0004377461612310235, 'beta_0': 0.8142689817968459, 'beta_1': 0.9962939172021922, 'epsilon': 3.1914768556465596e-06, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 14:27:45,348] Trial 371 finished with value: 0.587714813860941 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9528706848343377, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3452345031576256, 'global_pooling': 'max', 'learning_rate': 0.00015044915466337554, 'weight_decay': 0.00030326919627366, 'beta_0': 0.8207299024469651, 'beta_1': 0.9824831765138593, 'epsilon': 2.9358016252272886e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 15:04:22,618] Trial 372 finished with value: 0.035999604982989146 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9470749181864917, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30037549935440777, 'global_pooling': 'max', 'learning_rate': 0.04111834324236489, 'weight_decay': 0.0005295933275871348, 'beta_0': 0.8118532734941206, 'beta_1': 0.9971741613361419, 'epsilon': 4.387774848195961e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 996.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 986.69 MiB is free. Including non-PyTorch memory, this process has 43.59 GiB memory in use. Of the allocated memory 41.46 GiB is allocated by PyTorch, and 1001.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 15:17:06,669] Trial 373 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9408122461500235, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 152, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3211636013961671, 'global_pooling': 'max', 'learning_rate': 0.00028417970389602655, 'weight_decay': 0.00025307314180945554, 'beta_0': 0.8181245569911707, 'beta_1': 0.9817092138680069, 'epsilon': 9.979544896278016e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 15, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 15:43:12,208] Trial 374 finished with value: 0.6167569395029845 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9590992136717702, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 132, 'number_of_hidden_layers': 2, 'dropout_rate': 0.334206848979403, 'global_pooling': 'max', 'learning_rate': 0.00034335032782474464, 'weight_decay': 0.00021935597169969715, 'beta_0': 0.8161981615627225, 'beta_1': 0.9807558717470274, 'epsilon': 5.21352177957459e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 354.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 278.69 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 869.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 15:52:16,429] Trial 375 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8706722759958005, 'batch_size': 66, 'attention_heads': 9, 'hidden_dimension': 108, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31497986159225255, 'global_pooling': 'max', 'learning_rate': 0.0002441748347779474, 'weight_decay': 0.00036899197058838987, 'beta_0': 0.809185119926995, 'beta_1': 0.9827864384769507, 'epsilon': 1.3487677019658478e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 16:20:57,805] Trial 376 finished with value: 0.6087423976171806 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9508211963137135, 'batch_size': 32, 'attention_heads': 11, 'hidden_dimension': 128, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3301167091027371, 'global_pooling': 'max', 'learning_rate': 0.00017216866246745155, 'weight_decay': 0.0001773502365606873, 'beta_0': 0.813595489032022, 'beta_1': 0.9833021694134887, 'epsilon': 3.639752793548591e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 16:41:56,282] Trial 377 finished with value: 0.6228093658730826 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9628328909809517, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 145, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34213758296115815, 'global_pooling': 'max', 'learning_rate': 0.00020910990641540795, 'weight_decay': 0.00027578680965991705, 'beta_0': 0.8229614152811221, 'beta_1': 0.9968104973850247, 'epsilon': 7.811198010688377e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 2.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.57 GiB is free. Including non-PyTorch memory, this process has 42.98 GiB memory in use. Of the allocated memory 41.20 GiB is allocated by PyTorch, and 642.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 16:54:54,578] Trial 378 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9444144027496114, 'batch_size': 112, 'attention_heads': 9, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3257408541494132, 'global_pooling': 'max', 'learning_rate': 0.000371411247230952, 'weight_decay': 0.0003294216788944751, 'beta_0': 0.8198206945069215, 'beta_1': 0.9823245901562168, 'epsilon': 6.345264571590119e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 17:20:00,107] Trial 379 finished with value: 0.6206697957321924 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9564513433690026, 'batch_size': 37, 'attention_heads': 9, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33796763009605446, 'global_pooling': 'max', 'learning_rate': 0.0002875075567860342, 'weight_decay': 0.00022239761518881094, 'beta_0': 0.8155163813181291, 'beta_1': 0.9974228680840408, 'epsilon': 4.1922154029633445e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 17:38:56,963] Trial 380 finished with value: 0.06234215558843248 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9470186944421515, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 135, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30999696900917384, 'global_pooling': 'max', 'learning_rate': 0.01006190641752299, 'weight_decay': 0.0001959858741328004, 'beta_0': 0.8123178486861514, 'beta_1': 0.9955978275601864, 'epsilon': 5.3361838600975994e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 18:04:25,276] Trial 381 finished with value: 0.5877079656939271 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9529956750347726, 'batch_size': 39, 'attention_heads': 8, 'hidden_dimension': 124, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3231823694468206, 'global_pooling': 'max', 'learning_rate': 0.0001256625432104629, 'weight_decay': 0.00025534435958452656, 'beta_0': 0.8172278036192231, 'beta_1': 0.9828540722237884, 'epsilon': 3.211593349572036e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 16, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 2.85 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process has 43.28 GiB memory in use. Of the allocated memory 41.57 GiB is allocated by PyTorch, and 571.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 18:17:19,435] Trial 382 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9383347734957475, 'batch_size': 122, 'attention_heads': 9, 'hidden_dimension': 129, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3167024426440593, 'global_pooling': 'max', 'learning_rate': 0.0002547324640253245, 'weight_decay': 0.00030533368279089807, 'beta_0': 0.8064630152234596, 'beta_1': 0.9963580782274327, 'epsilon': 4.7363452027629137e-07, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 15, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 918.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 770.69 MiB is free. Including non-PyTorch memory, this process has 43.80 GiB memory in use. Of the allocated memory 40.70 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 18:35:20,916] Trial 383 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9419378525937911, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 139, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34821220933851077, 'global_pooling': 'max', 'learning_rate': 9.625091737112744e-05, 'weight_decay': 0.00041291929410801597, 'beta_0': 0.8143901739506831, 'beta_1': 0.9834671884071878, 'epsilon': 2.4998879143119784e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 18:54:46,272] Trial 384 finished with value: 0.6166162614844442 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9481105416491408, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33226613461221316, 'global_pooling': 'max', 'learning_rate': 0.0003475012605434811, 'weight_decay': 0.00015645811121665427, 'beta_0': 0.8247509518791912, 'beta_1': 0.9825228429343353, 'epsilon': 6.783345227067901e-05, 'balanced_loss': True, 'epochs': 71, 'early_stopping_patience': 14, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 19:22:28,881] Trial 385 finished with value: 0.5672715677622692 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9523284633543839, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 122, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33801134920351755, 'global_pooling': 'sum', 'learning_rate': 0.00041250884819763194, 'weight_decay': 0.00023612345141307766, 'beta_0': 0.8585746624742813, 'beta_1': 0.9818411869660832, 'epsilon': 4.4570997815686544e-05, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 19:41:24,962] Trial 386 finished with value: 0.6180109039109041 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9619034482654912, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30616531952354714, 'global_pooling': 'max', 'learning_rate': 0.00017882560494200003, 'weight_decay': 0.0003521975864415682, 'beta_0': 0.8106928921167674, 'beta_1': 0.9839035322357195, 'epsilon': 5.4905880659745497e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 20:01:16,044] Trial 387 finished with value: 0.6069262713729826 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9548731059842203, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31819804728686346, 'global_pooling': 'mean', 'learning_rate': 0.00031170241873292845, 'weight_decay': 0.0002827964707907694, 'beta_0': 0.8190719175228424, 'beta_1': 0.9831551983499656, 'epsilon': 3.785223373316384e-05, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 20:26:07,061] Trial 388 finished with value: 0.5927853528010234 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9488592537215809, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3272972357712358, 'global_pooling': 'max', 'learning_rate': 0.00022875980678248867, 'weight_decay': 0.00018935981710354742, 'beta_0': 0.8130789319714212, 'beta_1': 0.9967179912771463, 'epsilon': 7.769645856066922e-05, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.03 GiB. GPU 0 has a total capacity of 44.56 GiB of which 862.69 MiB is free. Including non-PyTorch memory, this process has 43.71 GiB memory in use. Of the allocated memory 41.51 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 20:39:07,161] Trial 389 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9437034366084468, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 127, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35882077320904593, 'global_pooling': 'max', 'learning_rate': 0.000415320534859672, 'weight_decay': 0.00022438201274466068, 'beta_0': 0.8166850528117038, 'beta_1': 0.9961795200525682, 'epsilon': 1.7494773642732355e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 20:54:57,139] Trial 390 finished with value: 0.5911000360994784 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9579582107784474, 'batch_size': 41, 'attention_heads': 8, 'hidden_dimension': 120, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3432021754244696, 'global_pooling': 'max', 'learning_rate': 0.00026585939732908057, 'weight_decay': 0.00012959754527390495, 'beta_0': 0.8086447115085971, 'beta_1': 0.9820052301523313, 'epsilon': 4.837645032421584e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.30 GiB. GPU 0 has a total capacity of 44.56 GiB of which 524.69 MiB is free. Including non-PyTorch memory, this process has 44.04 GiB memory in use. Of the allocated memory 42.35 GiB is allocated by PyTorch, and 551.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 21:12:53,623] Trial 391 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.950169278440866, 'batch_size': 35, 'attention_heads': 11, 'hidden_dimension': 137, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3127766212312086, 'global_pooling': 'max', 'learning_rate': 0.00034782210943021117, 'weight_decay': 0.0002686359680073411, 'beta_0': 0.8147421223711413, 'beta_1': 0.9928144520250517, 'epsilon': 5.9934805706943165e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 43.46 GiB memory in use. Of the allocated memory 42.06 GiB is allocated by PyTorch, and 251.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 21:25:59,083] Trial 392 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9462130927979657, 'batch_size': 57, 'attention_heads': 9, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33233108544066775, 'global_pooling': 'max', 'learning_rate': 0.00029361468344794384, 'weight_decay': 0.00017393475694476722, 'beta_0': 0.8189021205138997, 'beta_1': 0.9952705561447757, 'epsilon': 2.925576543574077e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
The selected strides are greater or equal to the total chunk size.
[I 2024-11-18 21:26:01,271] Trial 393 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.934680585845833, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 142, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35282767908969753, 'global_pooling': 'max', 'learning_rate': 0.00019784618065156042, 'weight_decay': 0.00031802019165690874, 'beta_0': 0.8123166110318883, 'beta_1': 0.9812940735825494, 'epsilon': 3.622049107831409e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 21:49:16,667] Trial 394 finished with value: 0.626496357843923 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9534874820368797, 'batch_size': 37, 'attention_heads': 10, 'hidden_dimension': 132, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3230625632830995, 'global_pooling': 'max', 'learning_rate': 0.00014657968180242117, 'weight_decay': 0.0002078280762721593, 'beta_0': 0.8214231555857014, 'beta_1': 0.9826990183100274, 'epsilon': 4.5144747253501664e-05, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 246.69 MiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 42.61 GiB is allocated by PyTorch, and 560.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-18 22:07:26,404] Trial 395 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9405754011346552, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30603073007827786, 'global_pooling': 'max', 'learning_rate': 0.00023359057344712287, 'weight_decay': 0.00039096150842846534, 'beta_0': 0.8160855608839771, 'beta_1': 0.984156950428665, 'epsilon': 6.463686833801476e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 13, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 22:25:43,766] Trial 396 finished with value: 0.6206765264667606 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9575991231473602, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3351540629771312, 'global_pooling': 'max', 'learning_rate': 0.0004203536958355501, 'weight_decay': 7.542761836390501e-05, 'beta_0': 0.8100973582459406, 'beta_1': 0.9975084721433894, 'epsilon': 5.507168282468182e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 22:45:02,990] Trial 397 finished with value: 0.6252452993498641 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9498306003975248, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 118, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3283530874017202, 'global_pooling': 'max', 'learning_rate': 0.0005126823572197711, 'weight_decay': 0.0002499198575156029, 'beta_0': 0.8140554747837028, 'beta_1': 0.995905752879333, 'epsilon': 3.9586761673865984e-05, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 23:08:58,884] Trial 398 finished with value: 0.6090089761051819 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9642672652288439, 'batch_size': 39, 'attention_heads': 9, 'hidden_dimension': 122, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31857326449913137, 'global_pooling': 'max', 'learning_rate': 0.00036041068249125093, 'weight_decay': 0.00047934804263805004, 'beta_0': 0.8176900960476772, 'beta_1': 0.9835055202807187, 'epsilon': 8.203515460787805e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 23:27:44,515] Trial 399 finished with value: 0.6165422604447098 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9454612866736152, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3472548696616617, 'global_pooling': 'max', 'learning_rate': 0.00029781195597507513, 'weight_decay': 0.00029112828103540814, 'beta_0': 0.8116063036655878, 'beta_1': 0.9970345108982723, 'epsilon': 3.168980964168112e-05, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 15, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-18 23:52:50,207] Trial 400 finished with value: 0.6029784418050828 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9538874627013207, 'batch_size': 35, 'attention_heads': 10, 'hidden_dimension': 110, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31127827258855345, 'global_pooling': 'max', 'learning_rate': 0.00018228577002408648, 'weight_decay': 0.0003469209647840625, 'beta_0': 0.8038930538307061, 'beta_1': 0.99808059436745, 'epsilon': 2.2940419693253587e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 00:13:53,993] Trial 401 finished with value: 0.6224533881754383 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.942031551474295, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 84, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30002318938978806, 'global_pooling': 'max', 'learning_rate': 0.0002450668768858004, 'weight_decay': 9.271393142661895e-06, 'beta_0': 0.81533313769181, 'beta_1': 0.983149623647338, 'epsilon': 4.7778229102733565e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 23, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 44.56 GiB of which 562.69 MiB is free. Including non-PyTorch memory, this process has 44.00 GiB memory in use. Of the allocated memory 42.48 GiB is allocated by PyTorch, and 381.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 00:33:52,790] Trial 402 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9500643960579124, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 136, 'number_of_hidden_layers': 2, 'dropout_rate': 0.339681268409099, 'global_pooling': 'max', 'learning_rate': 0.0003323766984791948, 'weight_decay': 0.00020999287450262466, 'beta_0': 0.8670792417999458, 'beta_1': 0.9822938388434296, 'epsilon': 6.498713039375968e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.38 GiB is free. Including non-PyTorch memory, this process has 43.17 GiB memory in use. Of the allocated memory 40.75 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 00:48:08,209] Trial 403 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9597172340081663, 'batch_size': 85, 'attention_heads': 9, 'hidden_dimension': 115, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3273397464989581, 'global_pooling': 'max', 'learning_rate': 0.0002123662366489506, 'weight_decay': 0.00015953551364106516, 'beta_0': 0.8073083432691457, 'beta_1': 0.996451838402424, 'epsilon': 5.423854554173813e-05, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 774.69 MiB is free. Including non-PyTorch memory, this process has 43.80 GiB memory in use. Of the allocated memory 42.06 GiB is allocated by PyTorch, and 603.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 01:05:54,769] Trial 404 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9370323636539838, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 146, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32073560647821064, 'global_pooling': 'max', 'learning_rate': 0.0004552690810852356, 'weight_decay': 0.00024574368019753315, 'beta_0': 0.8206001442864501, 'beta_1': 0.9828210878785394, 'epsilon': 4.0402937229958136e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 43.48 GiB memory in use. Of the allocated memory 41.17 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 01:19:02,778] Trial 405 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9463609253203049, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3365196009785211, 'global_pooling': 'max', 'learning_rate': 0.0002678382472418819, 'weight_decay': 0.0001882768235146761, 'beta_0': 0.8131942120302693, 'beta_1': 0.995649427695255, 'epsilon': 2.7437297872599452e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 13, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 01:36:50,015] Trial 406 finished with value: 0.6346544205901747 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9553938383611476, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34522242793019303, 'global_pooling': 'max', 'learning_rate': 0.0003703826694501259, 'weight_decay': 0.00029069872201146, 'beta_0': 0.8174241328420597, 'beta_1': 0.9820859495292772, 'epsilon': 1.1856292025827343e-06, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 01:54:43,184] Trial 407 finished with value: 0.629184514955459 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9532367393314628, 'batch_size': 41, 'attention_heads': 8, 'hidden_dimension': 98, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32932512322494467, 'global_pooling': 'mean', 'learning_rate': 0.0005791833486335821, 'weight_decay': 0.00030920643176963276, 'beta_0': 0.8173763000929329, 'beta_1': 0.9815631206097061, 'epsilon': 7.398582840866464e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 02:11:32,428] Trial 408 finished with value: 0.5440628527835788 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9488100431108267, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 91, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31128583736554394, 'global_pooling': 'sum', 'learning_rate': 0.00040702262674936356, 'weight_decay': 0.000381517541035914, 'beta_0': 0.8146380613054877, 'beta_1': 0.9822713110676041, 'epsilon': 1.0809112305823995e-06, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 02:31:23,648] Trial 409 finished with value: 0.5984649241457884 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9437047806954058, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 96, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5492784990046725, 'global_pooling': 'max', 'learning_rate': 0.00048344875658232233, 'weight_decay': 0.00027595436239688623, 'beta_0': 0.8105114247296361, 'beta_1': 0.9820380359617231, 'epsilon': 9.941219813601004e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 02:49:33,671] Trial 410 finished with value: 0.6430014903710013 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9543576794575239, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3677247490876787, 'global_pooling': 'max', 'learning_rate': 0.00034932465632193737, 'weight_decay': 0.0003216424365557218, 'beta_0': 0.8122106595223499, 'beta_1': 0.9816591148416164, 'epsilon': 4.697380101989803e-05, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 03:07:30,329] Trial 411 finished with value: 0.6108234655592318 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9570308417013792, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3594678008099521, 'global_pooling': 'max', 'learning_rate': 0.00035823090402467767, 'weight_decay': 0.00042452742836188753, 'beta_0': 0.8122138072885302, 'beta_1': 0.9811918214723211, 'epsilon': 4.885098756373261e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 03:25:01,875] Trial 412 finished with value: 0.6230904718008394 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9546288321710563, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 87, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37109527786012375, 'global_pooling': 'max', 'learning_rate': 0.0004025854841354173, 'weight_decay': 0.0003475541845817985, 'beta_0': 0.8084136639163354, 'beta_1': 0.9809173737045606, 'epsilon': 5.9817296336175217e-05, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 772.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 126.69 MiB is free. Including non-PyTorch memory, this process has 44.43 GiB memory in use. Of the allocated memory 42.72 GiB is allocated by PyTorch, and 570.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 03:38:03,636] Trial 413 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9163074506085216, 'batch_size': 36, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36443524957572904, 'global_pooling': 'max', 'learning_rate': 0.0005549065640805802, 'weight_decay': 0.00033016687920394463, 'beta_0': 0.8097349822834758, 'beta_1': 0.9815960146397916, 'epsilon': 7.06920240815006e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 03:56:51,703] Trial 414 finished with value: 0.6048952088905363 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9514512599937975, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3548778396778814, 'global_pooling': 'max', 'learning_rate': 0.0003415012457539016, 'weight_decay': 0.0002987512993056788, 'beta_0': 0.8132465070315481, 'beta_1': 0.9819321534484099, 'epsilon': 4.617420755680585e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 14, 'plateau_patience': 22, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1012.69 MiB is free. Including non-PyTorch memory, this process has 43.56 GiB memory in use. Of the allocated memory 41.85 GiB is allocated by PyTorch, and 575.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-19 04:09:54,426] Trial 415 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9229239399714665, 'batch_size': 38, 'attention_heads': 16, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3426984991705712, 'global_pooling': 'max', 'learning_rate': 0.00045514831226920084, 'weight_decay': 0.0003849372231225027, 'beta_0': 0.8113430638481679, 'beta_1': 0.981782789970354, 'epsilon': 3.4618913570892004e-05, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 04:26:41,264] Trial 416 finished with value: 0.6161333181942368 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9628848424875835, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 108, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33716782163375725, 'global_pooling': 'max', 'learning_rate': 0.0006520376238047413, 'weight_decay': 0.0005037896177695332, 'beta_0': 0.814989697972278, 'beta_1': 0.9823770595937225, 'epsilon': 5.317062035114266e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 7}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-19 04:46:00,834] Trial 417 finished with value: 0.6551138971056581 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.95592636265329, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 105, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3504601005437892, 'global_pooling': 'max', 'learning_rate': 0.000311337768264422, 'weight_decay': 0.00027074996369982315, 'beta_0': 0.8157489221497259, 'beta_1': 0.9813657356760784, 'epsilon': 7.966216247350535e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 417 with value: 0.6551138971056581.
[I 2024-11-19 05:05:00,920] Trial 418 finished with value: 0.629640378624284 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9571569885337311, 'batch_size': 42, 'attention_heads': 9, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3444159566817435, 'global_pooling': 'max', 'learning_rate': 0.0003231386399310263, 'weight_decay': 0.00028081896886720864, 'beta_0': 0.8134017893146421, 'beta_1': 0.9805544696581632, 'epsilon': 7.78098794012708e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 417 with value: 0.6551138971056581.
[I 2024-11-19 05:23:29,443] Trial 419 finished with value: 0.6152945913131264 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9650460098877486, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 93, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35193301066027133, 'global_pooling': 'max', 'learning_rate': 0.00015801200107752646, 'weight_decay': 0.00031973157027928627, 'beta_0': 0.8155897777434425, 'beta_1': 0.9800255512615439, 'epsilon': 1.015691081244186e-08, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 417 with value: 0.6551138971056581.
slurmstepd: error: *** JOB 13956100 ON gpu049 CANCELLED AT 2024-11-19T05:36:49 DUE TO TIME LIMIT ***
