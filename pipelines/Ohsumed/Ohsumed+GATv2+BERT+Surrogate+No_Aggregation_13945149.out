[I 2024-11-17 04:20:48,765] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2024-11-17 04:43:27,971] Trial 282 finished with value: 0.6524894676084485 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9499346691532357, 'batch_size': 37, 'attention_heads': 10, 'hidden_dimension': 108, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3396869026288477, 'global_pooling': 'max', 'learning_rate': 0.00030138594023748047, 'weight_decay': 0.00036252066442694485, 'beta_0': 0.8155594087299869, 'beta_1': 0.982906572010676, 'epsilon': 4.7566321998540284e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 23, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
The selected strides are greater or equal to the total chunk size.
[I 2024-11-17 04:43:29,993] Trial 283 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9466673235113483, 'batch_size': 37, 'attention_heads': 10, 'hidden_dimension': 107, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3427814667101094, 'global_pooling': 'max', 'learning_rate': 0.00028992911454962187, 'weight_decay': 0.00033769729875733466, 'beta_0': 0.8154974235135576, 'beta_1': 0.9832391549556014, 'epsilon': 8.504360552459564e-05, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 938.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 580.69 MiB is free. Including non-PyTorch memory, this process has 43.99 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 568.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 04:55:27,982] Trial 284 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9373301161168688, 'batch_size': 39, 'attention_heads': 10, 'hidden_dimension': 115, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3379479435351676, 'global_pooling': 'max', 'learning_rate': 0.00022613877898113014, 'weight_decay': 0.00042309192410266825, 'beta_0': 0.8135690544457357, 'beta_1': 0.9828453809364466, 'epsilon': 4.828567844745838e-05, 'balanced_loss': False, 'epochs': 76, 'early_stopping_patience': 23, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 05:13:11,383] Trial 285 finished with value: 0.6220003634836293 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9500795925900983, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34804523602842197, 'global_pooling': 'max', 'learning_rate': 0.00030299331326673096, 'weight_decay': 0.000393296376327779, 'beta_0': 0.8171318725349742, 'beta_1': 0.9822196606384092, 'epsilon': 6.208121414305826e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 44.56 GiB of which 532.69 MiB is free. Including non-PyTorch memory, this process has 44.03 GiB memory in use. Of the allocated memory 42.72 GiB is allocated by PyTorch, and 167.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 05:25:13,138] Trial 286 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9431069286515272, 'batch_size': 33, 'attention_heads': 11, 'hidden_dimension': 147, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33195890703972547, 'global_pooling': 'max', 'learning_rate': 0.00033445205813282554, 'weight_decay': 0.00025544408572403325, 'beta_0': 0.810412888810942, 'beta_1': 0.9825958155430089, 'epsilon': 4.376772898953103e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 05:44:41,320] Trial 287 finished with value: 0.6249019128047748 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9514051890446501, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 118, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3271131968113676, 'global_pooling': 'max', 'learning_rate': 0.0001943317450630054, 'weight_decay': 0.0003608632880902775, 'beta_0': 0.8182450569488953, 'beta_1': 0.9834481727013861, 'epsilon': 4.973079774404377e-05, 'balanced_loss': False, 'epochs': 77, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 06:06:36,932] Trial 288 finished with value: 0.6158901626596087 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9600632355368982, 'batch_size': 35, 'attention_heads': 7, 'hidden_dimension': 112, 'number_of_hidden_layers': 4, 'dropout_rate': 0.31702767802782794, 'global_pooling': 'max', 'learning_rate': 0.0002572191508732838, 'weight_decay': 0.0003064365169054689, 'beta_0': 0.8119946111343316, 'beta_1': 0.9829477052526439, 'epsilon': 6.174069616439335e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 24, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 06:21:49,205] Trial 289 finished with value: 0.5496949522317838 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9952896000096126, 'batch_size': 41, 'attention_heads': 10, 'hidden_dimension': 138, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3353381526473194, 'global_pooling': 'max', 'learning_rate': 0.0005183885785370513, 'weight_decay': 0.00022238302776140205, 'beta_0': 0.8150508373669622, 'beta_1': 0.9971331521056424, 'epsilon': 3.654593456924512e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 25, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 06:38:42,867] Trial 290 finished with value: 0.055682117273684544 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9482706438514593, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32294637269623766, 'global_pooling': 'sum', 'learning_rate': 0.014780080015112867, 'weight_decay': 0.0001827713782074461, 'beta_0': 0.81782096134958, 'beta_1': 0.9955848958308019, 'epsilon': 7.65707024753234e-05, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.56 GiB of which 458.69 MiB is free. Including non-PyTorch memory, this process has 44.11 GiB memory in use. Of the allocated memory 42.79 GiB is allocated by PyTorch, and 168.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 06:50:36,868] Trial 291 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9540462692728721, 'batch_size': 81, 'attention_heads': 9, 'hidden_dimension': 126, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31226899833407196, 'global_pooling': 'max', 'learning_rate': 0.0004010690824312869, 'weight_decay': 0.00028488904331445806, 'beta_0': 0.8137146185727162, 'beta_1': 0.9817872127587747, 'epsilon': 5.312462305317417e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 388.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 42.55 GiB is allocated by PyTorch, and 486.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 07:02:28,808] Trial 292 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9393593870780701, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 156, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3422821426886713, 'global_pooling': 'mean', 'learning_rate': 0.00032059591952606035, 'weight_decay': 0.0002365103680445638, 'beta_0': 0.8202448657954332, 'beta_1': 0.9963022917552363, 'epsilon': 4.232111851948753e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 936.69 MiB is free. Including non-PyTorch memory, this process has 43.64 GiB memory in use. Of the allocated memory 41.55 GiB is allocated by PyTorch, and 957.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 07:16:15,611] Trial 293 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9438472898110137, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3002301915238366, 'global_pooling': 'max', 'learning_rate': 0.00022265176763893645, 'weight_decay': 0.00020992892323334314, 'beta_0': 0.8091455962641465, 'beta_1': 0.9951072147418566, 'epsilon': 1.341196069606771e-06, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 07:34:17,604] Trial 294 finished with value: 0.5995067767747171 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.956716562159745, 'batch_size': 32, 'attention_heads': 10, 'hidden_dimension': 121, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3279133265517876, 'global_pooling': 'max', 'learning_rate': 0.0006513651454323875, 'weight_decay': 0.0003526231326641366, 'beta_0': 0.8154061343626268, 'beta_1': 0.9836610362544252, 'epsilon': 9.298793395062354e-07, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 16, 'plateau_patience': 23, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 07:51:32,039] Trial 295 finished with value: 0.6095089214632043 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9491078185498535, 'batch_size': 36, 'attention_heads': 7, 'hidden_dimension': 142, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31850282268177016, 'global_pooling': 'max', 'learning_rate': 0.0004619820885838519, 'weight_decay': 0.0002623845260163464, 'beta_0': 0.8551013458018368, 'beta_1': 0.9822264429314492, 'epsilon': 3.247083680314557e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 200.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 42.47 GiB is allocated by PyTorch, and 752.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 08:03:11,595] Trial 296 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9094468361727369, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33813330667707253, 'global_pooling': 'max', 'learning_rate': 0.00027483225587627863, 'weight_decay': 0.00047508140110763975, 'beta_0': 0.8120441685348521, 'beta_1': 0.9977841658458396, 'epsilon': 2.123286843112729e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.11 GiB. GPU 0 has a total capacity of 44.56 GiB of which 504.69 MiB is free. Including non-PyTorch memory, this process has 44.06 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 643.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 08:15:03,700] Trial 297 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.927765744512467, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.330715589519754, 'global_pooling': 'max', 'learning_rate': 0.00037224719206055027, 'weight_decay': 0.00016436785446683594, 'beta_0': 0.8176626309127661, 'beta_1': 0.9968333412428866, 'epsilon': 6.797693528015568e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 15, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 878.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 194.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 42.59 GiB is allocated by PyTorch, and 642.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 08:30:59,035] Trial 298 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9330429694585136, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3071496159541495, 'global_pooling': 'max', 'learning_rate': 0.005094193855197498, 'weight_decay': 0.0003067507849212503, 'beta_0': 0.8229111268859922, 'beta_1': 0.9829009506142886, 'epsilon': 5.237969362811879e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 08:48:07,450] Trial 299 finished with value: 0.6228807474152898 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9538186841545203, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 112, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3243465900889605, 'global_pooling': 'max', 'learning_rate': 0.0003166436741763138, 'weight_decay': 2.780192435164988e-05, 'beta_0': 0.8025971403729882, 'beta_1': 0.9984732932319497, 'epsilon': 4.1956143214028184e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 09:05:48,237] Trial 300 finished with value: 0.620716328746451 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9615993964862196, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 136, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3516716827492246, 'global_pooling': 'max', 'learning_rate': 0.000530726223627717, 'weight_decay': 0.00040008753645863655, 'beta_0': 0.8157830851900119, 'beta_1': 0.9957870319354546, 'epsilon': 3.513260472241391e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 14, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 44.56 GiB of which 970.69 MiB is free. Including non-PyTorch memory, this process has 43.61 GiB memory in use. Of the allocated memory 41.75 GiB is allocated by PyTorch, and 724.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 09:23:34,300] Trial 301 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9458046447125943, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 119, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31582734255112443, 'global_pooling': 'max', 'learning_rate': 0.0003990977397918671, 'weight_decay': 1.7303902799105686e-05, 'beta_0': 0.8137953962625156, 'beta_1': 0.9814264841403482, 'epsilon': 9.153932213860669e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 72.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 42.41 GiB is allocated by PyTorch, and 940.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 09:35:31,741] Trial 302 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9416350413517317, 'batch_size': 35, 'attention_heads': 10, 'hidden_dimension': 145, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48569445698397157, 'global_pooling': 'max', 'learning_rate': 0.00024142089962372686, 'weight_decay': 0.0001984376377217137, 'beta_0': 0.8192925071271705, 'beta_1': 0.9832163153229814, 'epsilon': 5.74083314599978e-05, 'balanced_loss': True, 'epochs': 56, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 10:00:57,497] Trial 303 finished with value: 0.6100031433322377 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9504671714016577, 'batch_size': 33, 'attention_heads': 9, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33618883418963874, 'global_pooling': 'max', 'learning_rate': 0.00019256184451411432, 'weight_decay': 0.0002403940112956683, 'beta_0': 0.8108658922103841, 'beta_1': 0.996447965570452, 'epsilon': 2.7987080297847766e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 10:17:45,215] Trial 304 finished with value: 0.6064551546950891 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9576241480109042, 'batch_size': 39, 'attention_heads': 8, 'hidden_dimension': 91, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31242149460648466, 'global_pooling': 'max', 'learning_rate': 0.00016691538736496203, 'weight_decay': 0.0002786034033153788, 'beta_0': 0.806810933887962, 'beta_1': 0.9825796519231039, 'epsilon': 4.8136961883713706e-05, 'balanced_loss': False, 'epochs': 83, 'early_stopping_patience': 15, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 10:35:12,086] Trial 305 finished with value: 0.62884326336353 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9462991693958188, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3224115675418783, 'global_pooling': 'max', 'learning_rate': 0.00044624492462124924, 'weight_decay': 5.821411173559055e-05, 'beta_0': 0.8167783507898189, 'beta_1': 0.997314181299593, 'epsilon': 1.4762441870795995e-08, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 10:58:15,491] Trial 306 finished with value: 0.6397758061479456 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9517693775282261, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32935308268569985, 'global_pooling': 'max', 'learning_rate': 0.0002853268937616526, 'weight_decay': 0.0003214791409645257, 'beta_0': 0.8132753194127832, 'beta_1': 0.9922288214085544, 'epsilon': 7.304866253005002e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 43.00 GiB memory in use. Of the allocated memory 41.02 GiB is allocated by PyTorch, and 847.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 11:14:01,863] Trial 307 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9490143347764199, 'batch_size': 105, 'attention_heads': 8, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34521254617668834, 'global_pooling': 'max', 'learning_rate': 0.00027050599469633483, 'weight_decay': 0.0003642555032740601, 'beta_0': 0.81167653648578, 'beta_1': 0.9945076128813323, 'epsilon': 7.439543804800345e-05, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1020.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 942.69 MiB is free. Including non-PyTorch memory, this process has 43.63 GiB memory in use. Of the allocated memory 41.97 GiB is allocated by PyTorch, and 525.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 11:31:31,491] Trial 308 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9401248747691267, 'batch_size': 37, 'attention_heads': 7, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33165505244788257, 'global_pooling': 'max', 'learning_rate': 0.00036557648903492616, 'weight_decay': 0.00033938093348579877, 'beta_0': 0.8130339278224917, 'beta_1': 0.9837512577789617, 'epsilon': 9.540162794353546e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 11:54:16,245] Trial 309 finished with value: 0.5903133604914559 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9540597041801928, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33958275534304366, 'global_pooling': 'max', 'learning_rate': 0.0002157088493881564, 'weight_decay': 0.0004184263498973313, 'beta_0': 0.8086776477498803, 'beta_1': 0.9934944700019288, 'epsilon': 6.554670716986508e-05, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 17, 'plateau_patience': 21, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.10 GiB. GPU 0 has a total capacity of 44.56 GiB of which 576.69 MiB is free. Including non-PyTorch memory, this process has 43.99 GiB memory in use. Of the allocated memory 42.46 GiB is allocated by PyTorch, and 389.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 12:10:00,187] Trial 310 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9440800594778535, 'batch_size': 40, 'attention_heads': 11, 'hidden_dimension': 139, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3296426665441747, 'global_pooling': 'max', 'learning_rate': 0.0002801309343752812, 'weight_decay': 0.0002849112581290099, 'beta_0': 0.8142629011351066, 'beta_1': 0.993554467188592, 'epsilon': 6.856035502213603e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 12:30:50,469] Trial 311 finished with value: 0.6033949589431602 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9606414658821139, 'batch_size': 62, 'attention_heads': 8, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30968794298763286, 'global_pooling': 'max', 'learning_rate': 0.0005789843236369413, 'weight_decay': 0.0003234235733441504, 'beta_0': 0.8779119637690382, 'beta_1': 0.9819379561271261, 'epsilon': 5.433437515026879e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 12:52:58,617] Trial 312 finished with value: 0.6258547736331859 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9499118152967386, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 121, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31860179839768893, 'global_pooling': 'max', 'learning_rate': 0.0004589844734698646, 'weight_decay': 0.00017734731673804457, 'beta_0': 0.8154463390013419, 'beta_1': 0.9949857674568372, 'epsilon': 3.0809805312178946e-07, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 13:13:12,607] Trial 313 finished with value: 0.5522622102046932 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9572375435227128, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3347019622984738, 'global_pooling': 'sum', 'learning_rate': 0.0003425260759147751, 'weight_decay': 0.00014502438379000897, 'beta_0': 0.8094083579663766, 'beta_1': 0.9923067853730041, 'epsilon': 4.001214489957622e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 13:35:25,784] Trial 314 finished with value: 0.6157635270535211 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9459761263475256, 'batch_size': 36, 'attention_heads': 7, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30634960119660853, 'global_pooling': 'max', 'learning_rate': 0.0003920748888491823, 'weight_decay': 0.0002182090645114998, 'beta_0': 0.81162295030534, 'beta_1': 0.9960932938145826, 'epsilon': 8.55412309447355e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 230.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 154.69 MiB is free. Including non-PyTorch memory, this process has 44.40 GiB memory in use. Of the allocated memory 42.62 GiB is allocated by PyTorch, and 649.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 13:44:47,171] Trial 315 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8385499536004608, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 127, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30000008323337735, 'global_pooling': 'max', 'learning_rate': 0.0002445134703470962, 'weight_decay': 0.00011536754977029062, 'beta_0': 0.8133878137354008, 'beta_1': 0.9810966067936341, 'epsilon': 5.867078518544573e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 888.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 182.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 42.63 GiB is allocated by PyTorch, and 610.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 14:00:45,481] Trial 316 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9373821497757124, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 148, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4677380179181432, 'global_pooling': 'mean', 'learning_rate': 0.00032625267833203336, 'weight_decay': 0.00027176586485986735, 'beta_0': 0.8207698396191785, 'beta_1': 0.9919409803126388, 'epsilon': 3.511290979010939e-05, 'balanced_loss': False, 'epochs': 81, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 14:18:36,774] Trial 317 finished with value: 0.5798620469939788 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.953830360329333, 'batch_size': 37, 'attention_heads': 9, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32664316679676936, 'global_pooling': 'max', 'learning_rate': 0.0004894977083316449, 'weight_decay': 6.137083477651089e-06, 'beta_0': 0.8164802438522643, 'beta_1': 0.9929925028897895, 'epsilon': 7.630560107798104e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 616.69 MiB is free. Including non-PyTorch memory, this process has 43.95 GiB memory in use. Of the allocated memory 42.35 GiB is allocated by PyTorch, and 465.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 14:34:20,301] Trial 318 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9423202787794426, 'batch_size': 42, 'attention_heads': 9, 'hidden_dimension': 141, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3435546164703104, 'global_pooling': 'max', 'learning_rate': 0.0006679157749586618, 'weight_decay': 0.00031184260512129834, 'beta_0': 0.8149101617171666, 'beta_1': 0.9953632560408802, 'epsilon': 4.146033956342322e-05, 'balanced_loss': False, 'epochs': 77, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 14:52:28,177] Trial 319 finished with value: 0.621038247412324 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9488705555741385, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.321367871632491, 'global_pooling': 'max', 'learning_rate': 0.0002837775445259477, 'weight_decay': 0.0004704051901483804, 'beta_0': 0.8106683838589368, 'beta_1': 0.9830317036901534, 'epsilon': 4.825194371938698e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 15:11:40,126] Trial 320 finished with value: 0.6137654295041247 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9526061676013924, 'batch_size': 39, 'attention_heads': 10, 'hidden_dimension': 124, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33393914049461315, 'global_pooling': 'max', 'learning_rate': 0.0004234270946981765, 'weight_decay': 0.00019187800139284348, 'beta_0': 0.8181284329957568, 'beta_1': 0.9824484394909283, 'epsilon': 2.8931131220522127e-05, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 15:34:15,830] Trial 321 finished with value: 0.5843259959217856 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9628804471218573, 'batch_size': 36, 'attention_heads': 8, 'hidden_dimension': 136, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3145631814115493, 'global_pooling': 'max', 'learning_rate': 0.00015882046872110132, 'weight_decay': 0.0003671054181906592, 'beta_0': 0.8051990470504661, 'beta_1': 0.9910244595676347, 'epsilon': 6.317576051984955e-05, 'balanced_loss': True, 'epochs': 55, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 15:53:59,938] Trial 322 finished with value: 0.636967113408028 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9464926629266643, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 129, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32823653074061215, 'global_pooling': 'max', 'learning_rate': 0.00021626783654942172, 'weight_decay': 0.0002484656218377417, 'beta_0': 0.8129192802569298, 'beta_1': 0.9835091667840239, 'epsilon': 5.243507892123062e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 16:14:01,028] Trial 323 finished with value: 0.646620417599793 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.957614441880608, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33870035341953847, 'global_pooling': 'max', 'learning_rate': 0.00019906184182993974, 'weight_decay': 0.0002447203228884001, 'beta_0': 0.8130105434809499, 'beta_1': 0.9838588307489498, 'epsilon': 5.644203082078201e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 16:33:12,697] Trial 324 finished with value: 0.6393822473565364 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9568974728301307, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 129, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33817151945044155, 'global_pooling': 'max', 'learning_rate': 0.00018451303824420357, 'weight_decay': 0.00024909436130196274, 'beta_0': 0.8131146400601776, 'beta_1': 0.9838072577814734, 'epsilon': 7.72818939901619e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 16:51:51,449] Trial 325 finished with value: 0.6133177669380364 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9601118005115326, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 129, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34141530592940506, 'global_pooling': 'max', 'learning_rate': 0.00012263290606180744, 'weight_decay': 0.00025052000987697294, 'beta_0': 0.813817402738876, 'beta_1': 0.9836257101242306, 'epsilon': 9.602869325287889e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 17:10:26,726] Trial 326 finished with value: 0.6007451205907397 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9569073976224997, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 121, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35377204008179897, 'global_pooling': 'max', 'learning_rate': 0.0001599820541927889, 'weight_decay': 0.00022057370617383485, 'beta_0': 0.8124782511460351, 'beta_1': 0.9832730733187424, 'epsilon': 7.879060691188397e-05, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 17:28:33,419] Trial 327 finished with value: 0.635119848799988 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9640310346148008, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 126, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34875084922957805, 'global_pooling': 'max', 'learning_rate': 0.00019361237022554783, 'weight_decay': 0.00025514793392367576, 'beta_0': 0.8083284374744182, 'beta_1': 0.9840097919020755, 'epsilon': 7.398076091904032e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 17:44:27,654] Trial 328 finished with value: 0.5909806793203398 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9641905426304009, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 62, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34025737218418206, 'global_pooling': 'max', 'learning_rate': 0.00014221719249428807, 'weight_decay': 0.0002465753165086245, 'beta_0': 0.8081011420778862, 'beta_1': 0.983821572731282, 'epsilon': 6.807031854469078e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 18:02:48,612] Trial 329 finished with value: 0.6188243054848924 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9566499994143619, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 126, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3568703756192417, 'global_pooling': 'max', 'learning_rate': 0.0001868108521127312, 'weight_decay': 0.00021211936784788855, 'beta_0': 0.8126840167576362, 'beta_1': 0.9839350428395707, 'epsilon': 6.0342262419177255e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 18:21:22,766] Trial 330 finished with value: 0.6253100456373096 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.968189731389698, 'batch_size': 33, 'attention_heads': 10, 'hidden_dimension': 129, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33888626374864594, 'global_pooling': 'max', 'learning_rate': 0.0001914533113329527, 'weight_decay': 0.0002594949925430397, 'beta_0': 0.8103065151318921, 'beta_1': 0.9819515921161548, 'epsilon': 8.675961456469415e-05, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 18:40:40,143] Trial 331 finished with value: 0.612304444760175 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9516052394483445, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 122, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3485908350635013, 'global_pooling': 'max', 'learning_rate': 0.00017242409821194936, 'weight_decay': 0.00029837120259086685, 'beta_0': 0.8151449547132418, 'beta_1': 0.9834737049949427, 'epsilon': 7.480133617580244e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 460.69 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 42.33 GiB is allocated by PyTorch, and 642.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 18:55:54,690] Trial 332 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9428891263983801, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3289944515981507, 'global_pooling': 'max', 'learning_rate': 0.0002175667098249743, 'weight_decay': 0.00021680360927021835, 'beta_0': 0.8635251639007797, 'beta_1': 0.9826462432799504, 'epsilon': 5.6976673441879395e-05, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 19:16:34,685] Trial 333 finished with value: 0.6427010166214214 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9545417833927647, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3474343294487913, 'global_pooling': 'max', 'learning_rate': 0.00020761261868965062, 'weight_decay': 0.00024147088791844738, 'beta_0': 0.813106536166405, 'beta_1': 0.9805934549111307, 'epsilon': 5.69700150175326e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 17, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 19:36:09,935] Trial 334 finished with value: 0.6215786229608345 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9533839679579197, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 132, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35910210134853504, 'global_pooling': 'max', 'learning_rate': 0.00015890876108713204, 'weight_decay': 0.00019977651076369992, 'beta_0': 0.8140046091803591, 'beta_1': 0.9802665990934958, 'epsilon': 5.2757046826965906e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 778.69 MiB is free. Including non-PyTorch memory, this process has 43.79 GiB memory in use. Of the allocated memory 41.78 GiB is allocated by PyTorch, and 879.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 19:48:19,335] Trial 335 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9351305888253946, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 126, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3492748204446001, 'global_pooling': 'max', 'learning_rate': 1.7549203848008e-05, 'weight_decay': 0.00023928356218072197, 'beta_0': 0.8948458802248084, 'beta_1': 0.9804013251753383, 'epsilon': 3.822319422766788e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
[I 2024-11-17 20:11:45,834] Trial 336 finished with value: 0.6223707796982544 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9505326021083146, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 132, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34677109893406993, 'global_pooling': 'max', 'learning_rate': 0.00013096956634720436, 'weight_decay': 0.00017846888815837664, 'beta_0': 0.8174764484479451, 'beta_1': 0.9808442101972673, 'epsilon': 5.4650285094180224e-05, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 17, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 282 with value: 0.6524894676084485.
slurmstepd: error: *** JOB 13945149 ON gpu006 CANCELLED AT 2024-11-17T20:20:37 DUE TO TIME LIMIT ***
