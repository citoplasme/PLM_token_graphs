[I 2024-11-16 07:18:58,275] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 41.11 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 07:37:15,701] Trial 235 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9496696549329058, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 142, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32913663522244285, 'global_pooling': 'max', 'learning_rate': 0.00026229876280500623, 'weight_decay': 0.0002888861919085765, 'beta_0': 0.8159542424571755, 'beta_1': 0.995734069957917, 'epsilon': 7.232574649085143e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 880.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 41.36 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 07:53:25,677] Trial 236 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9402428162660018, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 134, 'number_of_hidden_layers': 2, 'dropout_rate': 0.321050308232927, 'global_pooling': 'max', 'learning_rate': 0.0003336616364033303, 'weight_decay': 0.0001705575688071788, 'beta_0': 0.8215790869684946, 'beta_1': 0.99802064247642, 'epsilon': 5.894335758441481e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 08:16:31,878] Trial 237 finished with value: 0.6359246544057161 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.945491827248964, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3351969655803635, 'global_pooling': 'max', 'learning_rate': 0.0004666151639888544, 'weight_decay': 0.00031984805761643145, 'beta_0': 0.8140502216614226, 'beta_1': 0.9967444326689713, 'epsilon': 2.7283119947304354e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 08:39:25,914] Trial 238 finished with value: 0.6343428247775528 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9448922266363928, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33542170993568166, 'global_pooling': 'max', 'learning_rate': 0.0004429469248009789, 'weight_decay': 0.0003240312170207324, 'beta_0': 0.8139621243048768, 'beta_1': 0.9964443815271501, 'epsilon': 2.957447257253685e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 09:02:13,931] Trial 239 finished with value: 0.5896505042263884 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.944521733642161, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 113, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30045845456361486, 'global_pooling': 'max', 'learning_rate': 0.00045775552375618985, 'weight_decay': 0.00024479048492337245, 'beta_0': 0.8138238374344975, 'beta_1': 0.9964301458298419, 'epsilon': 2.6530824956690777e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 09:24:06,198] Trial 240 finished with value: 0.6082721898700584 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9539533205048321, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33592171054466613, 'global_pooling': 'max', 'learning_rate': 0.0005235853141006485, 'weight_decay': 0.0003364745792891543, 'beta_0': 0.8047185082327808, 'beta_1': 0.9970365079628811, 'epsilon': 2.9985551969276165e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 918.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 872.69 MiB is free. Including non-PyTorch memory, this process has 43.70 GiB memory in use. Of the allocated memory 42.06 GiB is allocated by PyTorch, and 497.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 09:42:33,491] Trial 241 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9386686092502842, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 107, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34340033489045535, 'global_pooling': 'max', 'learning_rate': 0.0003085899266599748, 'weight_decay': 0.00021720810960542287, 'beta_0': 0.8125626715292655, 'beta_1': 0.9971948671942678, 'epsilon': 2.4047151128020036e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 10:06:01,161] Trial 242 finished with value: 0.5894183749827612 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9513057511042101, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33315624579946607, 'global_pooling': 'max', 'learning_rate': 0.0005944066804644072, 'weight_decay': 0.000315108549983888, 'beta_0': 0.8152498977259542, 'beta_1': 0.9976157224331194, 'epsilon': 4.628537181307636e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 10:29:44,007] Trial 243 finished with value: 0.6250766380150826 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9572292437346053, 'batch_size': 37, 'attention_heads': 11, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.340372211151309, 'global_pooling': 'max', 'learning_rate': 0.0004287881003899161, 'weight_decay': 0.00025884672138532265, 'beta_0': 0.809691874687034, 'beta_1': 0.9970164600484887, 'epsilon': 3.459560606650616e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 10:51:27,361] Trial 244 finished with value: 0.6191302505158462 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9454064265399055, 'batch_size': 34, 'attention_heads': 7, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31568687505067516, 'global_pooling': 'max', 'learning_rate': 0.00023917753107015712, 'weight_decay': 0.00019513934991596222, 'beta_0': 0.8166299493198642, 'beta_1': 0.9965253558027786, 'epsilon': 5.073370003525923e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 44.56 GiB of which 914.69 MiB is free. Including non-PyTorch memory, this process has 43.66 GiB memory in use. Of the allocated memory 41.97 GiB is allocated by PyTorch, and 549.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 11:08:37,943] Trial 245 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9419064543530764, 'batch_size': 36, 'attention_heads': 7, 'hidden_dimension': 139, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30801779353768766, 'global_pooling': 'max', 'learning_rate': 0.0002856489708074379, 'weight_decay': 0.0003271864902007553, 'beta_0': 0.8112333691176197, 'beta_1': 0.9914986631466456, 'epsilon': 3.6283802509590375e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 11:31:32,506] Trial 246 finished with value: 0.591760542614545 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9479990932981676, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 112, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3326221430932212, 'global_pooling': 'max', 'learning_rate': 0.0004018223664904878, 'weight_decay': 0.00023444212309649916, 'beta_0': 0.8147527425374742, 'beta_1': 0.9980132502857683, 'epsilon': 4.283954679205211e-05, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 11:56:23,596] Trial 247 finished with value: 0.6253805061773243 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9563199086997375, 'batch_size': 39, 'attention_heads': 10, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3203736894886374, 'global_pooling': 'max', 'learning_rate': 0.0003329703472503361, 'weight_decay': 0.00015164460898281046, 'beta_0': 0.813187668592929, 'beta_1': 0.9968200645903651, 'epsilon': 5.9097519945683154e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 12:20:15,791] Trial 248 finished with value: 0.6150159925009733 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9490251188928908, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32729699509432425, 'global_pooling': 'max', 'learning_rate': 0.00038188255028418644, 'weight_decay': 0.0002728996715366576, 'beta_0': 0.8187999144679929, 'beta_1': 0.9960792426114002, 'epsilon': 4.03085160322201e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 12:44:55,243] Trial 249 finished with value: 0.6141452265804859 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9462477705464819, 'batch_size': 33, 'attention_heads': 9, 'hidden_dimension': 127, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3262738380891016, 'global_pooling': 'max', 'learning_rate': 0.000466012644856401, 'weight_decay': 0.0002929639737447475, 'beta_0': 0.8173770454748476, 'beta_1': 0.9958911921070676, 'epsilon': 3.164466836563012e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 13:08:37,223] Trial 250 finished with value: 0.6104152084382801 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9515484725311494, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33668847842463745, 'global_pooling': 'max', 'learning_rate': 0.0003547428554976426, 'weight_decay': 0.0002822648596280867, 'beta_0': 0.8160790781167088, 'beta_1': 0.9966554173377796, 'epsilon': 4.734413949680411e-05, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 13:32:21,681] Trial 251 finished with value: 0.6333879444639723 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9499765599150709, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 135, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3145478388943708, 'global_pooling': 'max', 'learning_rate': 0.00028602553404679125, 'weight_decay': 0.00022543655275822187, 'beta_0': 0.820158287625657, 'beta_1': 0.9950500452270935, 'epsilon': 3.897433369363197e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 13:56:22,204] Trial 252 finished with value: 0.622919157540288 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9528200399095859, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 135, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3140469086383298, 'global_pooling': 'max', 'learning_rate': 0.00022702852086138492, 'weight_decay': 0.000178633926838233, 'beta_0': 0.8083368621657304, 'beta_1': 0.9828135942881768, 'epsilon': 2.850260465361158e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 14:20:02,417] Trial 253 finished with value: 0.6201213777042266 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9589888094984924, 'batch_size': 36, 'attention_heads': 8, 'hidden_dimension': 142, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3085255528756002, 'global_pooling': 'max', 'learning_rate': 0.00028836281900679935, 'weight_decay': 0.00021750402187904955, 'beta_0': 0.8142740803735045, 'beta_1': 0.9951004192556145, 'epsilon': 5.464289772038703e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
The selected strides are greater or equal to the total chunk size.
[I 2024-11-16 14:20:04,390] Trial 254 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.942966932851399, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 135, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3193162547442379, 'global_pooling': 'max', 'learning_rate': 0.00019336146969624763, 'weight_decay': 0.00019488748543812422, 'beta_0': 0.8206988222075512, 'beta_1': 0.9961832084455746, 'epsilon': 7.224674054121846e-05, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 14:42:49,344] Trial 255 finished with value: 0.6227370935265969 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.954013884259771, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 128, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31204509221021576, 'global_pooling': 'max', 'learning_rate': 0.0002779380394851446, 'weight_decay': 0.0002409777976744507, 'beta_0': 0.8003115648978703, 'beta_1': 0.9934754690853582, 'epsilon': 3.6338225137106925e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 15:05:36,436] Trial 256 finished with value: 0.6025524801249249 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9481640253228172, 'batch_size': 32, 'attention_heads': 7, 'hidden_dimension': 131, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34083831833271666, 'global_pooling': 'max', 'learning_rate': 0.0005962021137805553, 'weight_decay': 0.00026085318646507425, 'beta_0': 0.8115635987216459, 'beta_1': 0.9821684100773957, 'epsilon': 4.676162261258377e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 928.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 508.69 MiB is free. Including non-PyTorch memory, this process has 44.06 GiB memory in use. Of the allocated memory 42.22 GiB is allocated by PyTorch, and 704.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 15:17:57,971] Trial 257 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9393322237823138, 'batch_size': 35, 'attention_heads': 9, 'hidden_dimension': 149, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30108324560611865, 'global_pooling': 'max', 'learning_rate': 0.00046374110965161173, 'weight_decay': 0.0003197409997679189, 'beta_0': 0.8168409672992765, 'beta_1': 0.9829476512834644, 'epsilon': 6.29040047851256e-05, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 15:41:16,741] Trial 258 finished with value: 0.6370432025031202 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9495799261211632, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32538778644458843, 'global_pooling': 'max', 'learning_rate': 0.00036672745177339133, 'weight_decay': 0.00022772032856096323, 'beta_0': 0.8185968281340404, 'beta_1': 0.9958789801865442, 'epsilon': 3.955636850854906e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 16:05:07,643] Trial 259 finished with value: 0.6234940600092043 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9449715826307176, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 118, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33026084252962545, 'global_pooling': 'max', 'learning_rate': 0.00032728060615085027, 'weight_decay': 0.0002284944061936864, 'beta_0': 0.8188477102919812, 'beta_1': 0.994985148398215, 'epsilon': 3.9439531763774995e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 16:29:19,674] Trial 260 finished with value: 0.6327014958613406 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9493662807354731, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32486647961427084, 'global_pooling': 'max', 'learning_rate': 0.00038240772749433157, 'weight_decay': 0.00020132762253720745, 'beta_0': 0.8137666546423628, 'beta_1': 0.9938737804977855, 'epsilon': 5.037959397858225e-05, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 16:52:25,500] Trial 261 finished with value: 0.6110717566534829 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9500594457447183, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32267213972212455, 'global_pooling': 'max', 'learning_rate': 0.0003780660208490916, 'weight_decay': 0.00017682524524510436, 'beta_0': 0.8142362650724221, 'beta_1': 0.9954430197528592, 'epsilon': 5.249634730455393e-05, 'balanced_loss': True, 'epochs': 55, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 17:15:55,776] Trial 262 finished with value: 0.6098697904607969 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9460082668598467, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 119, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3249530992058604, 'global_pooling': 'max', 'learning_rate': 0.0004336243191163533, 'weight_decay': 0.00020590383240287743, 'beta_0': 0.8165582819062118, 'beta_1': 0.996380517772178, 'epsilon': 4.70058793695276e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 17:41:28,260] Trial 263 finished with value: 0.6192287429477537 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9505843647936939, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3324946515091285, 'global_pooling': 'max', 'learning_rate': 0.0003278987921838649, 'weight_decay': 0.00015720219692820988, 'beta_0': 0.8126048193344769, 'beta_1': 0.9959296213238525, 'epsilon': 5.911443494471543e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 18:03:56,178] Trial 264 finished with value: 0.6126156026579669 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9537564638603278, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 138, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3176129408589989, 'global_pooling': 'max', 'learning_rate': 0.0005165573166464689, 'weight_decay': 0.0002545205988122123, 'beta_0': 0.8154097331227481, 'beta_1': 0.981563332006159, 'epsilon': 3.237022252741555e-05, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 44.56 GiB of which 716.69 MiB is free. Including non-PyTorch memory, this process has 43.85 GiB memory in use. Of the allocated memory 42.25 GiB is allocated by PyTorch, and 459.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 18:22:16,486] Trial 265 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9443125883779903, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 124, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33556279613599294, 'global_pooling': 'max', 'learning_rate': 0.0003968227543331964, 'weight_decay': 0.0001878765289308651, 'beta_0': 0.8222479074617387, 'beta_1': 0.9940227558432512, 'epsilon': 7.169885661222473e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 1004.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 608.69 MiB is free. Including non-PyTorch memory, this process has 43.96 GiB memory in use. Of the allocated memory 41.96 GiB is allocated by PyTorch, and 870.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 18:39:14,583] Trial 266 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9420003120782613, 'batch_size': 36, 'attention_heads': 12, 'hidden_dimension': 115, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32624804273094565, 'global_pooling': 'max', 'learning_rate': 0.00024400697775898912, 'weight_decay': 3.5167747122182647e-05, 'beta_0': 0.8100255938552635, 'beta_1': 0.9955373583266053, 'epsilon': 4.418932057461871e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 19:02:31,738] Trial 267 finished with value: 0.6089893598363618 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9493316436310775, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 131, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3138413914154808, 'global_pooling': 'max', 'learning_rate': 0.00029837535655946194, 'weight_decay': 0.0003533405938676115, 'beta_0': 0.8179870521366233, 'beta_1': 0.9966586410702278, 'epsilon': 3.81285646848099e-06, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 262.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 106.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 42.33 GiB is allocated by PyTorch, and 987.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 19:16:07,816] Trial 268 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8913554907531094, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 144, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3211616059128075, 'global_pooling': 'sum', 'learning_rate': 0.0004275302100001571, 'weight_decay': 0.0002039534127583351, 'beta_0': 0.8202568120261461, 'beta_1': 0.9823261320348072, 'epsilon': 5.356415478843588e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 19:38:33,319] Trial 269 finished with value: 0.5763726043741987 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9554220641630162, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 107, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3303653315940161, 'global_pooling': 'mean', 'learning_rate': 0.00034848505258839723, 'weight_decay': 0.000274432739169283, 'beta_0': 0.8143641446624569, 'beta_1': 0.9976886624012312, 'epsilon': 2.4617896551971398e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 804.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 572.69 MiB is free. Including non-PyTorch memory, this process has 43.99 GiB memory in use. Of the allocated memory 42.25 GiB is allocated by PyTorch, and 603.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 19:54:56,900] Trial 270 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9359043453688162, 'batch_size': 36, 'attention_heads': 8, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30715977684934187, 'global_pooling': 'max', 'learning_rate': 0.0005982062784093709, 'weight_decay': 0.0003096461479468082, 'beta_0': 0.8126217527290936, 'beta_1': 0.9940455074966158, 'epsilon': 8.370470342251373e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 894.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 838.69 MiB is free. Including non-PyTorch memory, this process has 43.73 GiB memory in use. Of the allocated memory 38.77 GiB is allocated by PyTorch, and 3.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 20:12:47,581] Trial 271 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9468280738982717, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 140, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3484892414430909, 'global_pooling': 'max', 'learning_rate': 0.0002551399144178938, 'weight_decay': 0.0002347409897587337, 'beta_0': 0.8166027581356177, 'beta_1': 0.9989828689610365, 'epsilon': 3.7691037309841925e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 20:35:00,901] Trial 272 finished with value: 0.6149341079522049 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.951663838505154, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34208051881772583, 'global_pooling': 'max', 'learning_rate': 0.00046294784805354987, 'weight_decay': 0.0001312854706335524, 'beta_0': 0.8107206024171061, 'beta_1': 0.9962793358413498, 'epsilon': 4.5189363638302396e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 20:59:16,198] Trial 273 finished with value: 0.6008467332325764 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9590563132140565, 'batch_size': 35, 'attention_heads': 10, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.336574418558845, 'global_pooling': 'max', 'learning_rate': 0.0001688539727280053, 'weight_decay': 0.00016745931082843006, 'beta_0': 0.8189019596795606, 'beta_1': 0.9833748123117018, 'epsilon': 6.205582310196187e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 1012.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 248.69 MiB is free. Including non-PyTorch memory, this process has 44.31 GiB memory in use. Of the allocated memory 42.65 GiB is allocated by PyTorch, and 521.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 21:15:52,206] Trial 274 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9406117387843118, 'batch_size': 37, 'attention_heads': 9, 'hidden_dimension': 134, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31765886099952567, 'global_pooling': 'max', 'learning_rate': 0.0003643221779554276, 'weight_decay': 0.00021965518051063337, 'beta_0': 0.8144185784158421, 'beta_1': 0.99428779934102, 'epsilon': 3.1580466975994134e-05, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 21:38:43,204] Trial 275 finished with value: 0.6049057848380605 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9470078674670035, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3262472936540498, 'global_pooling': 'max', 'learning_rate': 0.0003094674362378259, 'weight_decay': 4.557593500321473e-05, 'beta_0': 0.8080357656393554, 'beta_1': 0.982519728288855, 'epsilon': 5.237413095268616e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 868.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 484.69 MiB is free. Including non-PyTorch memory, this process has 44.08 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 915.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 21:55:43,517] Trial 276 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9202358909981757, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 122, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31114837758712477, 'global_pooling': 'max', 'learning_rate': 0.00020942503817471706, 'weight_decay': 0.0002576302305702901, 'beta_0': 0.8127437525080481, 'beta_1': 0.981782524832432, 'epsilon': 3.7335690492819064e-05, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 1.33 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 41.98 GiB is allocated by PyTorch, and 379.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 22:08:06,929] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9523852153673653, 'batch_size': 73, 'attention_heads': 9, 'hidden_dimension': 127, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33167598209812205, 'global_pooling': 'max', 'learning_rate': 0.0005217582122962305, 'weight_decay': 0.0003332988553972118, 'beta_0': 0.8216744734396236, 'beta_1': 0.9973707175994753, 'epsilon': 7.098402979169827e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 22:30:31,496] Trial 278 finished with value: 0.589119927962159 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9565322294717911, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32203402030225686, 'global_pooling': 'max', 'learning_rate': 0.00025415965922719763, 'weight_decay': 0.0001944862370967048, 'beta_0': 0.8055885936970653, 'beta_1': 0.9958978351820995, 'epsilon': 2.7348682494642686e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 193 with value: 0.649168643456903.
[I 2024-11-16 22:53:28,749] Trial 279 finished with value: 0.6059267652691133 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9440852575552484, 'batch_size': 35, 'attention_heads': 7, 'hidden_dimension': 135, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30439839044008893, 'global_pooling': 'max', 'learning_rate': 0.0003678822591252579, 'weight_decay': 0.000290829235742191, 'beta_0': 0.8164881412222036, 'beta_1': 0.9967484918257965, 'epsilon': 4.239642489547297e-05, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 72.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 42.54 GiB is allocated by PyTorch, and 813.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-16 23:06:31,371] Trial 280 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.879596885504534, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 91, 'number_of_hidden_layers': 2, 'dropout_rate': 0.47494870923569327, 'global_pooling': 'max', 'learning_rate': 0.00042428055729290537, 'weight_decay': 0.0001450335678985742, 'beta_0': 0.8198698118032507, 'beta_1': 0.9905840386188667, 'epsilon': 5.690759621683995e-05, 'balanced_loss': True, 'epochs': 53, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 193 with value: 0.649168643456903.
slurmstepd: error: *** JOB 13921739 ON gpu045 CANCELLED AT 2024-11-16T23:19:09 DUE TO TIME LIMIT ***
