[I 2024-11-17 04:20:21,675] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Grouped-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2024-11-17 04:47:53,856] Trial 267 finished with value: 0.6181775641601616 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8532669662506203, 'batch_size': 34, 'attention_heads': 14, 'hidden_dimension': 233, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4024840232909772, 'global_pooling': 'max', 'learning_rate': 7.518456278417071e-05, 'weight_decay': 0.00026790697611474805, 'beta_0': 0.8204628018761206, 'beta_1': 0.9872329565522653, 'epsilon': 1.385059543040363e-05, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 05:12:54,672] Trial 268 finished with value: 0.6374418523557707 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8625331864991161, 'batch_size': 32, 'attention_heads': 14, 'hidden_dimension': 238, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4098199995039866, 'global_pooling': 'max', 'learning_rate': 0.0001019171460675384, 'weight_decay': 0.00026328055104507845, 'beta_0': 0.8220588602867668, 'beta_1': 0.9864845018285632, 'epsilon': 7.959264115534256e-06, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 05:36:20,334] Trial 269 finished with value: 0.6120826564957014 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.862116812861527, 'batch_size': 32, 'attention_heads': 14, 'hidden_dimension': 233, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3922144694360053, 'global_pooling': 'max', 'learning_rate': 9.672387004083036e-05, 'weight_decay': 0.0002793454746946807, 'beta_0': 0.8135906663975736, 'beta_1': 0.9865959053890467, 'epsilon': 8.042600440595566e-06, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 06:02:06,658] Trial 270 finished with value: 0.6341032048409166 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8610126295426698, 'batch_size': 34, 'attention_heads': 14, 'hidden_dimension': 239, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4095110528213053, 'global_pooling': 'max', 'learning_rate': 0.00010659242556916864, 'weight_decay': 0.00032992576422872177, 'beta_0': 0.8225502229680925, 'beta_1': 0.9875500302847976, 'epsilon': 1.4802918489729711e-05, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 06:27:10,727] Trial 271 finished with value: 0.6259869163646445 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8618609879072133, 'batch_size': 33, 'attention_heads': 15, 'hidden_dimension': 238, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41197036792690556, 'global_pooling': 'max', 'learning_rate': 0.00011253887137174964, 'weight_decay': 0.0003417136201558054, 'beta_0': 0.820485893437589, 'beta_1': 0.9867631862606119, 'epsilon': 1.5290649146806452e-05, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 06:54:40,750] Trial 272 finished with value: 0.621463810503348 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8573767342825218, 'batch_size': 35, 'attention_heads': 14, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4058742730025197, 'global_pooling': 'max', 'learning_rate': 6.886614965596632e-05, 'weight_decay': 0.00024883530431174294, 'beta_0': 0.816496816083664, 'beta_1': 0.9874489383608015, 'epsilon': 1.2005891176146542e-05, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 07:19:06,879] Trial 273 finished with value: 0.6272950140400928 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8618095713749286, 'batch_size': 32, 'attention_heads': 14, 'hidden_dimension': 237, 'number_of_hidden_layers': 2, 'dropout_rate': 0.412345412591758, 'global_pooling': 'max', 'learning_rate': 8.638353574291045e-05, 'weight_decay': 0.0003114799030515748, 'beta_0': 0.8229997455991199, 'beta_1': 0.9870914026197096, 'epsilon': 8.244531587261812e-06, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 07:45:38,683] Trial 274 finished with value: 0.6383653102028143 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8563797402147387, 'batch_size': 36, 'attention_heads': 15, 'hidden_dimension': 238, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38723198376459383, 'global_pooling': 'max', 'learning_rate': 0.00012086399313558116, 'weight_decay': 0.00023408852624089607, 'beta_0': 0.8223195998651359, 'beta_1': 0.9867760422322568, 'epsilon': 1.5400183000074582e-05, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 08:10:09,267] Trial 275 finished with value: 0.5705071158298687 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8548584048852266, 'batch_size': 37, 'attention_heads': 15, 'hidden_dimension': 233, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38489597204998516, 'global_pooling': 'sum', 'learning_rate': 0.0001303457862619442, 'weight_decay': 0.00023261492721599568, 'beta_0': 0.8165825689873404, 'beta_1': 0.9865237546959149, 'epsilon': 9.485825253449605e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
CUDA out of memory. Tried to allocate 1.87 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 43.41 GiB memory in use. Of the allocated memory 37.91 GiB is allocated by PyTorch, and 4.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 08:27:14,729] Trial 276 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8574753141434752, 'batch_size': 36, 'attention_heads': 15, 'hidden_dimension': 237, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3755056829789784, 'global_pooling': 'max', 'learning_rate': 0.00010266665267183324, 'weight_decay': 0.00019332136262478975, 'beta_0': 0.8204793960347682, 'beta_1': 0.9868507001755946, 'epsilon': 1.1817716314011117e-05, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 710.69 MiB is free. Including non-PyTorch memory, this process has 43.86 GiB memory in use. Of the allocated memory 36.45 GiB is allocated by PyTorch, and 6.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 08:39:49,430] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8634146253679841, 'batch_size': 38, 'attention_heads': 14, 'hidden_dimension': 256, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4005025645124895, 'global_pooling': 'max', 'learning_rate': 0.00012066682913289713, 'weight_decay': 0.00025510354263343093, 'beta_0': 0.8167574448798688, 'beta_1': 0.9848363941785899, 'epsilon': 1.6605498654908782e-05, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 09:08:19,651] Trial 278 finished with value: 0.6322525646838137 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8534167864789756, 'batch_size': 34, 'attention_heads': 14, 'hidden_dimension': 240, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3993966110083142, 'global_pooling': 'max', 'learning_rate': 8.84248284033686e-05, 'weight_decay': 0.00021579749645465928, 'beta_0': 0.8202687075593778, 'beta_1': 0.9865887145123309, 'epsilon': 7.956030488354693e-06, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 09:35:52,933] Trial 279 finished with value: 0.6318749250711754 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8561869069212751, 'batch_size': 36, 'attention_heads': 14, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39285428888774815, 'global_pooling': 'max', 'learning_rate': 6.122194679227032e-05, 'weight_decay': 0.0002850841763071866, 'beta_0': 0.8244813789370747, 'beta_1': 0.9863582240009762, 'epsilon': 1.2735786145176356e-05, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
CUDA out of memory. Tried to allocate 1.94 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.43 GiB is free. Including non-PyTorch memory, this process has 43.12 GiB memory in use. Of the allocated memory 39.52 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 09:45:12,506] Trial 280 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8497778470101525, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 251, 'number_of_hidden_layers': 2, 'dropout_rate': 0.425098356366018, 'global_pooling': 'max', 'learning_rate': 0.00012907156062761036, 'weight_decay': 0.00020604395839925955, 'beta_0': 0.8136684486250109, 'beta_1': 0.9870153254541054, 'epsilon': 2.1079554826822617e-05, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 231 with value: 0.6544151158172802.
[I 2024-11-17 10:11:46,961] Trial 281 finished with value: 0.6557486637302313 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8648898474552925, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 234, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4157570488674529, 'global_pooling': 'max', 'learning_rate': 0.00010334967850281818, 'weight_decay': 0.0003845904702042367, 'beta_0': 0.8272688904433283, 'beta_1': 0.985075435409474, 'epsilon': 9.795086416355876e-06, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 42.82 GiB memory in use. Of the allocated memory 36.13 GiB is allocated by PyTorch, and 5.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 10:36:00,566] Trial 282 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.864369190002517, 'batch_size': 39, 'attention_heads': 15, 'hidden_dimension': 246, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41699202876092367, 'global_pooling': 'max', 'learning_rate': 7.882257446667463e-05, 'weight_decay': 0.00023656033317348085, 'beta_0': 0.8260458463221416, 'beta_1': 0.9850739977789774, 'epsilon': 9.422878671634153e-06, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 11:01:49,119] Trial 283 finished with value: 0.6357390306127836 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8643864896364186, 'batch_size': 34, 'attention_heads': 14, 'hidden_dimension': 239, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41194837538198315, 'global_pooling': 'max', 'learning_rate': 0.00011494952203196709, 'weight_decay': 0.00039399346745604217, 'beta_0': 0.8076018605644882, 'beta_1': 0.9847853531228353, 'epsilon': 7.5918862846074276e-06, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 11:28:40,927] Trial 284 finished with value: 0.6385421844929172 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8578588791808452, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 230, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41157260990994954, 'global_pooling': 'max', 'learning_rate': 0.00011984688986973617, 'weight_decay': 0.0003618501733858881, 'beta_0': 0.8067559670862733, 'beta_1': 0.9850249594140367, 'epsilon': 8.52797843622984e-06, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 11:54:22,487] Trial 285 finished with value: 0.627405943531062 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8576734418744467, 'batch_size': 34, 'attention_heads': 14, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41134958349694384, 'global_pooling': 'max', 'learning_rate': 0.00011365221905349933, 'weight_decay': 0.000376833835480725, 'beta_0': 0.8027401632886378, 'beta_1': 0.985017975579073, 'epsilon': 7.713201539230085e-06, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 12:19:40,540] Trial 286 finished with value: 0.5623204142503945 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8630376336831361, 'batch_size': 32, 'attention_heads': 15, 'hidden_dimension': 236, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40505044325502565, 'global_pooling': 'max', 'learning_rate': 0.00013456575171199157, 'weight_decay': 0.00041678048766197775, 'beta_0': 0.808615055815408, 'beta_1': 0.9852734799549733, 'epsilon': 1.0500114421386817e-05, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 42.81 GiB memory in use. Of the allocated memory 40.13 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 12:31:59,505] Trial 287 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8530235312606269, 'batch_size': 38, 'attention_heads': 15, 'hidden_dimension': 232, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4144235346138198, 'global_pooling': 'max', 'learning_rate': 8.953935429587645e-05, 'weight_decay': 0.0003509465737970946, 'beta_0': 0.8058878987309752, 'beta_1': 0.9857871335016251, 'epsilon': 9.252158771403421e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 12:57:46,919] Trial 288 finished with value: 0.6288774306015164 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8642201999478111, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 238, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4286526036504259, 'global_pooling': 'max', 'learning_rate': 0.00010680471025301697, 'weight_decay': 0.00042307331630341157, 'beta_0': 0.8128255979156427, 'beta_1': 0.9846377327921894, 'epsilon': 1.5239187616692314e-05, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 13:25:39,288] Trial 289 finished with value: 0.6311910658820228 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8587280553237242, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 240, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4235581678738555, 'global_pooling': 'max', 'learning_rate': 0.00012527680913267103, 'weight_decay': 0.00018182898402034452, 'beta_0': 0.8277025543884314, 'beta_1': 0.9863620741974719, 'epsilon': 8.096866311220388e-06, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 13:53:11,198] Trial 290 finished with value: 0.6315206728767658 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8550879917415051, 'batch_size': 35, 'attention_heads': 14, 'hidden_dimension': 230, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40688985973739145, 'global_pooling': 'max', 'learning_rate': 6.897526228089722e-05, 'weight_decay': 0.00029967903113221515, 'beta_0': 0.8160137077746468, 'beta_1': 0.9848762057029142, 'epsilon': 7.355242194871826e-05, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 281 with value: 0.6557486637302313.
[I 2024-11-17 14:17:51,570] Trial 291 finished with value: 0.6575659225668925 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8645139368383729, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 241, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3976866211711239, 'global_pooling': 'max', 'learning_rate': 0.0001415583362137864, 'weight_decay': 0.00021778024034572495, 'beta_0': 0.8216118296568086, 'beta_1': 0.9857355822267512, 'epsilon': 6.9871479225027695e-06, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
[I 2024-11-17 14:32:11,045] Trial 292 finished with value: 0.5797161327653526 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9845363739242636, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 241, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39720735868443746, 'global_pooling': 'max', 'learning_rate': 0.00014116160134303355, 'weight_decay': 0.00020982364865459972, 'beta_0': 0.8219299391157731, 'beta_1': 0.9857630762002128, 'epsilon': 7.008984347501242e-06, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
[I 2024-11-17 14:49:33,820] Trial 293 finished with value: 0.6227814440547113 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9614182680756566, 'batch_size': 39, 'attention_heads': 15, 'hidden_dimension': 235, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40127935377922686, 'global_pooling': 'max', 'learning_rate': 0.00010889744478395019, 'weight_decay': 0.00023964506404233345, 'beta_0': 0.8172743942565126, 'beta_1': 0.9854437840686715, 'epsilon': 1.0363291703328393e-05, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
[I 2024-11-17 15:16:00,528] Trial 294 finished with value: 0.6274866756238832 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8643070495548445, 'batch_size': 32, 'attention_heads': 14, 'hidden_dimension': 248, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41397346352124376, 'global_pooling': 'max', 'learning_rate': 8.004138593866364e-05, 'weight_decay': 0.0002631344479478501, 'beta_0': 0.8073575743686653, 'beta_1': 0.9862608514502145, 'epsilon': 1.3051632797594089e-05, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 43.30 GiB memory in use. Of the allocated memory 35.00 GiB is allocated by PyTorch, and 7.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 15:29:02,603] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8494647881960736, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 240, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40844501981838216, 'global_pooling': 'max', 'learning_rate': 0.00013902785399691142, 'weight_decay': 0.00037981170181602947, 'beta_0': 0.8209274911469489, 'beta_1': 0.9851140306392552, 'epsilon': 6.540446051062076e-06, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 17, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 43.28 GiB memory in use. Of the allocated memory 35.85 GiB is allocated by PyTorch, and 6.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-17 15:47:21,178] Trial 296 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8592648419781846, 'batch_size': 35, 'attention_heads': 16, 'hidden_dimension': 246, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4321975945918361, 'global_pooling': 'max', 'learning_rate': 9.813736775295031e-05, 'weight_decay': 0.0001851366688969949, 'beta_0': 0.81014396687301, 'beta_1': 0.984700363944345, 'epsilon': 9.113016808979568e-06, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 291 with value: 0.6575659225668925.
slurmstepd: error: *** JOB 13945147 ON gpu039 CANCELLED AT 2024-11-17T16:20:18 DUE TO TIME LIMIT ***
