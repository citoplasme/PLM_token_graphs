[I 2024-11-13 20:17:43,662] Using an existing study with name 'Ohsumed-GAT-google-bert-bert-base-uncased-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2024-11-13 20:38:04,670] Trial 30 finished with value: 0.2362603116046685 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9982479407762429, 'batch_size': 70, 'attention_heads': 9, 'hidden_dimension': 241, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3664357579252988, 'global_pooling': 'sum', 'learning_rate': 0.0004255007010130252, 'weight_decay': 7.203821205189772e-06, 'beta_0': 0.892023947983119, 'beta_1': 0.9812770842350512, 'epsilon': 3.0070162326071327e-06, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 104.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 42.73 GiB is allocated by PyTorch, and 585.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-13 20:55:26,642] Trial 31 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9106081074704349, 'batch_size': 84, 'attention_heads': 7, 'hidden_dimension': 70, 'number_of_hidden_layers': 1, 'dropout_rate': 0.32351925686556815, 'global_pooling': 'max', 'learning_rate': 0.00018185984984845903, 'weight_decay': 5.304748503302373e-06, 'beta_0': 0.8727372240146068, 'beta_1': 0.9842238504829, 'epsilon': 1.1286551377492847e-08, 'balanced_loss': True, 'epochs': 63, 'early_stopping_patience': 25, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 3.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 42.78 GiB memory in use. Of the allocated memory 41.16 GiB is allocated by PyTorch, and 480.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-13 21:07:53,334] Trial 32 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9323474252213133, 'batch_size': 69, 'attention_heads': 10, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3554080088241858, 'global_pooling': 'sum', 'learning_rate': 1.84590883579986e-05, 'weight_decay': 8.929196825909799e-05, 'beta_0': 0.8591954397366653, 'beta_1': 0.9946444806839707, 'epsilon': 1.1252677835631435e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-13 21:34:54,504] Trial 33 finished with value: 0.5695060665639212 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9723871275709165, 'batch_size': 56, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3368667216306182, 'global_pooling': 'sum', 'learning_rate': 0.0013615143511687261, 'weight_decay': 1.9871782886417332e-05, 'beta_0': 0.8485067664889678, 'beta_1': 0.9869468090043323, 'epsilon': 2.1280227896375595e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 43.03 GiB memory in use. Of the allocated memory 39.88 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-13 21:52:11,793] Trial 34 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.958123824628978, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 221, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42175106803307677, 'global_pooling': 'sum', 'learning_rate': 0.0007671807864038474, 'weight_decay': 0.0001817437958868209, 'beta_0': 0.834896161445195, 'beta_1': 0.9857587431387902, 'epsilon': 4.9530368629433854e-06, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 24, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-13 22:17:04,957] Trial 35 finished with value: 0.5857332243620731 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9802258265210644, 'batch_size': 54, 'attention_heads': 14, 'hidden_dimension': 183, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3753502944638602, 'global_pooling': 'sum', 'learning_rate': 0.0003119179016332406, 'weight_decay': 0.0008648423086264918, 'beta_0': 0.8223177168862821, 'beta_1': 0.9851616734513537, 'epsilon': 9.611271159992162e-06, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-13 22:36:25,296] Trial 36 finished with value: 0.516747338508361 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9885109250827075, 'batch_size': 54, 'attention_heads': 16, 'hidden_dimension': 162, 'number_of_hidden_layers': 0, 'dropout_rate': 0.37660660462808127, 'global_pooling': 'sum', 'learning_rate': 0.00039075050081962436, 'weight_decay': 0.0008441154803655337, 'beta_0': 0.8217431094888229, 'beta_1': 0.9909967781031849, 'epsilon': 2.0660923615252097e-06, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-13 23:02:51,763] Trial 37 finished with value: 0.5157402259698536 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9764727184963884, 'batch_size': 66, 'attention_heads': 14, 'hidden_dimension': 135, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3382940593733516, 'global_pooling': 'sum', 'learning_rate': 8.064182402125447e-05, 'weight_decay': 0.0004918967603898464, 'beta_0': 0.8123966899789267, 'beta_1': 0.9812803789785229, 'epsilon': 1.1246427129372223e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 3.32 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.37 GiB is free. Including non-PyTorch memory, this process has 42.18 GiB memory in use. Of the allocated memory 37.67 GiB is allocated by PyTorch, and 3.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-13 23:15:36,932] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.951561240514015, 'batch_size': 74, 'attention_heads': 15, 'hidden_dimension': 186, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40813585681662595, 'global_pooling': 'max', 'learning_rate': 0.00022287585570269263, 'weight_decay': 2.333752611815121e-05, 'beta_0': 0.8316901839727073, 'beta_1': 0.9849887033473168, 'epsilon': 3.943203508652823e-06, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-13 23:44:53,217] Trial 39 finished with value: 0.5274831181843592 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9853463876161322, 'batch_size': 91, 'attention_heads': 15, 'hidden_dimension': 198, 'number_of_hidden_layers': 4, 'dropout_rate': 0.32425141615907105, 'global_pooling': 'mean', 'learning_rate': 0.0005767130057609342, 'weight_decay': 0.0006542623962752251, 'beta_0': 0.8044714252275419, 'beta_1': 0.9834228264042524, 'epsilon': 1.3319736614914695e-06, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 98.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 42.05 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-13 23:52:51,913] Trial 40 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8487004757502096, 'batch_size': 124, 'attention_heads': 6, 'hidden_dimension': 142, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4718229936345373, 'global_pooling': 'sum', 'learning_rate': 0.0020965894923904303, 'weight_decay': 3.4944791232795145e-06, 'beta_0': 0.8211251696234062, 'beta_1': 0.9890127933178592, 'epsilon': 8.712590268201266e-06, 'balanced_loss': False, 'epochs': 81, 'early_stopping_patience': 16, 'plateau_patience': 22, 'plateau_divider': 9}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-14 00:09:09,699] Trial 41 finished with value: 0.02483275212522782 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9402854819946833, 'batch_size': 62, 'attention_heads': 5, 'hidden_dimension': 109, 'number_of_hidden_layers': 3, 'dropout_rate': 0.39329846465005935, 'global_pooling': 'max', 'learning_rate': 0.034111360481763844, 'weight_decay': 9.924323082235732e-05, 'beta_0': 0.8703621152563541, 'beta_1': 0.9902749355702517, 'epsilon': 8.726067391163091e-07, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 23, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 27 with value: 0.5921714818420317.
CUDA out of memory. Tried to allocate 232.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 32.69 MiB is free. Including non-PyTorch memory, this process has 44.52 GiB memory in use. Of the allocated memory 42.17 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 00:22:56,324] Trial 42 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8845565657015305, 'batch_size': 80, 'attention_heads': 14, 'hidden_dimension': 206, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3536890675673934, 'global_pooling': 'mean', 'learning_rate': 0.0059863793154990765, 'weight_decay': 0.00014718248894110555, 'beta_0': 0.8155578792165477, 'beta_1': 0.9800206088374543, 'epsilon': 2.2956010297358584e-07, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-14 00:50:37,241] Trial 43 finished with value: 0.5743234110844109 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9726675496274991, 'batch_size': 45, 'attention_heads': 12, 'hidden_dimension': 222, 'number_of_hidden_layers': 2, 'dropout_rate': 0.43729236582964387, 'global_pooling': 'sum', 'learning_rate': 0.00031578410555128705, 'weight_decay': 0.0002551174657668117, 'beta_0': 0.8399429635887663, 'beta_1': 0.9875485505727317, 'epsilon': 4.248338864655992e-05, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 27 with value: 0.5921714818420317.
[I 2024-11-14 01:16:07,704] Trial 44 finished with value: 0.5964873926295671 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9756962787077942, 'batch_size': 32, 'attention_heads': 11, 'hidden_dimension': 246, 'number_of_hidden_layers': 2, 'dropout_rate': 0.45628739859429157, 'global_pooling': 'sum', 'learning_rate': 0.00031156715065583645, 'weight_decay': 0.0003886545969021282, 'beta_0': 0.8384013740920508, 'beta_1': 0.9876866336258635, 'epsilon': 5.812898099410995e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 01:40:39,109] Trial 45 finished with value: 0.47659404261048804 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9944179346803539, 'batch_size': 55, 'attention_heads': 11, 'hidden_dimension': 244, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4943685274157913, 'global_pooling': 'sum', 'learning_rate': 0.00014970544952469566, 'weight_decay': 0.00043597517799440845, 'beta_0': 0.830630051314654, 'beta_1': 0.9846102161694198, 'epsilon': 4.85624869084282e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 02:08:50,266] Trial 46 finished with value: 0.5529536609099077 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9775026255087574, 'batch_size': 75, 'attention_heads': 10, 'hidden_dimension': 249, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4680026724270363, 'global_pooling': 'sum', 'learning_rate': 0.00029542329418943725, 'weight_decay': 0.0006785908627941942, 'beta_0': 0.8250461738630347, 'beta_1': 0.9853650486037776, 'epsilon': 6.52630975900876e-05, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 02:31:44,728] Trial 47 finished with value: 0.5123761906013915 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.952312769129452, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 212, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4570298283401972, 'global_pooling': 'sum', 'learning_rate': 7.022215689990219e-05, 'weight_decay': 0.00041997786566424047, 'beta_0': 0.834464965760135, 'beta_1': 0.9835026942223343, 'epsilon': 1.48876243856222e-05, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 21, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 02:46:04,663] Trial 48 finished with value: 0.520870643453516 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9928991802286983, 'batch_size': 70, 'attention_heads': 7, 'hidden_dimension': 184, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37600560905272606, 'global_pooling': 'mean', 'learning_rate': 0.0006289350546170576, 'weight_decay': 4.451849560791382e-05, 'beta_0': 0.8509798707853505, 'beta_1': 0.9811827415612769, 'epsilon': 2.7752299092495197e-05, 'balanced_loss': True, 'epochs': 130, 'early_stopping_patience': 24, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 44 with value: 0.5964873926295671.
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 20.69 MiB is free. Including non-PyTorch memory, this process has 44.53 GiB memory in use. Of the allocated memory 42.70 GiB is allocated by PyTorch, and 702.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 02:54:47,779] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8027589854656068, 'batch_size': 32, 'attention_heads': 5, 'hidden_dimension': 158, 'number_of_hidden_layers': 3, 'dropout_rate': 0.41303077432849683, 'global_pooling': 'max', 'learning_rate': 2.7902352902145472e-05, 'weight_decay': 1.1277876715145181e-05, 'beta_0': 0.8845546555124624, 'beta_1': 0.9881518637169298, 'epsilon': 4.135595322398963e-07, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 44 with value: 0.5964873926295671.
CUDA out of memory. Tried to allocate 2.08 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 43.21 GiB memory in use. Of the allocated memory 40.53 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 03:08:15,292] Trial 50 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9627584003966425, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 200, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5060954702146057, 'global_pooling': 'sum', 'learning_rate': 0.00320164739883335, 'weight_decay': 2.7610801716631598e-05, 'beta_0': 0.8373134466136973, 'beta_1': 0.9825896915919182, 'epsilon': 3.7995481751938513e-06, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 03:34:14,569] Trial 51 finished with value: 0.5274124640573763 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9797070462075984, 'batch_size': 59, 'attention_heads': 13, 'hidden_dimension': 228, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48268936193044026, 'global_pooling': 'sum', 'learning_rate': 0.00022403814135812575, 'weight_decay': 0.0007876612629830755, 'beta_0': 0.8641856332900295, 'beta_1': 0.9862446805439139, 'epsilon': 7.86031101449287e-06, 'balanced_loss': False, 'epochs': 87, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 44 with value: 0.5964873926295671.
CUDA out of memory. Tried to allocate 3.93 GiB. GPU 0 has a total capacity of 44.56 GiB of which 880.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 38.65 GiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 03:51:37,238] Trial 52 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9441090004441025, 'batch_size': 83, 'attention_heads': 15, 'hidden_dimension': 168, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5403848400622184, 'global_pooling': 'max', 'learning_rate': 0.00013214575713351275, 'weight_decay': 1.8680598020238798e-06, 'beta_0': 0.8202534894382227, 'beta_1': 0.9897449808282018, 'epsilon': 1.3161701040975964e-07, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 11, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 04:18:59,449] Trial 53 finished with value: 0.5643762470508128 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9713938055055867, 'batch_size': 45, 'attention_heads': 12, 'hidden_dimension': 222, 'number_of_hidden_layers': 2, 'dropout_rate': 0.45110341809366844, 'global_pooling': 'sum', 'learning_rate': 0.00032010447164739923, 'weight_decay': 0.0002567362860123773, 'beta_0': 0.8409227561798909, 'beta_1': 0.9874481041995528, 'epsilon': 4.366867341118123e-05, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 04:42:24,313] Trial 54 finished with value: 0.5624937591650331 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9886640186893166, 'batch_size': 51, 'attention_heads': 11, 'hidden_dimension': 247, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38536494621248574, 'global_pooling': 'sum', 'learning_rate': 0.0003500457710958475, 'weight_decay': 0.00033997044378570804, 'beta_0': 0.8390192872344039, 'beta_1': 0.9870958580945433, 'epsilon': 6.426480282175518e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 05:06:16,286] Trial 55 finished with value: 0.552775653407869 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9720256437436413, 'batch_size': 39, 'attention_heads': 12, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43433734618973197, 'global_pooling': 'sum', 'learning_rate': 0.0013410833592692479, 'weight_decay': 0.0006039379470926426, 'beta_0': 0.8448819165499768, 'beta_1': 0.9857543938879844, 'epsilon': 1.7204196497435243e-05, 'balanced_loss': False, 'epochs': 76, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 05:27:35,938] Trial 56 finished with value: 0.5196509507051441 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9569191958028266, 'batch_size': 44, 'attention_heads': 10, 'hidden_dimension': 256, 'number_of_hidden_layers': 0, 'dropout_rate': 0.364803628174928, 'global_pooling': 'sum', 'learning_rate': 0.00027357089401379545, 'weight_decay': 0.00012050571168063018, 'beta_0': 0.8297300597176482, 'beta_1': 0.9836390093546588, 'epsilon': 4.155891871151368e-05, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 05:43:20,439] Trial 57 finished with value: 0.3427574118508793 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9986361275391116, 'batch_size': 100, 'attention_heads': 11, 'hidden_dimension': 218, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4595998382299266, 'global_pooling': 'sum', 'learning_rate': 0.000695874277023632, 'weight_decay': 0.0002438567171199754, 'beta_0': 0.8329908721814608, 'beta_1': 0.9851202718039375, 'epsilon': 1.4238145376551033e-05, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 44 with value: 0.5964873926295671.
CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 43.34 GiB memory in use. Of the allocated memory 41.39 GiB is allocated by PyTorch, and 825.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 06:01:37,117] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9666381598745715, 'batch_size': 36, 'attention_heads': 13, 'hidden_dimension': 239, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4390651033107923, 'global_pooling': 'sum', 'learning_rate': 0.00046974016400825745, 'weight_decay': 0.00017893580987740683, 'beta_0': 0.8472755013528495, 'beta_1': 0.9887371349741095, 'epsilon': 3.1526938801642406e-08, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 23, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 06:19:27,911] Trial 59 finished with value: 0.5921313096431389 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9783447264938624, 'batch_size': 60, 'attention_heads': 12, 'hidden_dimension': 188, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4017911541160218, 'global_pooling': 'mean', 'learning_rate': 0.0001747585194435863, 'weight_decay': 7.975944931481382e-05, 'beta_0': 0.8243713929325813, 'beta_1': 0.9877448339099596, 'epsilon': 2.6945217838701105e-05, 'balanced_loss': True, 'epochs': 57, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 06:33:33,036] Trial 60 finished with value: 0.41745849888941694 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9823646328109291, 'batch_size': 60, 'attention_heads': 4, 'hidden_dimension': 150, 'number_of_hidden_layers': 0, 'dropout_rate': 0.397867554678756, 'global_pooling': 'mean', 'learning_rate': 5.607832830590404e-05, 'weight_decay': 8.97583648380616e-05, 'beta_0': 0.8243218125259524, 'beta_1': 0.9828217261572394, 'epsilon': 2.6018356204100155e-05, 'balanced_loss': True, 'epochs': 57, 'early_stopping_patience': 25, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 06:49:29,226] Trial 61 finished with value: 0.4795655686064066 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9916161343730172, 'batch_size': 64, 'attention_heads': 9, 'hidden_dimension': 186, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3451655325821611, 'global_pooling': 'mean', 'learning_rate': 0.00017088411024411548, 'weight_decay': 3.713842706068135e-05, 'beta_0': 0.8099694580564478, 'beta_1': 0.9842133434574619, 'epsilon': 6.805868593207083e-05, 'balanced_loss': True, 'epochs': 146, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 44 with value: 0.5964873926295671.
CUDA out of memory. Tried to allocate 2.79 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 42.65 GiB memory in use. Of the allocated memory 41.07 GiB is allocated by PyTorch, and 434.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 07:02:26,910] Trial 62 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9153539710335219, 'batch_size': 54, 'attention_heads': 10, 'hidden_dimension': 192, 'number_of_hidden_layers': 1, 'dropout_rate': 0.39960553766436474, 'global_pooling': 'mean', 'learning_rate': 3.689049091798599e-05, 'weight_decay': 0.0009982718449651857, 'beta_0': 0.8597819466450535, 'beta_1': 0.9866306444875763, 'epsilon': 4.900115308210624e-06, 'balanced_loss': True, 'epochs': 54, 'early_stopping_patience': 25, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 07:22:09,089] Trial 63 finished with value: 0.5694983435542273 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9769786253006115, 'batch_size': 51, 'attention_heads': 12, 'hidden_dimension': 170, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4228417460317059, 'global_pooling': 'mean', 'learning_rate': 9.612227154615881e-05, 'weight_decay': 7.571297159728098e-05, 'beta_0': 0.8280225703549977, 'beta_1': 0.9876171823297897, 'epsilon': 2.6605417312998275e-05, 'balanced_loss': True, 'epochs': 66, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 07:49:08,078] Trial 64 finished with value: 0.5675188001084777 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9673070778012831, 'batch_size': 48, 'attention_heads': 12, 'hidden_dimension': 207, 'number_of_hidden_layers': 2, 'dropout_rate': 0.44343963587634444, 'global_pooling': 'sum', 'learning_rate': 0.00024490040634781894, 'weight_decay': 0.000139981039947707, 'beta_0': 0.8170804409100757, 'beta_1': 0.9892302500014217, 'epsilon': 3.7175509731781844e-05, 'balanced_loss': True, 'epochs': 74, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 08:09:14,095] Trial 65 finished with value: 0.586569647707262 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9839231227039437, 'batch_size': 57, 'attention_heads': 11, 'hidden_dimension': 236, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3710404870681786, 'global_pooling': 'max', 'learning_rate': 0.0008970242688476085, 'weight_decay': 5.19984766882644e-05, 'beta_0': 0.839315896296016, 'beta_1': 0.9903827814865431, 'epsilon': 9.673680570495153e-05, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 44 with value: 0.5964873926295671.
[I 2024-11-14 08:28:57,764] Trial 66 finished with value: 0.6113475187044284 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9831278709878025, 'batch_size': 67, 'attention_heads': 11, 'hidden_dimension': 235, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3632993560146129, 'global_pooling': 'max', 'learning_rate': 0.0009435017560410393, 'weight_decay': 5.59466349299667e-05, 'beta_0': 0.8759085751982392, 'beta_1': 0.990220131904807, 'epsilon': 7.683056250294073e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 08:48:15,897] Trial 67 finished with value: 0.5761986246578484 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9820308644761263, 'batch_size': 68, 'attention_heads': 11, 'hidden_dimension': 234, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3712171952927523, 'global_pooling': 'max', 'learning_rate': 0.0009214323846270127, 'weight_decay': 5.419946728901918e-05, 'beta_0': 0.8754182819651153, 'beta_1': 0.9915735390399428, 'epsilon': 9.248253914830739e-05, 'balanced_loss': True, 'epochs': 64, 'early_stopping_patience': 21, 'plateau_patience': 10, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 09:06:45,155] Trial 68 finished with value: 0.5625760146438029 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9892864913816873, 'batch_size': 76, 'attention_heads': 11, 'hidden_dimension': 247, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35888051951062167, 'global_pooling': 'max', 'learning_rate': 0.00048819193724791504, 'weight_decay': 4.754303962703966e-05, 'beta_0': 0.8862964154303292, 'beta_1': 0.9927955334826494, 'epsilon': 9.979929859621959e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 09:20:21,354] Trial 69 finished with value: 0.26145201747794905 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9992125798214053, 'batch_size': 72, 'attention_heads': 10, 'hidden_dimension': 235, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3322880100156999, 'global_pooling': 'max', 'learning_rate': 0.0021614667920025394, 'weight_decay': 1.7677050498442776e-05, 'beta_0': 0.8775950238174477, 'beta_1': 0.9906692271230272, 'epsilon': 6.020691158475078e-05, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 09:39:46,378] Trial 70 finished with value: 0.5739971921874767 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9752026589329222, 'batch_size': 61, 'attention_heads': 10, 'hidden_dimension': 249, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31436064255983986, 'global_pooling': 'max', 'learning_rate': 0.0010479069557951543, 'weight_decay': 2.9726380109789154e-05, 'beta_0': 0.8941456498508205, 'beta_1': 0.9942870832838394, 'epsilon': 2.2457135404450404e-06, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 09:56:00,168] Trial 71 finished with value: 0.5789137123584552 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9853659569288796, 'batch_size': 78, 'attention_heads': 11, 'hidden_dimension': 228, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34854912409112226, 'global_pooling': 'max', 'learning_rate': 0.00011744771791498362, 'weight_decay': 1.3735272108461484e-05, 'beta_0': 0.8669504391716755, 'beta_1': 0.9903025085204025, 'epsilon': 7.427636212218354e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 10:19:24,604] Trial 72 finished with value: 0.5849577221742883 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9865065001432017, 'batch_size': 78, 'attention_heads': 11, 'hidden_dimension': 213, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38582490400007763, 'global_pooling': 'max', 'learning_rate': 0.00011988652186765857, 'weight_decay': 1.2670295396184966e-05, 'beta_0': 0.8700770394922422, 'beta_1': 0.9914090237555904, 'epsilon': 7.954459489241705e-05, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 25, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 10:41:45,079] Trial 73 finished with value: 0.5770478051237273 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.984810571912496, 'batch_size': 78, 'attention_heads': 11, 'hidden_dimension': 215, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34670401102391063, 'global_pooling': 'max', 'learning_rate': 0.00011598570937197264, 'weight_decay': 1.0311131754459422e-05, 'beta_0': 0.8697448529637242, 'beta_1': 0.9921658387575449, 'epsilon': 5.819436698413972e-05, 'balanced_loss': False, 'epochs': 87, 'early_stopping_patience': 25, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 11:03:21,455] Trial 74 finished with value: 0.5496584904414424 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9936232790994389, 'batch_size': 89, 'attention_heads': 9, 'hidden_dimension': 225, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38438254241706954, 'global_pooling': 'max', 'learning_rate': 6.562603295062278e-05, 'weight_decay': 7.779376840591422e-06, 'beta_0': 0.8798687809482935, 'beta_1': 0.990215497423821, 'epsilon': 8.132907635089356e-05, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
CUDA out of memory. Tried to allocate 2.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 42.83 GiB memory in use. Of the allocated memory 39.91 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 11:16:15,615] Trial 75 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.962862780144907, 'batch_size': 67, 'attention_heads': 10, 'hidden_dimension': 242, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3647967254134932, 'global_pooling': 'max', 'learning_rate': 0.00020246435384510306, 'weight_decay': 1.348942878418796e-05, 'beta_0': 0.8726007195886509, 'beta_1': 0.9915374989122829, 'epsilon': 3.295519601456202e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 11:33:25,297] Trial 76 finished with value: 0.5854073093100624 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9844503980224697, 'batch_size': 72, 'attention_heads': 11, 'hidden_dimension': 230, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37408124973771756, 'global_pooling': 'max', 'learning_rate': 0.0004107364258374259, 'weight_decay': 2.1558218751132394e-05, 'beta_0': 0.8678599684507361, 'beta_1': 0.9895920874432685, 'epsilon': 7.694766950907189e-05, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-14 11:51:21,379] Trial 77 finished with value: 0.5860575999911898 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9792732597535433, 'batch_size': 72, 'attention_heads': 12, 'hidden_dimension': 211, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3799975076123972, 'global_pooling': 'max', 'learning_rate': 0.0004115827508625707, 'weight_decay': 2.4577959798044904e-05, 'beta_0': 0.8835252779354239, 'beta_1': 0.9885120440839872, 'epsilon': 2.049717897478942e-05, 'balanced_loss': False, 'epochs': 80, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 66 with value: 0.6113475187044284.
CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 460.69 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 40.31 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 12:04:20,019] Trial 78 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9575789949233281, 'batch_size': 72, 'attention_heads': 13, 'hidden_dimension': 193, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4081885038687079, 'global_pooling': 'max', 'learning_rate': 0.00041973583739765243, 'weight_decay': 2.2795143425069782e-05, 'beta_0': 0.883895973655691, 'beta_1': 0.9883760687309945, 'epsilon': 2.2657887698312034e-05, 'balanced_loss': False, 'epochs': 80, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 66 with value: 0.6113475187044284.
CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 98.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 42.57 GiB is allocated by PyTorch, and 758.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-14 12:14:57,050] Trial 79 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8873012906964253, 'batch_size': 57, 'attention_heads': 12, 'hidden_dimension': 202, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37782240736921957, 'global_pooling': 'max', 'learning_rate': 0.0014525429168704286, 'weight_decay': 3.763406612995462e-05, 'beta_0': 0.8878212090871072, 'beta_1': 0.9895801334800227, 'epsilon': 5.0030194940481957e-05, 'balanced_loss': True, 'epochs': 70, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 66 with value: 0.6113475187044284.
slurmstepd: error: *** JOB 13921741 ON gpu005 CANCELLED AT 2024-11-14T12:18:02 DUE TO TIME LIMIT ***
