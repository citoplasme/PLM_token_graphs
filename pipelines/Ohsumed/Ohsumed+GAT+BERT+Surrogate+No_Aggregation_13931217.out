[I 2024-11-15 11:20:28,191] Using an existing study with name 'Ohsumed-GAT-google-bert-bert-base-uncased-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2024-11-15 11:39:56,048] Trial 163 finished with value: 0.5906536690883484 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9722976936575981, 'batch_size': 61, 'attention_heads': 12, 'hidden_dimension': 237, 'number_of_hidden_layers': 1, 'dropout_rate': 0.520659600526148, 'global_pooling': 'mean', 'learning_rate': 0.0006774096037771528, 'weight_decay': 9.431969483132527e-06, 'beta_0': 0.8962138771463776, 'beta_1': 0.9883052331378004, 'epsilon': 2.5091607466093233e-05, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-15 11:57:41,698] Trial 164 finished with value: 0.6014664615893208 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9768244672630395, 'batch_size': 60, 'attention_heads': 13, 'hidden_dimension': 251, 'number_of_hidden_layers': 1, 'dropout_rate': 0.361452831392252, 'global_pooling': 'mean', 'learning_rate': 0.0005910252295230753, 'weight_decay': 1.2206376646568773e-05, 'beta_0': 0.8561035080656548, 'beta_1': 0.9868532262245593, 'epsilon': 2.8139510586661e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-15 12:15:40,703] Trial 165 finished with value: 0.5958637922354818 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.976153064409385, 'batch_size': 64, 'attention_heads': 12, 'hidden_dimension': 250, 'number_of_hidden_layers': 1, 'dropout_rate': 0.336838768555925, 'global_pooling': 'mean', 'learning_rate': 0.0005315656019861763, 'weight_decay': 1.4753542713273862e-05, 'beta_0': 0.8625820413881174, 'beta_1': 0.9868860055043923, 'epsilon': 2.846133648375975e-05, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 66 with value: 0.6113475187044284.
CUDA out of memory. Tried to allocate 2.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.14 GiB is free. Including non-PyTorch memory, this process has 42.42 GiB memory in use. Of the allocated memory 38.65 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-15 12:27:42,460] Trial 166 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9646832642315726, 'batch_size': 58, 'attention_heads': 12, 'hidden_dimension': 253, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3631580076009756, 'global_pooling': 'mean', 'learning_rate': 0.0005953194772042091, 'weight_decay': 1.1938482010518242e-05, 'beta_0': 0.8506711744033654, 'beta_1': 0.9863397876986822, 'epsilon': 3.558003613421319e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 66 with value: 0.6113475187044284.
CUDA out of memory. Tried to allocate 2.85 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.31 GiB is free. Including non-PyTorch memory, this process has 42.24 GiB memory in use. Of the allocated memory 38.31 GiB is allocated by PyTorch, and 2.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-15 12:41:28,913] Trial 167 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9693643431263844, 'batch_size': 67, 'attention_heads': 13, 'hidden_dimension': 243, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37152034731807176, 'global_pooling': 'mean', 'learning_rate': 0.0010362383055746729, 'weight_decay': 7.740517489216223e-06, 'beta_0': 0.8581467425942304, 'beta_1': 0.987751094498983, 'epsilon': 2.1883806080517636e-05, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 66 with value: 0.6113475187044284.
[I 2024-11-15 12:59:49,602] Trial 168 finished with value: 0.6139205620388262 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.975728369327387, 'batch_size': 65, 'attention_heads': 13, 'hidden_dimension': 248, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3614353707257939, 'global_pooling': 'mean', 'learning_rate': 0.0017274468985719162, 'weight_decay': 1.2756472949529284e-05, 'beta_0': 0.8657299457964919, 'beta_1': 0.9888549829866481, 'epsilon': 1.9230689619639274e-05, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
CUDA out of memory. Tried to allocate 2.79 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.03 GiB is free. Including non-PyTorch memory, this process has 42.52 GiB memory in use. Of the allocated memory 38.65 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-15 13:12:18,805] Trial 169 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9592216774741634, 'batch_size': 64, 'attention_heads': 13, 'hidden_dimension': 248, 'number_of_hidden_layers': 1, 'dropout_rate': 0.35934390042207165, 'global_pooling': 'mean', 'learning_rate': 0.0018033336312213342, 'weight_decay': 1.095313191581059e-05, 'beta_0': 0.8549233141227364, 'beta_1': 0.9910092981389913, 'epsilon': 1.9492721022228335e-05, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 22, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
[I 2024-11-15 13:32:57,508] Trial 170 finished with value: 0.5642330393453964 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9746438942237134, 'batch_size': 66, 'attention_heads': 14, 'hidden_dimension': 252, 'number_of_hidden_layers': 1, 'dropout_rate': 0.36622678046084767, 'global_pooling': 'mean', 'learning_rate': 0.0047359225025196145, 'weight_decay': 1.535669290571879e-05, 'beta_0': 0.8657081461287953, 'beta_1': 0.9896247696143379, 'epsilon': 2.4734058263315705e-05, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 23, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 168 with value: 0.6139205620388262.
CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 39.72 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-11-15 13:45:28,365] Trial 171 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9678423123041479, 'batch_size': 69, 'attention_heads': 13, 'hidden_dimension': 256, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3510287265341483, 'global_pooling': 'mean', 'learning_rate': 0.0024523983888810248, 'weight_decay': 1.2861372557745855e-05, 'beta_0': 0.8606507837433691, 'beta_1': 0.9887883876911642, 'epsilon': 8.129633916838216e-06, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
[I 2024-11-15 14:02:15,147] Trial 172 finished with value: 0.6050399489128679 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9787161270681489, 'batch_size': 60, 'attention_heads': 12, 'hidden_dimension': 233, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3569192210194168, 'global_pooling': 'mean', 'learning_rate': 0.0012247241462884566, 'weight_decay': 8.59455897874327e-06, 'beta_0': 0.870022733670404, 'beta_1': 0.9882323610220973, 'epsilon': 3.083265094946116e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
[I 2024-11-15 14:25:50,883] Trial 173 finished with value: 0.24510631575183273 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9813050179744067, 'batch_size': 62, 'attention_heads': 13, 'hidden_dimension': 239, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3541755356224842, 'global_pooling': 'mean', 'learning_rate': 0.09285939748894982, 'weight_decay': 9.20042920517433e-06, 'beta_0': 0.8712567920182497, 'beta_1': 0.9889208797047154, 'epsilon': 2.991070373097504e-05, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
[I 2024-11-15 14:43:14,138] Trial 174 finished with value: 0.5954505606920687 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9786925018930295, 'batch_size': 56, 'attention_heads': 13, 'hidden_dimension': 245, 'number_of_hidden_layers': 1, 'dropout_rate': 0.36067396099875865, 'global_pooling': 'mean', 'learning_rate': 0.0012393637361560559, 'weight_decay': 7.720335056410226e-06, 'beta_0': 0.8566053156707352, 'beta_1': 0.9893154673410185, 'epsilon': 1.7109570899049767e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 168 with value: 0.6139205620388262.
