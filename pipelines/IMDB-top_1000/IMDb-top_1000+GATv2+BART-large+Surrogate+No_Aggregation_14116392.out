[I 2024-12-19 04:56:19,463] Using an existing study with name 'IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 906.69 MiB is free. Including non-PyTorch memory, this process has 43.67 GiB memory in use. Of the allocated memory 34.85 GiB is allocated by PyTorch, and 7.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 05:13:12,448] Trial 44 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9881177224843662, 'batch_size': 38, 'attention_heads': 7, 'hidden_dimension': 93, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3006334746373, 'global_pooling': 'mean', 'learning_rate': 0.0007648256118277873, 'weight_decay': 5.716101824155402e-05, 'beta_0': 0.8431302724039046, 'beta_1': 0.9879586807893355, 'epsilon': 2.2784025951542862e-08, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 05:31:12,626] Trial 45 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9929970423338467, 'batch_size': 32, 'attention_heads': 6, 'hidden_dimension': 115, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31942317098499917, 'global_pooling': 'mean', 'learning_rate': 0.0017422889121194072, 'weight_decay': 0.0001724761058624229, 'beta_0': 0.8577736008406707, 'beta_1': 0.9860292915375297, 'epsilon': 2.1905717325642588e-08, 'balanced_loss': False, 'epochs': 126, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 62.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 36.05 GiB is allocated by PyTorch, and 7.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 05:47:56,467] Trial 46 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9753945984030085, 'batch_size': 33, 'attention_heads': 6, 'hidden_dimension': 111, 'number_of_hidden_layers': 1, 'dropout_rate': 0.31559757557756707, 'global_pooling': 'mean', 'learning_rate': 0.0023131637568933705, 'weight_decay': 0.000190817239044443, 'beta_0': 0.8562364009465665, 'beta_1': 0.9894994120829784, 'epsilon': 9.084615622455812e-08, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 60.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 34.33 GiB is allocated by PyTorch, and 9.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 06:04:30,331] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9828670131161286, 'batch_size': 37, 'attention_heads': 5, 'hidden_dimension': 121, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3381143711196285, 'global_pooling': 'mean', 'learning_rate': 0.0009692952104402581, 'weight_decay': 5.917038091864455e-05, 'beta_0': 0.8617020485174959, 'beta_1': 0.9862129766239738, 'epsilon': 2.3932184406632263e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 44.56 GiB of which 706.69 MiB is free. Including non-PyTorch memory, this process has 43.86 GiB memory in use. Of the allocated memory 37.94 GiB is allocated by PyTorch, and 4.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 06:19:40,176] Trial 48 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9695379320734918, 'batch_size': 32, 'attention_heads': 7, 'hidden_dimension': 137, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3104070152465321, 'global_pooling': 'mean', 'learning_rate': 0.006635070671988932, 'weight_decay': 0.0001842388188260007, 'beta_0': 0.8379847104317071, 'beta_1': 0.9884365111732493, 'epsilon': 2.2566650189753347e-08, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 06:37:59,405] Trial 49 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9935565668154643, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 132, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34615499533492056, 'global_pooling': 'mean', 'learning_rate': 0.0016312160785764483, 'weight_decay': 4.3957998077948454e-05, 'beta_0': 0.8735854704380774, 'beta_1': 0.9926795147904548, 'epsilon': 8.643382957303532e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.50 GiB is free. Including non-PyTorch memory, this process has 43.05 GiB memory in use. Of the allocated memory 33.76 GiB is allocated by PyTorch, and 8.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 06:54:35,387] Trial 50 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.987393315499716, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 134, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3447520779049463, 'global_pooling': 'mean', 'learning_rate': 0.0016381425734899489, 'weight_decay': 4.362012693552335e-05, 'beta_0': 0.8701765752071834, 'beta_1': 0.9926874310114354, 'epsilon': 6.99683288181429e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 14, 'plateau_patience': 10, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 348.69 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 35.20 GiB is allocated by PyTorch, and 7.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 07:11:17,997] Trial 51 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.978795845682035, 'batch_size': 36, 'attention_heads': 5, 'hidden_dimension': 90, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32639897204223445, 'global_pooling': 'mean', 'learning_rate': 0.0006336303115984672, 'weight_decay': 9.95140492215099e-05, 'beta_0': 0.8798997918751819, 'beta_1': 0.9961635747421946, 'epsilon': 1.1434383429990622e-07, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 07:29:11,472] Trial 52 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9937333788266155, 'batch_size': 30, 'attention_heads': 7, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3011044042666648, 'global_pooling': 'mean', 'learning_rate': 0.0014798324975375155, 'weight_decay': 2.263230130665984e-05, 'beta_0': 0.8708078777754602, 'beta_1': 0.9898847957297324, 'epsilon': 3.9730122639550835e-08, 'balanced_loss': False, 'epochs': 136, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 07:46:59,308] Trial 53 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9937394288668474, 'batch_size': 29, 'attention_heads': 7, 'hidden_dimension': 100, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3188402298609516, 'global_pooling': 'mean', 'learning_rate': 0.0015553907681279582, 'weight_decay': 2.4057045682681576e-05, 'beta_0': 0.8693469188518693, 'beta_1': 0.9907916692786485, 'epsilon': 3.723251107309695e-08, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 08:04:52,229] Trial 54 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9945282535752589, 'batch_size': 29, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.334378484771524, 'global_pooling': 'mean', 'learning_rate': 0.004823017713680711, 'weight_decay': 2.360585708158479e-05, 'beta_0': 0.8749379882273792, 'beta_1': 0.9921179624421442, 'epsilon': 4.362595780717248e-08, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 08:22:25,000] Trial 55 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9844747020947617, 'batch_size': 30, 'attention_heads': 8, 'hidden_dimension': 51, 'number_of_hidden_layers': 1, 'dropout_rate': 0.300720750791826, 'global_pooling': 'mean', 'learning_rate': 0.0053460981801359405, 'weight_decay': 2.3069162952596643e-05, 'beta_0': 0.8875431563853272, 'beta_1': 0.9920205634603961, 'epsilon': 4.2376784877576056e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 08:40:36,945] Trial 56 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9906347765772442, 'batch_size': 28, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33335471061154737, 'global_pooling': 'mean', 'learning_rate': 0.0025319778604072207, 'weight_decay': 2.3696859031096356e-05, 'beta_0': 0.8648785954114994, 'beta_1': 0.9909568216228901, 'epsilon': 3.5533906856896774e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 09:00:02,255] Trial 57 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9886221725677182, 'batch_size': 29, 'attention_heads': 8, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33116229494687416, 'global_pooling': 'mean', 'learning_rate': 0.003522667421052373, 'weight_decay': 2.499579536201916e-05, 'beta_0': 0.8762976055621937, 'beta_1': 0.9908861862992366, 'epsilon': 1.4760940941260219e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 43.53 GiB memory in use. Of the allocated memory 33.11 GiB is allocated by PyTorch, and 9.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 09:16:58,672] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.988972948033973, 'batch_size': 29, 'attention_heads': 9, 'hidden_dimension': 103, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33279395647330023, 'global_pooling': 'mean', 'learning_rate': 0.003639638052120088, 'weight_decay': 2.476717154160047e-05, 'beta_0': 0.8661570093859902, 'beta_1': 0.9906372548573179, 'epsilon': 1.4042291242777296e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 43.09 GiB memory in use. Of the allocated memory 33.45 GiB is allocated by PyTorch, and 8.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 09:33:40,741] Trial 59 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9811739413463134, 'batch_size': 25, 'attention_heads': 8, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3643011337016171, 'global_pooling': 'mean', 'learning_rate': 0.025525578424119413, 'weight_decay': 1.7978618147045516e-05, 'beta_0': 0.8760896757696972, 'beta_1': 0.9938980194181102, 'epsilon': 3.189726630231956e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 44.56 GiB of which 666.69 MiB is free. Including non-PyTorch memory, this process has 43.90 GiB memory in use. Of the allocated memory 37.11 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 09:44:37,018] Trial 60 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9509537220983371, 'batch_size': 28, 'attention_heads': 9, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33536278130183, 'global_pooling': 'mean', 'learning_rate': 0.0025301621596230924, 'weight_decay': 3.205046650064906e-05, 'beta_0': 0.885452728372733, 'beta_1': 0.9932142289916391, 'epsilon': 5.969668940477536e-08, 'balanced_loss': False, 'epochs': 151, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 43.07 GiB memory in use. Of the allocated memory 35.47 GiB is allocated by PyTorch, and 6.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 09:51:45,508] Trial 61 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9235669672700004, 'batch_size': 30, 'attention_heads': 6, 'hidden_dimension': 81, 'number_of_hidden_layers': 1, 'dropout_rate': 0.31741387983674113, 'global_pooling': 'mean', 'learning_rate': 0.005221594330132262, 'weight_decay': 1.2023295261029092e-05, 'beta_0': 0.8963196368556586, 'beta_1': 0.9907859247987924, 'epsilon': 3.460419898472196e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 10:09:18,808] Trial 62 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9964670438783226, 'batch_size': 28, 'attention_heads': 8, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3282909181982547, 'global_pooling': 'mean', 'learning_rate': 0.00938092650294894, 'weight_decay': 7.152579070005894e-05, 'beta_0': 0.8725930713417179, 'beta_1': 0.9896715596669139, 'epsilon': 1.134460915595345e-08, 'balanced_loss': False, 'epochs': 138, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 10:26:47,848] Trial 63 finished with value: 0.9212121212121213 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9957333390694315, 'batch_size': 26, 'attention_heads': 8, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35171049582563885, 'global_pooling': 'mean', 'learning_rate': 0.014417982191631484, 'weight_decay': 2.0605562903631093e-05, 'beta_0': 0.8707244147632077, 'beta_1': 0.989814291175116, 'epsilon': 1.5472780210145554e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 10, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 10:45:34,730] Trial 64 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9905939046442146, 'batch_size': 29, 'attention_heads': 9, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5907045208146005, 'global_pooling': 'mean', 'learning_rate': 0.00880257442976611, 'weight_decay': 3.817250776871992e-05, 'beta_0': 0.8808667093308392, 'beta_1': 0.9922583190953753, 'epsilon': 1.0700137804725703e-08, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 11:02:44,673] Trial 65 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9964875817182746, 'batch_size': 28, 'attention_heads': 6, 'hidden_dimension': 61, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3287829171832577, 'global_pooling': 'mean', 'learning_rate': 0.0028942789613605135, 'weight_decay': 2.8257506081391008e-05, 'beta_0': 0.8636317707130828, 'beta_1': 0.991245680866271, 'epsilon': 1.6104047164160995e-08, 'balanced_loss': False, 'epochs': 197, 'early_stopping_patience': 13, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 1.23 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 43.34 GiB memory in use. Of the allocated memory 33.88 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 11:19:50,486] Trial 66 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9854664850970911, 'batch_size': 25, 'attention_heads': 8, 'hidden_dimension': 93, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37467759868692124, 'global_pooling': 'sum', 'learning_rate': 0.001345085388188524, 'weight_decay': 7.901489803172158e-05, 'beta_0': 0.8898809211423612, 'beta_1': 0.9909135982944605, 'epsilon': 5.212990331143499e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 3.54 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.80 GiB is free. Including non-PyTorch memory, this process has 42.76 GiB memory in use. Of the allocated memory 35.40 GiB is allocated by PyTorch, and 6.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 11:36:37,793] Trial 67 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9762063265683084, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 120, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34116055281503466, 'global_pooling': 'mean', 'learning_rate': 0.01797522755588587, 'weight_decay': 1.598659629901992e-05, 'beta_0': 0.8751135900969638, 'beta_1': 0.9917475346647788, 'epsilon': 3.432109295997128e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 10, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.40 GiB is free. Including non-PyTorch memory, this process has 42.15 GiB memory in use. Of the allocated memory 34.32 GiB is allocated by PyTorch, and 6.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 11:52:27,464] Trial 68 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9703579277424531, 'batch_size': 31, 'attention_heads': 9, 'hidden_dimension': 86, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3192139090583687, 'global_pooling': 'mean', 'learning_rate': 0.004410071440268588, 'weight_decay': 3.667552544292856e-05, 'beta_0': 0.8676618557409345, 'beta_1': 0.994985148398215, 'epsilon': 1.7301494067338625e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 5}. Best is trial 41 with value: 0.9575757575757575.
CUDA out of memory. Tried to allocate 918.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 296.69 MiB is free. Including non-PyTorch memory, this process has 44.26 GiB memory in use. Of the allocated memory 37.87 GiB is allocated by PyTorch, and 5.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 12:02:00,757] Trial 69 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9377753950203856, 'batch_size': 22, 'attention_heads': 8, 'hidden_dimension': 70, 'number_of_hidden_layers': 1, 'dropout_rate': 0.394036586596602, 'global_pooling': 'mean', 'learning_rate': 0.03257844632023736, 'weight_decay': 1.261337897803542e-05, 'beta_0': 0.8527983401887529, 'beta_1': 0.9899514135071331, 'epsilon': 3.0776942821477486e-08, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 12, 'plateau_patience': 12, 'plateau_divider': 4}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 12:18:50,696] Trial 70 finished with value: 0.9454545454545454 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.996514287796033, 'batch_size': 26, 'attention_heads': 4, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31428683937524454, 'global_pooling': 'mean', 'learning_rate': 0.007489607762398565, 'weight_decay': 9.55539300978471e-06, 'beta_0': 0.8720728523204597, 'beta_1': 0.9932894683605376, 'epsilon': 4.5183243955960984e-08, 'balanced_loss': False, 'epochs': 151, 'early_stopping_patience': 10, 'plateau_patience': 13, 'plateau_divider': 3}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-19 12:37:48,943] Trial 71 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9826658840512809, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 77, 'number_of_hidden_layers': 2, 'dropout_rate': 0.327821664978777, 'global_pooling': 'sum', 'learning_rate': 0.0019816078609499813, 'weight_decay': 0.0001369826264259152, 'beta_0': 0.8799028754140509, 'beta_1': 0.9889159082580817, 'epsilon': 1.0231068334359036e-08, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 14, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 12:55:25,100] Trial 72 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9901204062370762, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 55, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32801395084718143, 'global_pooling': 'sum', 'learning_rate': 0.0019362326103212462, 'weight_decay': 0.0001416156895920045, 'beta_0': 0.8821474958058073, 'beta_1': 0.9890801898421471, 'epsilon': 1.025718144209731e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
CUDA out of memory. Tried to allocate 1.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1000.69 MiB is free. Including non-PyTorch memory, this process has 43.58 GiB memory in use. Of the allocated memory 36.48 GiB is allocated by PyTorch, and 5.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 13:12:09,472] Trial 73 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9821905068160403, 'batch_size': 28, 'attention_heads': 7, 'hidden_dimension': 80, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3516834143047512, 'global_pooling': 'sum', 'learning_rate': 0.0012210764850434056, 'weight_decay': 7.979911508388777e-05, 'beta_0': 0.877294248304704, 'beta_1': 0.9897663833113928, 'epsilon': 1.3679809492278247e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 13:31:36,084] Trial 74 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9864176296877104, 'batch_size': 27, 'attention_heads': 5, 'hidden_dimension': 107, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3355701865210251, 'global_pooling': 'sum', 'learning_rate': 0.0005904887420959996, 'weight_decay': 1.9956480124898982e-05, 'beta_0': 0.8644007837474138, 'beta_1': 0.988712277998027, 'epsilon': 1.83418827783438e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 13:49:58,974] Trial 75 finished with value: 0.9333333333333333 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9864667313896676, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3114080100656722, 'global_pooling': 'sum', 'learning_rate': 0.0006563441831072503, 'weight_decay': 0.00029770621001803477, 'beta_0': 0.8639464276907689, 'beta_1': 0.988647700200435, 'epsilon': 1.758663326060946e-08, 'balanced_loss': False, 'epochs': 192, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 14:07:53,442] Trial 76 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9909048362596014, 'batch_size': 22, 'attention_heads': 5, 'hidden_dimension': 88, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34083903495754997, 'global_pooling': 'sum', 'learning_rate': 0.00043599090460522285, 'weight_decay': 2.630180002191746e-05, 'beta_0': 0.8677526876586713, 'beta_1': 0.991266457611746, 'epsilon': 2.7695807249766524e-08, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 14:25:50,908] Trial 77 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9868995628287317, 'batch_size': 21, 'attention_heads': 5, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3645392101240925, 'global_pooling': 'sum', 'learning_rate': 0.00047650625417678104, 'weight_decay': 1.952099573462978e-05, 'beta_0': 0.8913555227426695, 'beta_1': 0.9875212777474093, 'epsilon': 6.204134007519407e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 14:45:43,026] Trial 78 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9831855884085449, 'batch_size': 25, 'attention_heads': 6, 'hidden_dimension': 89, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3413704756406397, 'global_pooling': 'sum', 'learning_rate': 0.0012307757830800838, 'weight_decay': 0.0007639240316304, 'beta_0': 0.8466528705304281, 'beta_1': 0.9868927420146328, 'epsilon': 1.9155634273237005e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 13, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 15:04:39,212] Trial 79 finished with value: 0.9575757575757575 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9782631781982173, 'batch_size': 22, 'attention_heads': 6, 'hidden_dimension': 76, 'number_of_hidden_layers': 1, 'dropout_rate': 0.34343834418705504, 'global_pooling': 'sum', 'learning_rate': 0.0003356088966321859, 'weight_decay': 0.0009550562035090517, 'beta_0': 0.8479633169692076, 'beta_1': 0.9874221958562132, 'epsilon': 2.6825962450068438e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 13, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
CUDA out of memory. Tried to allocate 1.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 43.33 GiB memory in use. Of the allocated memory 34.68 GiB is allocated by PyTorch, and 7.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 15:20:37,948] Trial 80 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9782215532885492, 'batch_size': 22, 'attention_heads': 6, 'hidden_dimension': 89, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3788756712891915, 'global_pooling': 'sum', 'learning_rate': 0.0003149177886800246, 'weight_decay': 0.0009184246923462755, 'beta_0': 0.8485857780574031, 'beta_1': 0.9868633984936963, 'epsilon': 2.8303102932608446e-08, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 13, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 15:38:22,256] Trial 81 finished with value: 0.9454545454545454 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9716479417679595, 'batch_size': 19, 'attention_heads': 5, 'hidden_dimension': 73, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3561311895115861, 'global_pooling': 'sum', 'learning_rate': 0.00025585440731071236, 'weight_decay': 0.0004465493085130717, 'beta_0': 0.8568897587473053, 'beta_1': 0.9865842018981771, 'epsilon': 1.920129540318089e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 14, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 15:54:59,589] Trial 82 finished with value: 0.9515151515151515 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.983773532285267, 'batch_size': 24, 'attention_heads': 4, 'hidden_dimension': 81, 'number_of_hidden_layers': 1, 'dropout_rate': 0.34197209449810156, 'global_pooling': 'sum', 'learning_rate': 0.0003772032255391781, 'weight_decay': 0.0007485888398083664, 'beta_0': 0.8440700675146043, 'beta_1': 0.9873817190207903, 'epsilon': 1.2691275460112478e-08, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 16:11:15,241] Trial 83 finished with value: 0.9515151515151515 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9821130773160887, 'batch_size': 25, 'attention_heads': 5, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32070622670968346, 'global_pooling': 'sum', 'learning_rate': 0.0005441112726900706, 'weight_decay': 0.0005969480066024786, 'beta_0': 0.8546329179644951, 'beta_1': 0.9887796139343511, 'epsilon': 2.6463431279046195e-08, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
CUDA out of memory. Tried to allocate 746.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 214.69 MiB is free. Including non-PyTorch memory, this process has 44.34 GiB memory in use. Of the allocated memory 40.21 GiB is allocated by PyTorch, and 2.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 16:25:36,974] Trial 84 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9596277968215378, 'batch_size': 22, 'attention_heads': 6, 'hidden_dimension': 108, 'number_of_hidden_layers': 3, 'dropout_rate': 0.31107676955695707, 'global_pooling': 'sum', 'learning_rate': 0.0001839115056634635, 'weight_decay': 0.0004610667842312402, 'beta_0': 0.8457969979883324, 'beta_1': 0.9877379587056297, 'epsilon': 2.014819317927935e-08, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 44.56 GiB of which 752.69 MiB is free. Including non-PyTorch memory, this process has 43.82 GiB memory in use. Of the allocated memory 36.82 GiB is allocated by PyTorch, and 5.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 16:42:08,270] Trial 85 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.977228814523349, 'batch_size': 27, 'attention_heads': 6, 'hidden_dimension': 78, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3433241064239608, 'global_pooling': 'sum', 'learning_rate': 0.0013249496648981883, 'weight_decay': 0.00030093727195295466, 'beta_0': 0.8610252485962568, 'beta_1': 0.9904399792650923, 'epsilon': 1.3441814387680309e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 17:00:24,454] Trial 86 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9912590021962565, 'batch_size': 20, 'attention_heads': 5, 'hidden_dimension': 89, 'number_of_hidden_layers': 2, 'dropout_rate': 0.348222318838882, 'global_pooling': 'sum', 'learning_rate': 0.0011128987450437166, 'weight_decay': 2.945626653606533e-05, 'beta_0': 0.8685631526699201, 'beta_1': 0.9853541936971968, 'epsilon': 4.030895489618876e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 17, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 43.18 GiB memory in use. Of the allocated memory 34.12 GiB is allocated by PyTorch, and 7.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 17:15:52,965] Trial 87 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9734932137690386, 'batch_size': 17, 'attention_heads': 5, 'hidden_dimension': 88, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3487200033896548, 'global_pooling': 'sum', 'learning_rate': 0.0007632562453440794, 'weight_decay': 0.0009389795291658615, 'beta_0': 0.8502435303165419, 'beta_1': 0.9849111485193138, 'epsilon': 1.8015459739662115e-07, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 18, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 17:34:05,233] Trial 88 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9884515569475815, 'batch_size': 21, 'attention_heads': 4, 'hidden_dimension': 93, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3692402784598809, 'global_pooling': 'sum', 'learning_rate': 0.0011174126171806344, 'weight_decay': 2.890917337078572e-05, 'beta_0': 0.8462817212647368, 'beta_1': 0.9843460641084039, 'epsilon': 2.664092035060143e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 17:53:50,103] Trial 89 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9910078121223689, 'batch_size': 20, 'attention_heads': 6, 'hidden_dimension': 122, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38493506096969493, 'global_pooling': 'sum', 'learning_rate': 0.0006317212165262386, 'weight_decay': 4.84861467096641e-05, 'beta_0': 0.883102306404065, 'beta_1': 0.985471031069567, 'epsilon': 1.8793119506872086e-08, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 18:09:58,148] Trial 90 finished with value: 0.9333333333333333 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9997638023129956, 'batch_size': 18, 'attention_heads': 5, 'hidden_dimension': 76, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3587656061991806, 'global_pooling': 'sum', 'learning_rate': 0.0033570618254357503, 'weight_decay': 0.0007385785824143541, 'beta_0': 0.8672199557648904, 'beta_1': 0.9811900582045325, 'epsilon': 5.126100302278847e-08, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 15, 'plateau_patience': 21, 'plateau_divider': 3}. Best is trial 71 with value: 0.9636363636363636.
[I 2024-12-19 18:30:04,003] Trial 91 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9805396659199135, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 70, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32457105015969634, 'global_pooling': 'sum', 'learning_rate': 0.001994629726825529, 'weight_decay': 0.00025111016335547197, 'beta_0': 0.8587254568877359, 'beta_1': 0.9891567615044861, 'epsilon': 6.956555711019252e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 12, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-19 18:48:33,612] Trial 92 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.98563492866972, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 68, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32415702575332833, 'global_pooling': 'sum', 'learning_rate': 0.0019621547919592107, 'weight_decay': 3.8940162305330745e-05, 'beta_0': 0.8600732154594437, 'beta_1': 0.9892111997016192, 'epsilon': 6.941904011337849e-08, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 12, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 43.12 GiB memory in use. Of the allocated memory 33.60 GiB is allocated by PyTorch, and 8.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 19:05:37,473] Trial 93 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9804832032908777, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 149, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3077953183586889, 'global_pooling': 'sum', 'learning_rate': 0.0009969779996402005, 'weight_decay': 0.00023387053615124098, 'beta_0': 0.8529088239672675, 'beta_1': 0.9883784821784037, 'epsilon': 4.0456767791105994e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 2.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.71 GiB is free. Including non-PyTorch memory, this process has 42.85 GiB memory in use. Of the allocated memory 36.58 GiB is allocated by PyTorch, and 5.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 19:21:07,317] Trial 94 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9675981722632452, 'batch_size': 26, 'attention_heads': 7, 'hidden_dimension': 86, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33926528250147214, 'global_pooling': 'sum', 'learning_rate': 0.0004010876754712955, 'weight_decay': 0.00011316810348640121, 'beta_0': 0.8795537734160788, 'beta_1': 0.986532211172728, 'epsilon': 1.6065673085997414e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 13, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-19 19:40:04,205] Trial 95 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9835763236096686, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 58, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5292731102343491, 'global_pooling': 'sum', 'learning_rate': 0.0007854879951852831, 'weight_decay': 0.0004627534111129295, 'beta_0': 0.8626791482024149, 'beta_1': 0.9986428252915244, 'epsilon': 2.6760749177178507e-08, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-19 19:59:17,374] Trial 96 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.979572427302399, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.555189125699272, 'global_pooling': 'sum', 'learning_rate': 0.0005036429067105568, 'weight_decay': 0.00042827260413855363, 'beta_0': 0.8581754593504044, 'beta_1': 0.998656364440037, 'epsilon': 2.3884896465089054e-08, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 986.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 278.69 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 36.54 GiB is allocated by PyTorch, and 6.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-19 20:16:40,581] Trial 97 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9755227337096216, 'batch_size': 25, 'attention_heads': 6, 'hidden_dimension': 58, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5304868227779416, 'global_pooling': 'sum', 'learning_rate': 0.0007817627914206644, 'weight_decay': 0.0005824418147908076, 'beta_0': 0.86291677277793, 'beta_1': 0.9972164964961376, 'epsilon': 1.0397173379241684e-07, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 11, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-19 20:35:12,779] Trial 98 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9877516519226758, 'batch_size': 27, 'attention_heads': 7, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5449546923795, 'global_pooling': 'sum', 'learning_rate': 0.0002056529840939668, 'weight_decay': 0.000348936658150446, 'beta_0': 0.838439841029056, 'beta_1': 0.9880503158596896, 'epsilon': 2.878493546648935e-08, 'balanced_loss': False, 'epochs': 134, 'early_stopping_patience': 10, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-19 20:54:02,073] Trial 99 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9830393390937002, 'batch_size': 22, 'attention_heads': 6, 'hidden_dimension': 51, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4996742675692636, 'global_pooling': 'sum', 'learning_rate': 0.00029870131923714617, 'weight_decay': 0.00023164334782214665, 'beta_0': 0.8486229330963181, 'beta_1': 0.9892725838155371, 'epsilon': 1.962675579087283e-08, 'balanced_loss': False, 'epochs': 122, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
slurmstepd: error: *** JOB 14116392 ON gpu035 CANCELLED AT 2024-12-19T20:56:26 DUE TO TIME LIMIT ***
