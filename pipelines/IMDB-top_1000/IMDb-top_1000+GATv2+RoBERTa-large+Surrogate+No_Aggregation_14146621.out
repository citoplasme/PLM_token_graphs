Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2024-12-24 04:29:28,076] Using an existing study with name 'IMDb-top_1000-GATv2-FacebookAI-roberta-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors
[I 2024-12-24 04:57:40,403] Trial 272 finished with value: 0.896969696969697 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9954212426813942, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 205, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41388127302606703, 'global_pooling': 'mean', 'learning_rate': 3.669591461834728e-05, 'weight_decay': 2.0435839230284137e-05, 'beta_0': 0.8496266956320686, 'beta_1': 0.9894707432155797, 'epsilon': 1.8062501742923424e-08, 'balanced_loss': True, 'epochs': 188, 'early_stopping_patience': 14, 'plateau_patience': 21, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-24 04:57:41,251] Trial 273 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9995711376152902, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 210, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5998087315924654, 'global_pooling': 'mean', 'learning_rate': 0.001548012519302409, 'weight_decay': 9.868934246279496e-05, 'beta_0': 0.8398484772573853, 'beta_1': 0.9821094518512538, 'epsilon': 2.5806273896325875e-08, 'balanced_loss': True, 'epochs': 191, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 05:23:20,757] Trial 274 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9969244803070372, 'batch_size': 37, 'attention_heads': 7, 'hidden_dimension': 196, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37912163220027006, 'global_pooling': 'mean', 'learning_rate': 0.0010187121792315159, 'weight_decay': 2.349340681021374e-05, 'beta_0': 0.8459826113992205, 'beta_1': 0.9879785872044856, 'epsilon': 2.1968580231632672e-08, 'balanced_loss': True, 'epochs': 158, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 05:40:51,657] Trial 275 finished with value: 0.9272727272727272 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9947845320096784, 'batch_size': 44, 'attention_heads': 8, 'hidden_dimension': 213, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3598275981968413, 'global_pooling': 'mean', 'learning_rate': 0.0006512321167433667, 'weight_decay': 1.1752003418812842e-05, 'beta_0': 0.8422025868553713, 'beta_1': 0.9892859479914619, 'epsilon': 1.1549616536607413e-08, 'balanced_loss': True, 'epochs': 185, 'early_stopping_patience': 12, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 06:07:39,274] Trial 276 finished with value: 0.9454545454545454 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9930093781949516, 'batch_size': 43, 'attention_heads': 9, 'hidden_dimension': 185, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43071759075247334, 'global_pooling': 'mean', 'learning_rate': 0.001233413287302976, 'weight_decay': 0.00011804821848862406, 'beta_0': 0.837685328960352, 'beta_1': 0.9964105427948954, 'epsilon': 1.8294549307553426e-08, 'balanced_loss': True, 'epochs': 181, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 788.69 MiB is free. Including non-PyTorch memory, this process has 43.78 GiB memory in use. Of the allocated memory 37.44 GiB is allocated by PyTorch, and 5.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 06:31:24,279] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9898407025295743, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 171, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43962734982768675, 'global_pooling': 'mean', 'learning_rate': 0.0017436790119334973, 'weight_decay': 2.6554773457249176e-05, 'beta_0': 0.8370033538174302, 'beta_1': 0.9961316752007355, 'epsilon': 1.715372719706679e-08, 'balanced_loss': True, 'epochs': 181, 'early_stopping_patience': 16, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 06:58:51,609] Trial 278 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9919829898822481, 'batch_size': 42, 'attention_heads': 9, 'hidden_dimension': 158, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43043273215149014, 'global_pooling': 'mean', 'learning_rate': 0.0012867687413239898, 'weight_decay': 0.00011903316682372935, 'beta_0': 0.8376363072323403, 'beta_1': 0.9800830550712508, 'epsilon': 1.4491954878818575e-08, 'balanced_loss': True, 'epochs': 178, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 7.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 560.69 MiB is free. Including non-PyTorch memory, this process has 44.01 GiB memory in use. Of the allocated memory 37.54 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 07:26:42,066] Trial 279 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9881531938761768, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.423338943703572, 'global_pooling': 'mean', 'learning_rate': 0.0009396713107909767, 'weight_decay': 9.77063958433665e-05, 'beta_0': 0.833700490932035, 'beta_1': 0.9972509634164366, 'epsilon': 1.017857986059912e-08, 'balanced_loss': True, 'epochs': 181, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 07:53:04,331] Trial 280 finished with value: 0.8666666666666667 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9934651459747954, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 184, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4345842197383902, 'global_pooling': 'mean', 'learning_rate': 0.014780080015112867, 'weight_decay': 1.776496496331644e-05, 'beta_0': 0.8503530011398174, 'beta_1': 0.9965050078302599, 'epsilon': 2.0461427953955227e-08, 'balanced_loss': True, 'epochs': 193, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 2.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 94.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 35.28 GiB is allocated by PyTorch, and 8.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 08:17:14,152] Trial 281 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9914760306222239, 'batch_size': 45, 'attention_heads': 9, 'hidden_dimension': 179, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3833292554403892, 'global_pooling': 'mean', 'learning_rate': 0.001234518405732784, 'weight_decay': 3.1081106109394935e-05, 'beta_0': 0.8572776772957839, 'beta_1': 0.9958420174576725, 'epsilon': 3.003230823830668e-08, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 14, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 08:42:59,684] Trial 282 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9948376357140064, 'batch_size': 41, 'attention_heads': 6, 'hidden_dimension': 188, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3687575586839561, 'global_pooling': 'mean', 'learning_rate': 0.002151126269002272, 'weight_decay': 0.00013662561328989698, 'beta_0': 0.8404644698711415, 'beta_1': 0.9950531182307559, 'epsilon': 1.3182738610903892e-08, 'balanced_loss': True, 'epochs': 129, 'early_stopping_patience': 12, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 09:03:27,793] Trial 283 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9900626426137533, 'batch_size': 44, 'attention_heads': 6, 'hidden_dimension': 147, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38864016057652623, 'global_pooling': 'mean', 'learning_rate': 0.0014838470832097214, 'weight_decay': 0.0001532657906407275, 'beta_0': 0.8451561721494221, 'beta_1': 0.9964060255211681, 'epsilon': 1.8361618563914673e-08, 'balanced_loss': True, 'epochs': 185, 'early_stopping_patience': 10, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.27 GiB is free. Including non-PyTorch memory, this process has 40.28 GiB memory in use. Of the allocated memory 28.34 GiB is allocated by PyTorch, and 10.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 09:25:30,654] Trial 284 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9873226557387961, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 182, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40216408524384234, 'global_pooling': 'mean', 'learning_rate': 0.0011249041622206784, 'weight_decay': 2.525210095234769e-05, 'beta_0': 0.8364918353780981, 'beta_1': 0.9971307653241858, 'epsilon': 4.6395775311337434e-08, 'balanced_loss': True, 'epochs': 190, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 09:48:08,841] Trial 285 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.992960545654387, 'batch_size': 39, 'attention_heads': 6, 'hidden_dimension': 199, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42678344565171517, 'global_pooling': 'mean', 'learning_rate': 0.0008385982024366882, 'weight_decay': 0.00021380680024586123, 'beta_0': 0.8551013458018368, 'beta_1': 0.997801495614573, 'epsilon': 2.2884959052524397e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 10:11:55,880] Trial 286 finished with value: 0.9151515151515152 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9976154093282933, 'batch_size': 43, 'attention_heads': 8, 'hidden_dimension': 165, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37329853316857997, 'global_pooling': 'mean', 'learning_rate': 0.0017516869475011445, 'weight_decay': 3.4374678714291176e-05, 'beta_0': 0.8478907838732926, 'beta_1': 0.9968513366553581, 'epsilon': 4.942151838059908e-07, 'balanced_loss': True, 'epochs': 200, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 10:34:49,066] Trial 287 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9942654770674413, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 221, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35680443947546475, 'global_pooling': 'mean', 'learning_rate': 0.0009707112625730592, 'weight_decay': 2.857806288592182e-05, 'beta_0': 0.8389849906799789, 'beta_1': 0.9904592090462373, 'epsilon': 3.671465376310327e-08, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 11, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 194.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 40.55 GiB is allocated by PyTorch, and 2.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 10:56:53,764] Trial 288 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9835819447643863, 'batch_size': 31, 'attention_heads': 8, 'hidden_dimension': 141, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4156137059800055, 'global_pooling': 'mean', 'learning_rate': 0.0006941889632872278, 'weight_decay': 0.0001682928143729698, 'beta_0': 0.860260475881337, 'beta_1': 0.9965592967566658, 'epsilon': 1.6062809588526633e-08, 'balanced_loss': True, 'epochs': 188, 'early_stopping_patience': 13, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 11:16:47,401] Trial 289 finished with value: 0.8 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9999037795900441, 'batch_size': 42, 'attention_heads': 8, 'hidden_dimension': 191, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4043366600768847, 'global_pooling': 'max', 'learning_rate': 2.677729565995597e-05, 'weight_decay': 6.962811834419689e-05, 'beta_0': 0.862636563224284, 'beta_1': 0.9959873999089122, 'epsilon': 2.7823620373942067e-08, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 11:38:20,173] Trial 290 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9910947493708784, 'batch_size': 45, 'attention_heads': 7, 'hidden_dimension': 153, 'number_of_hidden_layers': 2, 'dropout_rate': 0.386733632253354, 'global_pooling': 'mean', 'learning_rate': 0.0012549844161030836, 'weight_decay': 5.628973753286495e-05, 'beta_0': 0.8432225817485386, 'beta_1': 0.9953730005102751, 'epsilon': 1.9279122092876867e-08, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 11:58:37,951] Trial 291 finished with value: 0.8363636363636363 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9963814773969478, 'batch_size': 35, 'attention_heads': 5, 'hidden_dimension': 177, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3930682963741591, 'global_pooling': 'sum', 'learning_rate': 0.0008765245249954408, 'weight_decay': 0.00011124202083428062, 'beta_0': 0.8456675244213934, 'beta_1': 0.9897662918339252, 'epsilon': 2.4531314278626145e-08, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 12:21:09,652] Trial 292 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9936906435714585, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 161, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3981637430843832, 'global_pooling': 'mean', 'learning_rate': 0.0005946967812319244, 'weight_decay': 2.1380849593341515e-05, 'beta_0': 0.8413368506908457, 'beta_1': 0.9887935033670361, 'epsilon': 1.3267024109975016e-08, 'balanced_loss': True, 'epochs': 191, 'early_stopping_patience': 14, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 7.69 GiB is free. Including non-PyTorch memory, this process has 36.86 GiB memory in use. Of the allocated memory 26.45 GiB is allocated by PyTorch, and 9.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 12:43:10,447] Trial 293 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9890052799419999, 'batch_size': 44, 'attention_heads': 7, 'hidden_dimension': 185, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3802382767553076, 'global_pooling': 'mean', 'learning_rate': 0.001069650474600442, 'weight_decay': 8.490333352772613e-05, 'beta_0': 0.8513371254632007, 'beta_1': 0.9981486577560922, 'epsilon': 3.080304758001393e-08, 'balanced_loss': True, 'epochs': 186, 'early_stopping_patience': 10, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 13:01:51,142] Trial 294 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9978248393774549, 'batch_size': 47, 'attention_heads': 6, 'hidden_dimension': 145, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41850534746769485, 'global_pooling': 'mean', 'learning_rate': 0.0014720578503147725, 'weight_decay': 3.7519970350468296e-05, 'beta_0': 0.8438128762880974, 'beta_1': 0.9962485969831583, 'epsilon': 2.0579072621634223e-08, 'balanced_loss': True, 'epochs': 175, 'early_stopping_patience': 12, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 44.56 GiB of which 654.69 MiB is free. Including non-PyTorch memory, this process has 43.91 GiB memory in use. Of the allocated memory 37.59 GiB is allocated by PyTorch, and 5.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 13:10:32,156] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9347651233298508, 'batch_size': 41, 'attention_heads': 8, 'hidden_dimension': 235, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4071016365033712, 'global_pooling': 'mean', 'learning_rate': 0.0007616033810403103, 'weight_decay': 3.1529935592007495e-05, 'beta_0': 0.8483831300822969, 'beta_1': 0.9885863660905533, 'epsilon': 1.650379297244205e-08, 'balanced_loss': True, 'epochs': 196, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 72.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 36.32 GiB is allocated by PyTorch, and 7.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 13:32:42,744] Trial 296 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9920184232639828, 'batch_size': 43, 'attention_heads': 8, 'hidden_dimension': 216, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40071816409639455, 'global_pooling': 'mean', 'learning_rate': 0.0012370371597453145, 'weight_decay': 2.33566243686669e-05, 'beta_0': 0.8465518708510853, 'beta_1': 0.9922288214085544, 'epsilon': 3.925972269547421e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 13:59:49,683] Trial 297 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9949758523107411, 'batch_size': 46, 'attention_heads': 7, 'hidden_dimension': 157, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3907377769563929, 'global_pooling': 'mean', 'learning_rate': 0.0005662017550700533, 'weight_decay': 8.2055815516203e-06, 'beta_0': 0.8390300203024296, 'beta_1': 0.9873834887980782, 'epsilon': 2.5499868648359696e-08, 'balanced_loss': True, 'epochs': 190, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 14:20:40,204] Trial 298 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9961875030291223, 'batch_size': 45, 'attention_heads': 8, 'hidden_dimension': 151, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4448033435256357, 'global_pooling': 'mean', 'learning_rate': 0.002089653242920682, 'weight_decay': 2.7057501236373596e-05, 'beta_0': 0.835472556443493, 'beta_1': 0.9891226313752091, 'epsilon': 2.157157754100782e-08, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 14:42:28,734] Trial 299 finished with value: 0.9393939393939394 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9928508399532623, 'batch_size': 58, 'attention_heads': 9, 'hidden_dimension': 168, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3750249905019236, 'global_pooling': 'mean', 'learning_rate': 0.0008863335618445915, 'weight_decay': 1.8910964802191776e-05, 'beta_0': 0.8418699127226622, 'beta_1': 0.9975422279163857, 'epsilon': 3.074878862988069e-08, 'balanced_loss': True, 'epochs': 94, 'early_stopping_patience': 14, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 38.69 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 15:00:21,266] Trial 300 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9864146630601036, 'batch_size': 40, 'attention_heads': 11, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5884039649668332, 'global_pooling': 'mean', 'learning_rate': 0.0007133172839780944, 'weight_decay': 6.9727836550752835e-06, 'beta_0': 0.8451053872170106, 'beta_1': 0.9881452025641276, 'epsilon': 1.4828781137541539e-08, 'balanced_loss': True, 'epochs': 181, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 4}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 43.42 GiB memory in use. Of the allocated memory 34.34 GiB is allocated by PyTorch, and 7.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 15:34:29,047] Trial 301 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9896991767019938, 'batch_size': 33, 'attention_heads': 6, 'hidden_dimension': 138, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4113404577355812, 'global_pooling': 'mean', 'learning_rate': 0.0010264628243707831, 'weight_decay': 1.5545003756849925e-05, 'beta_0': 0.8496907523919618, 'beta_1': 0.9968332578472079, 'epsilon': 1.1901228890254077e-08, 'balanced_loss': True, 'epochs': 90, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 226.69 MiB is free. Including non-PyTorch memory, this process has 44.33 GiB memory in use. Of the allocated memory 35.73 GiB is allocated by PyTorch, and 7.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 15:44:53,084] Trial 302 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.950148080891, 'batch_size': 48, 'attention_heads': 7, 'hidden_dimension': 147, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39675724825110065, 'global_pooling': 'mean', 'learning_rate': 0.0014706871488900553, 'weight_decay': 4.2352129718717224e-05, 'beta_0': 0.8408182283031711, 'beta_1': 0.9955140924529313, 'epsilon': 1.6959511780412976e-08, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 12, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 16:05:45,983] Trial 303 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.998053916443886, 'batch_size': 49, 'attention_heads': 8, 'hidden_dimension': 203, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3858703204811117, 'global_pooling': 'mean', 'learning_rate': 0.0006150675175151146, 'weight_decay': 4.98253644959487e-05, 'beta_0': 0.8442240359169628, 'beta_1': 0.988438439183462, 'epsilon': 2.547434990694406e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 16:29:34,405] Trial 304 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.994777177056023, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 163, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42250958583302245, 'global_pooling': 'mean', 'learning_rate': 0.0007603709916817322, 'weight_decay': 3.52650881343506e-05, 'beta_0': 0.8321193826290735, 'beta_1': 0.9965359628455591, 'epsilon': 2.130622072576901e-08, 'balanced_loss': True, 'epochs': 198, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 16:49:10,062] Trial 305 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.991183218974762, 'batch_size': 44, 'attention_heads': 6, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3399324427976282, 'global_pooling': 'mean', 'learning_rate': 0.0010976516468417963, 'weight_decay': 2.938422389264775e-05, 'beta_0': 0.8481458324960531, 'beta_1': 0.9877129683207173, 'epsilon': 3.5252571507277044e-08, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 13, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 17:09:22,611] Trial 306 finished with value: 0.9212121212121213 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9934742094864758, 'batch_size': 56, 'attention_heads': 8, 'hidden_dimension': 142, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4051313551136268, 'global_pooling': 'mean', 'learning_rate': 0.0017029524087396309, 'weight_decay': 2.467985103862618e-05, 'beta_0': 0.8381051930625727, 'beta_1': 0.9959760410781704, 'epsilon': 8.816625312323643e-08, 'balanced_loss': True, 'epochs': 191, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 17:36:14,706] Trial 307 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9961636906456004, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 209, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3817707533769937, 'global_pooling': 'mean', 'learning_rate': 0.0004301388767047399, 'weight_decay': 4.2652150493373315e-05, 'beta_0': 0.8435302990286049, 'beta_1': 0.9972366564732046, 'epsilon': 1.0110364934872515e-08, 'balanced_loss': True, 'epochs': 183, 'early_stopping_patience': 11, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 17:56:16,485] Trial 308 finished with value: 0.9030303030303031 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9999075072052503, 'batch_size': 46, 'attention_heads': 7, 'hidden_dimension': 123, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3938757615297075, 'global_pooling': 'mean', 'learning_rate': 0.0008826146211299555, 'weight_decay': 3.240681017810254e-05, 'beta_0': 0.8465464202635244, 'beta_1': 0.9889221913329735, 'epsilon': 1.801931780018551e-08, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 18:17:26,309] Trial 309 finished with value: 0.7212121212121212 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9921497139875768, 'batch_size': 43, 'attention_heads': 9, 'hidden_dimension': 174, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3653336242594635, 'global_pooling': 'mean', 'learning_rate': 0.0013089071719152798, 'weight_decay': 2.233781838183358e-05, 'beta_0': 0.8520949529765923, 'beta_1': 0.9911342570470759, 'epsilon': 2.9044242636116982e-08, 'balanced_loss': True, 'epochs': 188, 'early_stopping_patience': 12, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 1.70 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 43.21 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-24 18:35:56,149] Trial 310 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9882963442576453, 'batch_size': 45, 'attention_heads': 8, 'hidden_dimension': 154, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4305900681807629, 'global_pooling': 'mean', 'learning_rate': 0.000491490804124765, 'weight_decay': 6.015912439589707e-05, 'beta_0': 0.8422476683201471, 'beta_1': 0.9869392257004505, 'epsilon': 1.4023262137752432e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 10, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 18:56:25,518] Trial 311 finished with value: 0.9454545454545454 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9975375270532549, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 136, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4024596387697741, 'global_pooling': 'max', 'learning_rate': 0.0007066664712609779, 'weight_decay': 2.850681212710585e-05, 'beta_0': 0.839822876132509, 'beta_1': 0.9934138495808492, 'epsilon': 2.400984949041031e-08, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 19:21:35,744] Trial 312 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9979156827250777, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 136, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4000290026174495, 'global_pooling': 'max', 'learning_rate': 0.0007634644151072595, 'weight_decay': 2.6921191875671674e-05, 'beta_0': 0.8362280605739751, 'beta_1': 0.9943532790154133, 'epsilon': 1.9438025675626867e-08, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 19:44:32,387] Trial 313 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9941440766684163, 'batch_size': 39, 'attention_heads': 7, 'hidden_dimension': 224, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38872367722828244, 'global_pooling': 'max', 'learning_rate': 0.0009644601818371509, 'weight_decay': 2.039961774857897e-05, 'beta_0': 0.8643574530618969, 'beta_1': 0.9947974216615394, 'epsilon': 2.6093878017680184e-08, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 20:05:17,871] Trial 314 finished with value: 0.593939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9903077988734903, 'batch_size': 41, 'attention_heads': 6, 'hidden_dimension': 148, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3934796507052574, 'global_pooling': 'sum', 'learning_rate': 0.0006731623287003786, 'weight_decay': 3.038811635809619e-05, 'beta_0': 0.8393881062309956, 'beta_1': 0.9962188812237182, 'epsilon': 4.515947298166202e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-24 20:26:38,205] Trial 315 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9966346778844433, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 141, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4034924642116026, 'global_pooling': 'max', 'learning_rate': 0.0010998538012667249, 'weight_decay': 2.7085396050136547e-05, 'beta_0': 0.8538947685846614, 'beta_1': 0.9935260304924075, 'epsilon': 3.5081841624789893e-08, 'balanced_loss': True, 'epochs': 147, 'early_stopping_patience': 13, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-24 20:26:39,011] Trial 316 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9984246829105777, 'batch_size': 41, 'attention_heads': 9, 'hidden_dimension': 197, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4144846659173872, 'global_pooling': 'max', 'learning_rate': 0.0008286429802417557, 'weight_decay': 3.6678583397177244e-05, 'beta_0': 0.8479755152065318, 'beta_1': 0.9894892068732569, 'epsilon': 2.213693896045259e-08, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 13, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
slurmstepd: error: *** JOB 14146621 ON gpu007 CANCELLED AT 2024-12-24T20:29:26 DUE TO TIME LIMIT ***
