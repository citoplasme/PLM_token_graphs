[I 2024-12-18 06:16:14,290] A new study created in RDB with name: IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 830.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 486.69 MiB is free. Including non-PyTorch memory, this process has 44.08 GiB memory in use. Of the allocated memory 37.26 GiB is allocated by PyTorch, and 5.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:31:49,034] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9304211818735242, 'batch_size': 41, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 4.09 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.70 GiB is free. Including non-PyTorch memory, this process has 40.85 GiB memory in use. Of the allocated memory 33.98 GiB is allocated by PyTorch, and 5.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:49:50,275] Trial 1 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9939404991670033, 'batch_size': 59, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 684.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 36.89 GiB is allocated by PyTorch, and 5.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:06:03,919] Trial 2 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.988712402130207, 'batch_size': 39, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 43.27 GiB memory in use. Of the allocated memory 35.76 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:13:16,247] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9227912369025688, 'batch_size': 36, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 808.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 220.69 MiB is free. Including non-PyTorch memory, this process has 44.34 GiB memory in use. Of the allocated memory 36.87 GiB is allocated by PyTorch, and 6.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:22:43,968] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9367746354405981, 'batch_size': 46, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 20.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.35 GiB is free. Including non-PyTorch memory, this process has 39.21 GiB memory in use. Of the allocated memory 32.27 GiB is allocated by PyTorch, and 5.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:38:41,993] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9725883083302352, 'batch_size': 59, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 744.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 620.69 MiB is free. Including non-PyTorch memory, this process has 43.95 GiB memory in use. Of the allocated memory 40.58 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:52:44,452] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9576846194237897, 'batch_size': 40, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 1.93 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 43.46 GiB memory in use. Of the allocated memory 34.91 GiB is allocated by PyTorch, and 7.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:00:20,707] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9359455202104634, 'batch_size': 30, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 2.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1006.69 MiB is free. Including non-PyTorch memory, this process has 43.57 GiB memory in use. Of the allocated memory 34.85 GiB is allocated by PyTorch, and 7.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:15:55,253] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9726018724589289, 'batch_size': 63, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 2.16 GiB. GPU 0 has a total capacity of 44.56 GiB of which 842.69 MiB is free. Including non-PyTorch memory, this process has 43.73 GiB memory in use. Of the allocated memory 38.58 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:27:00,197] Trial 9 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9491566713529321, 'batch_size': 39, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 0 with value: -1.0.
CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 43.08 GiB memory in use. Of the allocated memory 36.01 GiB is allocated by PyTorch, and 5.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:36:22,948] Trial 10 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9023898459569726, 'batch_size': 17, 'attention_heads': 8, 'hidden_dimension': 35, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3940343634200046, 'global_pooling': 'sum', 'learning_rate': 0.012672826384589787, 'weight_decay': 5.0142658678326185e-06, 'beta_0': 0.8971810161346192, 'beta_1': 0.9987182916384133, 'epsilon': 1.2023215866960696e-08, 'balanced_loss': True, 'epochs': 91, 'early_stopping_patience': 25, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 0 with value: -1.0.
[I 2024-12-18 09:04:14,448] Trial 11 finished with value: 0.896969696969697 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9990319415968159, 'batch_size': 52, 'attention_heads': 10, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3723786878035909, 'global_pooling': 'max', 'learning_rate': 7.919413614819874e-05, 'weight_decay': 1.040021777795823e-05, 'beta_0': 0.846594973912821, 'beta_1': 0.9854409613744951, 'epsilon': 7.800672033250071e-07, 'balanced_loss': True, 'epochs': 53, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 856.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 176.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 35.68 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:14:26,996] Trial 12 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9089255130438255, 'batch_size': 50, 'attention_heads': 9, 'hidden_dimension': 65, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4043818343290936, 'global_pooling': 'max', 'learning_rate': 3.7951784871656494e-05, 'weight_decay': 9.112084128938665e-06, 'beta_0': 0.8478038385745598, 'beta_1': 0.9847976982734662, 'epsilon': 5.519180548370294e-07, 'balanced_loss': True, 'epochs': 94, 'early_stopping_patience': 21, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 976.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 388.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 37.60 GiB is allocated by PyTorch, and 5.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:27:41,897] Trial 13 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9289538969478266, 'batch_size': 51, 'attention_heads': 8, 'hidden_dimension': 83, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3321202786248901, 'global_pooling': 'max', 'learning_rate': 0.02273101612424298, 'weight_decay': 1.1752003418812842e-05, 'beta_0': 0.8512409606840873, 'beta_1': 0.9941420202127368, 'epsilon': 2.7481510042749063e-05, 'balanced_loss': True, 'epochs': 82, 'early_stopping_patience': 22, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.56 GiB of which 588.69 MiB is free. Including non-PyTorch memory, this process has 43.98 GiB memory in use. Of the allocated memory 36.47 GiB is allocated by PyTorch, and 6.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:45:59,260] Trial 14 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9553729192528705, 'batch_size': 29, 'attention_heads': 12, 'hidden_dimension': 33, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40623288840305743, 'global_pooling': 'max', 'learning_rate': 5.790872487997769e-05, 'weight_decay': 2.4018575366154073e-05, 'beta_0': 0.8526200970566881, 'beta_1': 0.9848676992532891, 'epsilon': 1.0060069412640557e-08, 'balanced_loss': True, 'epochs': 71, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 740.69 MiB is free. Including non-PyTorch memory, this process has 43.83 GiB memory in use. Of the allocated memory 35.68 GiB is allocated by PyTorch, and 7.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:55:46,293] Trial 15 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9173881468953131, 'batch_size': 47, 'attention_heads': 8, 'hidden_dimension': 142, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5101867334009248, 'global_pooling': 'max', 'learning_rate': 0.0004265238643521284, 'weight_decay': 3.3295056335092956e-06, 'beta_0': 0.8208751820907969, 'beta_1': 0.9926530989594438, 'epsilon': 2.6875285255761655e-07, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 9}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 2.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 964.69 MiB is free. Including non-PyTorch memory, this process has 43.61 GiB memory in use. Of the allocated memory 36.61 GiB is allocated by PyTorch, and 5.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:22:08,952] Trial 16 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9777226794786454, 'batch_size': 55, 'attention_heads': 4, 'hidden_dimension': 106, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3584416422818444, 'global_pooling': 'sum', 'learning_rate': 0.00122860622618721, 'weight_decay': 3.252827237038469e-05, 'beta_0': 0.8600212244181678, 'beta_1': 0.9869401104105625, 'epsilon': 1.1598630173023627e-05, 'balanced_loss': True, 'epochs': 50, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 714.69 MiB is free. Including non-PyTorch memory, this process has 43.86 GiB memory in use. Of the allocated memory 36.77 GiB is allocated by PyTorch, and 5.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:38:29,642] Trial 17 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9432885769596184, 'batch_size': 31, 'attention_heads': 10, 'hidden_dimension': 66, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41364686887947905, 'global_pooling': 'max', 'learning_rate': 0.009373415816671718, 'weight_decay': 8.373363305542007e-06, 'beta_0': 0.8423514868495393, 'beta_1': 0.9829083119408621, 'epsilon': 7.979939088392969e-07, 'balanced_loss': True, 'epochs': 111, 'early_stopping_patience': 19, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 11:19:34,986] Trial 18 finished with value: 0.8545454545454545 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9981639742533072, 'batch_size': 18, 'attention_heads': 12, 'hidden_dimension': 161, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4362665054703549, 'global_pooling': 'max', 'learning_rate': 1.1501132716041072e-05, 'weight_decay': 1.0920521920017111e-06, 'beta_0': 0.8175180850431135, 'beta_1': 0.9912172047475992, 'epsilon': 2.6704627590275237e-06, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 12:08:57,259] Trial 19 finished with value: 0.8242424242424242 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9983336020741597, 'batch_size': 20, 'attention_heads': 16, 'hidden_dimension': 171, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43262398005352815, 'global_pooling': 'max', 'learning_rate': 1.1356910844967922e-05, 'weight_decay': 1.2030021400214904e-06, 'beta_0': 0.820269387461979, 'beta_1': 0.9874976134514227, 'epsilon': 3.3503783026673938e-06, 'balanced_loss': True, 'epochs': 78, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 5.81 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 41.83 GiB memory in use. Of the allocated memory 32.12 GiB is allocated by PyTorch, and 8.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:35:00,233] Trial 20 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9840299764884499, 'batch_size': 27, 'attention_heads': 13, 'hidden_dimension': 255, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5164621767817628, 'global_pooling': 'sum', 'learning_rate': 3.099399350231769e-05, 'weight_decay': 8.228621017251533e-05, 'beta_0': 0.822224914622008, 'beta_1': 0.9948763794220682, 'epsilon': 2.020216610895641e-06, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 13:20:38,372] Trial 21 finished with value: 0.806060606060606 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.998694437688148, 'batch_size': 19, 'attention_heads': 16, 'hidden_dimension': 172, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4328014113983141, 'global_pooling': 'max', 'learning_rate': 1.1205585911399095e-05, 'weight_decay': 1.2717783122173454e-06, 'beta_0': 0.8164058210543942, 'beta_1': 0.987143776668423, 'epsilon': 9.93258111229013e-06, 'balanced_loss': True, 'epochs': 70, 'early_stopping_patience': 23, 'plateau_patience': 11, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 14:02:13,077] Trial 22 finished with value: 0.8484848484848485 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9991694263006645, 'batch_size': 23, 'attention_heads': 16, 'hidden_dimension': 166, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4380753339680799, 'global_pooling': 'max', 'learning_rate': 2.125048782535045e-05, 'weight_decay': 2.558341796507721e-06, 'beta_0': 0.8028823561811946, 'beta_1': 0.9891421700459744, 'epsilon': 4.4588232818749576e-06, 'balanced_loss': True, 'epochs': 79, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.62 GiB is free. Including non-PyTorch memory, this process has 42.94 GiB memory in use. Of the allocated memory 31.33 GiB is allocated by PyTorch, and 10.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 14:28:29,646] Trial 23 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.984521303734885, 'batch_size': 24, 'attention_heads': 12, 'hidden_dimension': 127, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4844768384122291, 'global_pooling': 'max', 'learning_rate': 8.03305868514325e-05, 'weight_decay': 2.4358311117380778e-06, 'beta_0': 0.8013958268586786, 'beta_1': 0.9889070032004784, 'epsilon': 3.571320337472344e-07, 'balanced_loss': True, 'epochs': 97, 'early_stopping_patience': 24, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 730.69 MiB is free. Including non-PyTorch memory, this process has 43.84 GiB memory in use. Of the allocated memory 36.06 GiB is allocated by PyTorch, and 6.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 14:52:19,316] Trial 24 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9651345870613025, 'batch_size': 23, 'attention_heads': 14, 'hidden_dimension': 155, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3713544011288641, 'global_pooling': 'max', 'learning_rate': 2.6755290918670495e-05, 'weight_decay': 1.6656581456035123e-05, 'beta_0': 0.8095547931408965, 'beta_1': 0.9826180690151296, 'epsilon': 1.03634312905351e-06, 'balanced_loss': True, 'epochs': 68, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 43.01 GiB memory in use. Of the allocated memory 35.06 GiB is allocated by PyTorch, and 6.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:19:13,360] Trial 25 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9898879774588984, 'batch_size': 23, 'attention_heads': 11, 'hidden_dimension': 171, 'number_of_hidden_layers': 4, 'dropout_rate': 0.44227001682411543, 'global_pooling': 'max', 'learning_rate': 0.00017673294853182375, 'weight_decay': 5.2951458870980225e-06, 'beta_0': 0.8100695987929997, 'beta_1': 0.9916929819179178, 'epsilon': 2.960331061371588e-06, 'balanced_loss': True, 'epochs': 86, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 15:52:00,743] Trial 26 finished with value: 0.8181818181818182 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9995501883600627, 'batch_size': 35, 'attention_heads': 15, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42056663593162347, 'global_pooling': 'max', 'learning_rate': 2.5023128224061706e-05, 'weight_decay': 2.089360274705482e-06, 'beta_0': 0.8012169057345605, 'beta_1': 0.9903669639975574, 'epsilon': 1.99803996297257e-05, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 43.31 GiB memory in use. Of the allocated memory 30.92 GiB is allocated by PyTorch, and 11.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:19:02,063] Trial 27 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9798970138160616, 'batch_size': 17, 'attention_heads': 13, 'hidden_dimension': 188, 'number_of_hidden_layers': 3, 'dropout_rate': 0.47277459035270775, 'global_pooling': 'max', 'learning_rate': 6.828827972235481e-05, 'weight_decay': 6.51051519143956e-06, 'beta_0': 0.8279869858548541, 'beta_1': 0.9888101775304755, 'epsilon': 6.241733555610147e-06, 'balanced_loss': True, 'epochs': 74, 'early_stopping_patience': 24, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 598.69 MiB is free. Including non-PyTorch memory, this process has 43.97 GiB memory in use. Of the allocated memory 33.26 GiB is allocated by PyTorch, and 9.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:45:14,438] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9909926695723857, 'batch_size': 26, 'attention_heads': 10, 'hidden_dimension': 143, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5225980008340627, 'global_pooling': 'max', 'learning_rate': 1.8605332617861502e-05, 'weight_decay': 3.3235061528431776e-06, 'beta_0': 0.8411015115665882, 'beta_1': 0.9939102902718501, 'epsilon': 1.2046485783823063e-06, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 22, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 212.69 MiB is free. Including non-PyTorch memory, this process has 44.35 GiB memory in use. Of the allocated memory 36.43 GiB is allocated by PyTorch, and 6.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:59:40,482] Trial 29 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9645983301022172, 'batch_size': 44, 'attention_heads': 9, 'hidden_dimension': 82, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3858489357665991, 'global_pooling': 'sum', 'learning_rate': 0.00040238826534130464, 'weight_decay': 1.8339093789161848e-06, 'beta_0': 0.8139118254064872, 'beta_1': 0.991417097549921, 'epsilon': 5.479648324302844e-07, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 4.63 GiB. GPU 0 has a total capacity of 44.56 GiB of which 28.69 MiB is free. Including non-PyTorch memory, this process has 44.53 GiB memory in use. Of the allocated memory 38.07 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 17:26:16,774] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9740514437585762, 'batch_size': 34, 'attention_heads': 11, 'hidden_dimension': 115, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4536998801994345, 'global_pooling': 'max', 'learning_rate': 5.064511117569413e-05, 'weight_decay': 3.53363242493564e-06, 'beta_0': 0.829689701935632, 'beta_1': 0.9858659799391807, 'epsilon': 1.659479676511342e-07, 'balanced_loss': True, 'epochs': 102, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 18:09:38,387] Trial 31 finished with value: 0.806060606060606 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9993953337494772, 'batch_size': 20, 'attention_heads': 16, 'hidden_dimension': 171, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43303760352330056, 'global_pooling': 'max', 'learning_rate': 1.4370138801629844e-05, 'weight_decay': 1.161391234134202e-06, 'beta_0': 0.8180912234642196, 'beta_1': 0.9883028522885049, 'epsilon': 3.435135444190704e-06, 'balanced_loss': True, 'epochs': 77, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacity of 44.56 GiB of which 316.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 32.85 GiB is allocated by PyTorch, and 10.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 18:36:35,979] Trial 32 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9954258346793236, 'batch_size': 21, 'attention_heads': 16, 'hidden_dimension': 162, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3613054581152861, 'global_pooling': 'max', 'learning_rate': 1.0069677683979879e-05, 'weight_decay': 1.0140064730796525e-06, 'beta_0': 0.8245381790057732, 'beta_1': 0.9839750971571763, 'epsilon': 1.955444997090743e-06, 'balanced_loss': True, 'epochs': 62, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 2.98 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 42.81 GiB memory in use. Of the allocated memory 37.19 GiB is allocated by PyTorch, and 4.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:02:56,475] Trial 33 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9859183691782141, 'batch_size': 19, 'attention_heads': 15, 'hidden_dimension': 182, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4217726038233515, 'global_pooling': 'max', 'learning_rate': 0.00013804523189091288, 'weight_decay': 1.6524698063366808e-06, 'beta_0': 0.8068191814315706, 'beta_1': 0.9869384459520907, 'epsilon': 7.114108928251001e-06, 'balanced_loss': True, 'epochs': 82, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 44.56 GiB of which 670.69 MiB is free. Including non-PyTorch memory, this process has 43.90 GiB memory in use. Of the allocated memory 33.38 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:29:58,174] Trial 34 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9940373439485983, 'batch_size': 16, 'attention_heads': 15, 'hidden_dimension': 201, 'number_of_hidden_layers': 4, 'dropout_rate': 0.464166652978116, 'global_pooling': 'max', 'learning_rate': 2.0553934381431106e-05, 'weight_decay': 3.5217966806413197e-06, 'beta_0': 0.8361857671255765, 'beta_1': 0.986134362416846, 'epsilon': 4.421847404188067e-06, 'balanced_loss': True, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 1.71 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 43.32 GiB memory in use. Of the allocated memory 36.15 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:46:42,380] Trial 35 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9922635733221017, 'batch_size': 27, 'attention_heads': 13, 'hidden_dimension': 144, 'number_of_hidden_layers': 3, 'dropout_rate': 0.49438834270870274, 'global_pooling': 'mean', 'learning_rate': 7.975270775105055e-05, 'weight_decay': 2.4693909708194884e-06, 'beta_0': 0.8063871341065543, 'beta_1': 0.990319447424958, 'epsilon': 2.368137802253122e-06, 'balanced_loss': True, 'epochs': 52, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 6}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 11.84 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 42.73 GiB memory in use. Of the allocated memory 36.62 GiB is allocated by PyTorch, and 4.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:02:42,219] Trial 36 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9807050983793435, 'batch_size': 43, 'attention_heads': 16, 'hidden_dimension': 217, 'number_of_hidden_layers': 4, 'dropout_rate': 0.44676015573997174, 'global_pooling': 'max', 'learning_rate': 3.639032697375552e-05, 'weight_decay': 1.5834159665871038e-06, 'beta_0': 0.8141894545094638, 'beta_1': 0.9824374804389217, 'epsilon': 2.3754310519102683e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 25, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 11 with value: 0.896969696969697.
CUDA out of memory. Tried to allocate 8.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 7.21 GiB is free. Including non-PyTorch memory, this process has 37.34 GiB memory in use. Of the allocated memory 27.06 GiB is allocated by PyTorch, and 9.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:29:31,368] Trial 37 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9886328312856725, 'batch_size': 55, 'attention_heads': 14, 'hidden_dimension': 236, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35277845533428015, 'global_pooling': 'max', 'learning_rate': 1.8106784903164554e-05, 'weight_decay': 1.4298301254268389e-05, 'beta_0': 0.8254335266263964, 'beta_1': 0.9901832080475299, 'epsilon': 1.437063164132508e-06, 'balanced_loss': True, 'epochs': 87, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 11 with value: 0.896969696969697.
[I 2024-12-18 20:52:25,360] Trial 38 finished with value: 0.9333333333333333 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9945194639105507, 'batch_size': 33, 'attention_heads': 11, 'hidden_dimension': 160, 'number_of_hidden_layers': 3, 'dropout_rate': 0.30582202595639796, 'global_pooling': 'mean', 'learning_rate': 0.00011238852464837925, 'weight_decay': 5.355234612012268e-06, 'beta_0': 0.832025771504297, 'beta_1': 0.9880589121375674, 'epsilon': 9.435271524111181e-05, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 10, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 38 with value: 0.9333333333333333.
CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 726.69 MiB is free. Including non-PyTorch memory, this process has 43.84 GiB memory in use. Of the allocated memory 36.84 GiB is allocated by PyTorch, and 5.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 21:07:29,189] Trial 39 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9653282993224095, 'batch_size': 64, 'attention_heads': 11, 'hidden_dimension': 157, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3234562628069413, 'global_pooling': 'mean', 'learning_rate': 0.0002956243605467621, 'weight_decay': 6.470867615878606e-06, 'beta_0': 0.833320722524139, 'beta_1': 0.9837346128143677, 'epsilon': 6.236320838926794e-05, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 38 with value: 0.9333333333333333.
[I 2024-12-18 21:26:33,822] Trial 40 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9932167160906763, 'batch_size': 32, 'attention_heads': 7, 'hidden_dimension': 133, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3053275154265274, 'global_pooling': 'mean', 'learning_rate': 0.0008557945565425122, 'weight_decay': 6.452410630600642e-05, 'beta_0': 0.8387109288269862, 'beta_1': 0.9893331861059352, 'epsilon': 1.2083921889676848e-07, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 40 with value: 0.9454545454545454.
[I 2024-12-18 21:44:59,611] Trial 41 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9945801401026193, 'batch_size': 31, 'attention_heads': 7, 'hidden_dimension': 134, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30203670858628456, 'global_pooling': 'mean', 'learning_rate': 0.000985619070289282, 'weight_decay': 5.6807570675674346e-05, 'beta_0': 0.84124953136008, 'beta_1': 0.9891024106656388, 'epsilon': 2.5040804097029526e-08, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
[I 2024-12-18 22:03:25,077] Trial 42 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9935503914635387, 'batch_size': 37, 'attention_heads': 7, 'hidden_dimension': 138, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3037845661348362, 'global_pooling': 'mean', 'learning_rate': 0.0009405646367919515, 'weight_decay': 5.20764916141835e-05, 'beta_0': 0.8417901034166178, 'beta_1': 0.9881474885949798, 'epsilon': 2.372605312733331e-08, 'balanced_loss': False, 'epochs': 128, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 41 with value: 0.9575757575757575.
slurmstepd: error: *** JOB 14110899 ON gpu044 CANCELLED AT 2024-12-18T22:16:13 DUE TO TIME LIMIT ***
