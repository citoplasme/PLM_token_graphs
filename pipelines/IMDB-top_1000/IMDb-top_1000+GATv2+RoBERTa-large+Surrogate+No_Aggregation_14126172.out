Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2024-12-20 05:56:37,088] Using an existing study with name 'IMDb-top_1000-GATv2-FacebookAI-roberta-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors
[I 2024-12-20 06:22:14,401] Trial 93 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9973419448452235, 'batch_size': 38, 'attention_heads': 6, 'hidden_dimension': 156, 'number_of_hidden_layers': 3, 'dropout_rate': 0.41388127302606703, 'global_pooling': 'mean', 'learning_rate': 0.0005948632479876269, 'weight_decay': 1.6866111071489916e-06, 'beta_0': 0.8496058859028948, 'beta_1': 0.9954230615662414, 'epsilon': 2.1856114126267692e-08, 'balanced_loss': True, 'epochs': 196, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 06:47:40,532] Trial 94 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9972692514915642, 'batch_size': 36, 'attention_heads': 6, 'hidden_dimension': 143, 'number_of_hidden_layers': 3, 'dropout_rate': 0.42984561867848836, 'global_pooling': 'mean', 'learning_rate': 0.0005911745129395941, 'weight_decay': 3.0778014862194756e-06, 'beta_0': 0.8482368951845984, 'beta_1': 0.9960262372197484, 'epsilon': 1.4473780314081548e-08, 'balanced_loss': True, 'epochs': 197, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 07:11:52,754] Trial 95 finished with value: 0.7454545454545455 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9999403274259792, 'batch_size': 38, 'attention_heads': 5, 'hidden_dimension': 153, 'number_of_hidden_layers': 3, 'dropout_rate': 0.44762906313897377, 'global_pooling': 'mean', 'learning_rate': 0.0005161084050857437, 'weight_decay': 1.1641560896862473e-06, 'beta_0': 0.8512988181831113, 'beta_1': 0.9957514473519063, 'epsilon': 1.8962569126271874e-08, 'balanced_loss': True, 'epochs': 193, 'early_stopping_patience': 10, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 07:40:20,571] Trial 96 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9904185114003272, 'batch_size': 33, 'attention_heads': 8, 'hidden_dimension': 163, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3992248202604678, 'global_pooling': 'mean', 'learning_rate': 0.0011535144143908081, 'weight_decay': 4.253038787512235e-05, 'beta_0': 0.8407793539693889, 'beta_1': 0.9953523645286064, 'epsilon': 2.2893946465566024e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 08:06:29,793] Trial 97 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9968025312041169, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 166, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41492762293289487, 'global_pooling': 'mean', 'learning_rate': 0.0007208308112335309, 'weight_decay': 5.800133380581796e-05, 'beta_0': 0.8629606353491985, 'beta_1': 0.9947856687661171, 'epsilon': 1.5017396257646126e-08, 'balanced_loss': True, 'epochs': 198, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 08:31:58,724] Trial 98 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9976820041584071, 'batch_size': 33, 'attention_heads': 8, 'hidden_dimension': 162, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3730404531607381, 'global_pooling': 'mean', 'learning_rate': 0.00028912644384523044, 'weight_decay': 4.3344370974165426e-05, 'beta_0': 0.860463112229623, 'beta_1': 0.9951514636242375, 'epsilon': 1.4698386432163584e-08, 'balanced_loss': True, 'epochs': 198, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 08:59:09,307] Trial 99 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9961181728100832, 'batch_size': 34, 'attention_heads': 7, 'hidden_dimension': 204, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40465529064467276, 'global_pooling': 'mean', 'learning_rate': 0.0006631897442602279, 'weight_decay': 6.348954802815087e-05, 'beta_0': 0.8644468109871731, 'beta_1': 0.9956448439837617, 'epsilon': 6.084928252075277e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.47 GiB is free. Including non-PyTorch memory, this process has 40.09 GiB memory in use. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 9.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 09:21:01,907] Trial 100 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9864217170572859, 'batch_size': 35, 'attention_heads': 7, 'hidden_dimension': 203, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40501978181093945, 'global_pooling': 'mean', 'learning_rate': 0.000727192175037086, 'weight_decay': 9.77063958433665e-05, 'beta_0': 0.8637072825929704, 'beta_1': 0.9956227811915608, 'epsilon': 2.2676760015969435e-08, 'balanced_loss': True, 'epochs': 188, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 09:48:12,583] Trial 101 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.995684524718646, 'batch_size': 38, 'attention_heads': 7, 'hidden_dimension': 219, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4169874802224094, 'global_pooling': 'mean', 'learning_rate': 0.0004211183702349635, 'weight_decay': 6.1677658562366e-05, 'beta_0': 0.8695184328444644, 'beta_1': 0.9965434303096633, 'epsilon': 4.139593386577216e-08, 'balanced_loss': True, 'epochs': 193, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 43.34 GiB memory in use. Of the allocated memory 38.26 GiB is allocated by PyTorch, and 3.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 10:12:25,082] Trial 102 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9901664124538148, 'batch_size': 37, 'attention_heads': 7, 'hidden_dimension': 191, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39702869137304614, 'global_pooling': 'mean', 'learning_rate': 0.00033414003262962017, 'weight_decay': 3.639582828272357e-05, 'beta_0': 0.8730069800933921, 'beta_1': 0.994347749578932, 'epsilon': 6.16120916282761e-08, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 10, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 10:37:47,456] Trial 103 finished with value: 0.9212121212121213 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.997896632545837, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 166, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4090620005454001, 'global_pooling': 'mean', 'learning_rate': 0.001071000039268043, 'weight_decay': 2.4050607716725035e-05, 'beta_0': 0.856584173074245, 'beta_1': 0.9948241052501029, 'epsilon': 2.4382731472568702e-08, 'balanced_loss': True, 'epochs': 189, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 11:06:27,494] Trial 104 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9928262487385288, 'batch_size': 33, 'attention_heads': 6, 'hidden_dimension': 195, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4220738002380693, 'global_pooling': 'mean', 'learning_rate': 0.0005626011089242127, 'weight_decay': 6.321827694736056e-05, 'beta_0': 0.8640272877376886, 'beta_1': 0.9954314001708829, 'epsilon': 3.299144337487843e-08, 'balanced_loss': True, 'epochs': 195, 'early_stopping_patience': 10, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 11:31:29,456] Trial 105 finished with value: 0.5696969696969697 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.992705604667552, 'batch_size': 33, 'attention_heads': 6, 'hidden_dimension': 215, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3864146903457915, 'global_pooling': 'mean', 'learning_rate': 0.0006654146932639652, 'weight_decay': 7.241253116958135e-05, 'beta_0': 0.8664188093408371, 'beta_1': 0.9965821739860549, 'epsilon': 3.588543595474464e-08, 'balanced_loss': True, 'epochs': 196, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 11:57:45,847] Trial 106 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9957563638662154, 'batch_size': 40, 'attention_heads': 6, 'hidden_dimension': 196, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42311104933430177, 'global_pooling': 'mean', 'learning_rate': 0.0001983041847039719, 'weight_decay': 6.258257443028524e-05, 'beta_0': 0.8798966395075792, 'beta_1': 0.9834559258385036, 'epsilon': 5.223533665032645e-08, 'balanced_loss': True, 'epochs': 200, 'early_stopping_patience': 10, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 91 with value: 0.9393939393939394.
CUDA out of memory. Tried to allocate 3.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 418.69 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 39.87 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 12:19:30,731] Trial 107 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.984181937590403, 'batch_size': 34, 'attention_heads': 5, 'hidden_dimension': 185, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4112004739788299, 'global_pooling': 'mean', 'learning_rate': 0.0008692344057614616, 'weight_decay': 4.420308933412844e-05, 'beta_0': 0.8626860822154871, 'beta_1': 0.9954980379130752, 'epsilon': 9.877078902934937e-08, 'balanced_loss': True, 'epochs': 191, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 974.69 MiB is free. Including non-PyTorch memory, this process has 43.60 GiB memory in use. Of the allocated memory 37.13 GiB is allocated by PyTorch, and 5.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 12:43:25,596] Trial 108 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9879486096120869, 'batch_size': 37, 'attention_heads': 6, 'hidden_dimension': 210, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40118354666482403, 'global_pooling': 'mean', 'learning_rate': 1.0993032231668202e-05, 'weight_decay': 9.062437848317733e-05, 'beta_0': 0.8719760919783104, 'beta_1': 0.9946330760194285, 'epsilon': 1.5466291482236322e-08, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 13:12:06,550] Trial 109 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.991478592453448, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 179, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39225356923611493, 'global_pooling': 'mean', 'learning_rate': 0.0005196552276641018, 'weight_decay': 5.3115522991395093e-05, 'beta_0': 0.8495758215265324, 'beta_1': 0.996575562366806, 'epsilon': 2.645110949043227e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 12, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 91 with value: 0.9393939393939394.
[I 2024-12-20 13:39:46,166] Trial 110 finished with value: 0.9515151515151515 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.991848939139928, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 181, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3783326478922573, 'global_pooling': 'mean', 'learning_rate': 0.0005817067557597735, 'weight_decay': 5.178725441668603e-05, 'beta_0': 0.8397044599541584, 'beta_1': 0.9964952431970833, 'epsilon': 2.703914085665427e-08, 'balanced_loss': True, 'epochs': 186, 'early_stopping_patience': 12, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 5.47 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.80 GiB is free. Including non-PyTorch memory, this process has 40.76 GiB memory in use. Of the allocated memory 32.84 GiB is allocated by PyTorch, and 6.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 13:48:20,582] Trial 111 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9466215888410638, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 174, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37033791407680644, 'global_pooling': 'mean', 'learning_rate': 0.0004835972066178501, 'weight_decay': 2.9315079292939747e-05, 'beta_0': 0.839101589238264, 'beta_1': 0.9965100694242487, 'epsilon': 1.7582034239691473e-08, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 12, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 1.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 38.71 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 14:12:15,348] Trial 112 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.990454119280379, 'batch_size': 47, 'attention_heads': 7, 'hidden_dimension': 167, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3521928951784228, 'global_pooling': 'mean', 'learning_rate': 0.0008632161296564895, 'weight_decay': 3.689563349867631e-05, 'beta_0': 0.8406166375338594, 'beta_1': 0.9979929602116361, 'epsilon': 2.5537659084031204e-08, 'balanced_loss': True, 'epochs': 189, 'early_stopping_patience': 12, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1014.69 MiB is free. Including non-PyTorch memory, this process has 43.56 GiB memory in use. Of the allocated memory 36.02 GiB is allocated by PyTorch, and 6.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 14:20:19,022] Trial 113 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9314968509238278, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 181, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37868486888618264, 'global_pooling': 'mean', 'learning_rate': 0.0005810509077762776, 'weight_decay': 5.6017905384207306e-05, 'beta_0': 0.8474753920608393, 'beta_1': 0.9953080476162394, 'epsilon': 3.212485816472379e-08, 'balanced_loss': True, 'epochs': 198, 'early_stopping_patience': 11, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 14:47:54,724] Trial 114 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9942567559325657, 'batch_size': 44, 'attention_heads': 8, 'hidden_dimension': 189, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4213252776450877, 'global_pooling': 'mean', 'learning_rate': 0.0003900615670256814, 'weight_decay': 4.762525085789044e-05, 'beta_0': 0.849143316900904, 'beta_1': 0.9972627862635713, 'epsilon': 3.803725705358946e-08, 'balanced_loss': True, 'epochs': 186, 'early_stopping_patience': 11, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 15:14:02,509] Trial 115 finished with value: 0.6606060606060606 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9924037836887638, 'batch_size': 35, 'attention_heads': 8, 'hidden_dimension': 201, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39161916354832893, 'global_pooling': 'mean', 'learning_rate': 0.0009993005271244125, 'weight_decay': 3.913280814859811e-05, 'beta_0': 0.8530258615479146, 'beta_1': 0.9967492959001237, 'epsilon': 1.2781932243706119e-08, 'balanced_loss': True, 'epochs': 62, 'early_stopping_patience': 12, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 15:39:23,753] Trial 116 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.997408271089748, 'batch_size': 39, 'attention_heads': 6, 'hidden_dimension': 178, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39255990280559694, 'global_pooling': 'mean', 'learning_rate': 0.0006011368480878297, 'weight_decay': 6.528761158812978e-05, 'beta_0': 0.8454883455965205, 'beta_1': 0.9959641295276773, 'epsilon': 2.9869219090112574e-08, 'balanced_loss': True, 'epochs': 182, 'early_stopping_patience': 10, 'plateau_patience': 21, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 16:06:52,509] Trial 117 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9940487927987119, 'batch_size': 33, 'attention_heads': 7, 'hidden_dimension': 194, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4133832006522369, 'global_pooling': 'mean', 'learning_rate': 0.0007146544374114651, 'weight_decay': 0.00011019969167915073, 'beta_0': 0.8653019279094184, 'beta_1': 0.9961791958937004, 'epsilon': 6.917786162667072e-08, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 2.81 GiB. GPU 0 has a total capacity of 44.56 GiB of which 988.69 MiB is free. Including non-PyTorch memory, this process has 43.59 GiB memory in use. Of the allocated memory 35.31 GiB is allocated by PyTorch, and 7.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 16:30:50,676] Trial 118 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.988953229062303, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 155, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40336769848756376, 'global_pooling': 'mean', 'learning_rate': 0.001488696157752782, 'weight_decay': 5.3224720032458974e-05, 'beta_0': 0.8421996329552116, 'beta_1': 0.9979128442804633, 'epsilon': 1.6943639013047398e-08, 'balanced_loss': True, 'epochs': 190, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 16:54:24,576] Trial 119 finished with value: 0.7878787878787878 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9916644441438879, 'batch_size': 36, 'attention_heads': 7, 'hidden_dimension': 183, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3605935699496862, 'global_pooling': 'sum', 'learning_rate': 0.0005089822946774454, 'weight_decay': 2.0382940335312846e-05, 'beta_0': 0.8504324917979164, 'beta_1': 0.9940196335157404, 'epsilon': 4.8092084893381715e-08, 'balanced_loss': True, 'epochs': 197, 'early_stopping_patience': 12, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 17:15:25,632] Trial 120 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9961944434428888, 'batch_size': 43, 'attention_heads': 6, 'hidden_dimension': 204, 'number_of_hidden_layers': 2, 'dropout_rate': 0.43089766866566753, 'global_pooling': 'mean', 'learning_rate': 0.00118880974362227, 'weight_decay': 3.231232214979127e-05, 'beta_0': 0.8615784389450251, 'beta_1': 0.9970287564422278, 'epsilon': 2.2364012417461817e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 17:37:34,652] Trial 121 finished with value: 0.9090909090909091 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9983324377349563, 'batch_size': 43, 'attention_heads': 15, 'hidden_dimension': 206, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4308754663829516, 'global_pooling': 'mean', 'learning_rate': 0.0011717233222238528, 'weight_decay': 3.118757886559338e-05, 'beta_0': 0.8579250065330436, 'beta_1': 0.9975783519880307, 'epsilon': 1.0240450047805516e-08, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 17:58:58,089] Trial 122 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9960886455170999, 'batch_size': 45, 'attention_heads': 8, 'hidden_dimension': 215, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4154545030800927, 'global_pooling': 'mean', 'learning_rate': 0.0009406107753486407, 'weight_decay': 2.1981940663087288e-05, 'beta_0': 0.8462716553307678, 'beta_1': 0.9879597937927277, 'epsilon': 2.1387076540405163e-08, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 18:20:37,425] Trial 123 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9964415728296463, 'batch_size': 42, 'attention_heads': 8, 'hidden_dimension': 216, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39702383296236265, 'global_pooling': 'mean', 'learning_rate': 0.0009334386365925936, 'weight_decay': 2.327528020028172e-05, 'beta_0': 0.847012905670338, 'beta_1': 0.9879253756921612, 'epsilon': 2.1011965070624157e-08, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 18:42:41,369] Trial 124 finished with value: 0.9454545454545454 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9956538259478407, 'batch_size': 46, 'attention_heads': 8, 'hidden_dimension': 241, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3944839621293674, 'global_pooling': 'mean', 'learning_rate': 0.0012373928043539707, 'weight_decay': 1.6798598580380497e-05, 'beta_0': 0.8456243082055672, 'beta_1': 0.9878231369995146, 'epsilon': 2.080100052031003e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 19:03:08,266] Trial 125 finished with value: 0.9030303030303031 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9986786342478622, 'batch_size': 46, 'attention_heads': 8, 'hidden_dimension': 244, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3780163680872266, 'global_pooling': 'mean', 'learning_rate': 0.0012034769415462175, 'weight_decay': 2.1554735253365333e-05, 'beta_0': 0.8464590319718434, 'beta_1': 0.9881626070000529, 'epsilon': 2.190578503062978e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 19:28:54,437] Trial 126 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9949905392957984, 'batch_size': 45, 'attention_heads': 9, 'hidden_dimension': 251, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39727219131668823, 'global_pooling': 'mean', 'learning_rate': 0.0014544952200353678, 'weight_decay': 1.5901975749764674e-05, 'beta_0': 0.8446929487959411, 'beta_1': 0.9868742287777337, 'epsilon': 2.3850063871454087e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 2.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 42.88 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 2.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 19:47:45,804] Trial 127 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9908709495189622, 'batch_size': 49, 'attention_heads': 8, 'hidden_dimension': 216, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3922599895524336, 'global_pooling': 'mean', 'learning_rate': 0.0017656735676257636, 'weight_decay': 2.451961685142576e-05, 'beta_0': 0.8508546498687707, 'beta_1': 0.9876931888869445, 'epsilon': 1.276439144754204e-08, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 20:20:27,313] Trial 128 finished with value: 0.593939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9999467655349157, 'batch_size': 43, 'attention_heads': 9, 'hidden_dimension': 222, 'number_of_hidden_layers': 2, 'dropout_rate': 0.44226595667594587, 'global_pooling': 'mean', 'learning_rate': 0.0009300892762426563, 'weight_decay': 1.0437180195219096e-05, 'beta_0': 0.8404589024037209, 'beta_1': 0.9879848139915893, 'epsilon': 1.9113450460261516e-08, 'balanced_loss': True, 'epochs': 177, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 3.59 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 43.18 GiB memory in use. Of the allocated memory 35.60 GiB is allocated by PyTorch, and 6.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 20:38:16,464] Trial 129 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9859756261992741, 'batch_size': 45, 'attention_heads': 10, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3827496647134797, 'global_pooling': 'mean', 'learning_rate': 0.0024158600859615345, 'weight_decay': 1.6183566127666055e-05, 'beta_0': 0.8483047415773471, 'beta_1': 0.9893088670641531, 'epsilon': 2.8135920046540046e-08, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 540.69 MiB is free. Including non-PyTorch memory, this process has 44.03 GiB memory in use. Of the allocated memory 35.33 GiB is allocated by PyTorch, and 7.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 20:44:38,792] Trial 130 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.913664803275008, 'batch_size': 48, 'attention_heads': 9, 'hidden_dimension': 241, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3511164024805776, 'global_pooling': 'mean', 'learning_rate': 0.0010506507213740512, 'weight_decay': 1.7064546999612368e-05, 'beta_0': 0.8424734147261091, 'beta_1': 0.9885086130091925, 'epsilon': 2.1195431313381373e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.93 GiB is free. Including non-PyTorch memory, this process has 42.62 GiB memory in use. Of the allocated memory 38.98 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 21:02:28,580] Trial 131 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9939320082182914, 'batch_size': 42, 'attention_heads': 16, 'hidden_dimension': 231, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42664522694875956, 'global_pooling': 'mean', 'learning_rate': 0.0012884765295263587, 'weight_decay': 2.923301197660654e-05, 'beta_0': 0.8536458099878642, 'beta_1': 0.9898151843756444, 'epsilon': 2.6365645017083207e-08, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 21:24:02,603] Trial 132 finished with value: 0.9272727272727272 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9895620378413795, 'batch_size': 40, 'attention_heads': 8, 'hidden_dimension': 212, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37389893497816595, 'global_pooling': 'mean', 'learning_rate': 0.0008281976365835784, 'weight_decay': 3.394900388652732e-05, 'beta_0': 0.8384325125576437, 'beta_1': 0.9864201981644741, 'epsilon': 1.2821335459236611e-08, 'balanced_loss': True, 'epochs': 182, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
[I 2024-12-20 21:44:43,436] Trial 133 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.995414068237007, 'batch_size': 44, 'attention_heads': 8, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4155036456954823, 'global_pooling': 'mean', 'learning_rate': 0.0007959668335110419, 'weight_decay': 1.0159771919145749e-05, 'beta_0': 0.8456501484684005, 'beta_1': 0.9873622401428385, 'epsilon': 1.6865865963715048e-08, 'balanced_loss': True, 'epochs': 192, 'early_stopping_patience': 12, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 110 with value: 0.9515151515151515.
slurmstepd: error: *** JOB 14126172 ON gpu049 CANCELLED AT 2024-12-20T21:56:25 DUE TO TIME LIMIT ***
