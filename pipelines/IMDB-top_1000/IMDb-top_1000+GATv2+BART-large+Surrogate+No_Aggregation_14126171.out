[I 2024-12-20 05:56:31,261] Using an existing study with name 'IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
[I 2024-12-20 06:15:03,791] Trial 101 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9847166342281207, 'batch_size': 31, 'attention_heads': 7, 'hidden_dimension': 64, 'number_of_hidden_layers': 3, 'dropout_rate': 0.30188093864727134, 'global_pooling': 'sum', 'learning_rate': 0.002903676070291282, 'weight_decay': 0.0006639165402356961, 'beta_0': 0.8585434751873988, 'beta_1': 0.9871031473125199, 'epsilon': 1.3191058384403166e-08, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 12, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 956.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 382.69 MiB is free. Including non-PyTorch memory, this process has 44.18 GiB memory in use. Of the allocated memory 34.82 GiB is allocated by PyTorch, and 8.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 06:20:55,025] Trial 102 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9033582526345979, 'batch_size': 18, 'attention_heads': 4, 'hidden_dimension': 105, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5998087315924654, 'global_pooling': 'sum', 'learning_rate': 0.0005881636956569015, 'weight_decay': 1.460539444068023e-05, 'beta_0': 0.8551763717563208, 'beta_1': 0.9886750774501546, 'epsilon': 5.380671140460169e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 06:38:37,177] Trial 103 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.992179015465684, 'batch_size': 20, 'attention_heads': 5, 'hidden_dimension': 71, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3360563264996497, 'global_pooling': 'sum', 'learning_rate': 0.001394193868599617, 'weight_decay': 0.0005337774017908756, 'beta_0': 0.8703418224374739, 'beta_1': 0.9900603737760744, 'epsilon': 1.0173639990055181e-08, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 10, 'plateau_patience': 21, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 06:56:46,993] Trial 104 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9917069684711745, 'batch_size': 21, 'attention_heads': 5, 'hidden_dimension': 94, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3229445163758037, 'global_pooling': 'sum', 'learning_rate': 0.0019454248745974791, 'weight_decay': 0.0003730463206526105, 'beta_0': 0.8731264855562036, 'beta_1': 0.985242684099991, 'epsilon': 7.325805414924758e-08, 'balanced_loss': False, 'epochs': 136, 'early_stopping_patience': 14, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 07:15:31,661] Trial 105 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9895089927060825, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 83, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3475669459234986, 'global_pooling': 'sum', 'learning_rate': 0.001081245228106193, 'weight_decay': 0.0008610973600318087, 'beta_0': 0.8654104129449767, 'beta_1': 0.9876469048346637, 'epsilon': 3.3584310308021153e-07, 'balanced_loss': False, 'epochs': 141, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 07:32:57,286] Trial 106 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9978849947449098, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 77, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3307160513591791, 'global_pooling': 'sum', 'learning_rate': 0.0009167831167581495, 'weight_decay': 2.035666193182203e-05, 'beta_0': 0.8779451369129817, 'beta_1': 0.9866217746778017, 'epsilon': 3.599588282014858e-08, 'balanced_loss': False, 'epochs': 155, 'early_stopping_patience': 10, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 44.56 GiB of which 750.69 MiB is free. Including non-PyTorch memory, this process has 43.82 GiB memory in use. Of the allocated memory 36.72 GiB is allocated by PyTorch, and 5.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 07:50:03,260] Trial 107 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9791172525297044, 'batch_size': 19, 'attention_heads': 7, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31270663934283655, 'global_pooling': 'sum', 'learning_rate': 0.0021737668799908256, 'weight_decay': 0.0005090537454000475, 'beta_0': 0.8410226154042123, 'beta_1': 0.9858708325128167, 'epsilon': 2.579048707877518e-08, 'balanced_loss': False, 'epochs': 130, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 08:08:51,065] Trial 108 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9859092971351487, 'batch_size': 25, 'attention_heads': 5, 'hidden_dimension': 84, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3577386030042948, 'global_pooling': 'sum', 'learning_rate': 0.0007165334797457926, 'weight_decay': 2.8566775788412864e-05, 'beta_0': 0.8690198100371666, 'beta_1': 0.9834566095928378, 'epsilon': 2.1116524599118706e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 11, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 43.22 GiB memory in use. Of the allocated memory 33.29 GiB is allocated by PyTorch, and 8.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 08:25:56,578] Trial 109 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9837694436688347, 'batch_size': 27, 'attention_heads': 6, 'hidden_dimension': 127, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4747496049930354, 'global_pooling': 'sum', 'learning_rate': 0.0015325264414838829, 'weight_decay': 0.0008113304291575976, 'beta_0': 0.8617044430565883, 'beta_1': 0.9895574491884064, 'epsilon': 1.4826729094748205e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 08:43:34,839] Trial 110 finished with value: 0.9151515151515152 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9873648628469781, 'batch_size': 26, 'attention_heads': 4, 'hidden_dimension': 60, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30635615474166605, 'global_pooling': 'sum', 'learning_rate': 0.0003487866201711811, 'weight_decay': 3.3414441307095585e-05, 'beta_0': 0.8672293676058141, 'beta_1': 0.9916357074760543, 'epsilon': 1.469896398054572e-07, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 11, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.40 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 43.46 GiB memory in use. Of the allocated memory 35.76 GiB is allocated by PyTorch, and 6.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 08:59:53,708] Trial 111 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9811945379725349, 'batch_size': 33, 'attention_heads': 6, 'hidden_dimension': 90, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3260421117903691, 'global_pooling': 'sum', 'learning_rate': 0.0004668950211280641, 'weight_decay': 0.0002552672893255312, 'beta_0': 0.8356116248547167, 'beta_1': 0.9884014721714574, 'epsilon': 3.0029999522189606e-08, 'balanced_loss': False, 'epochs': 155, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 09:17:43,759] Trial 112 finished with value: 0.9212121212121213 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.995074301114471, 'batch_size': 61, 'attention_heads': 5, 'hidden_dimension': 96, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3184679473000712, 'global_pooling': 'sum', 'learning_rate': 0.0008281737669004742, 'weight_decay': 0.0001446131563670041, 'beta_0': 0.8758574935696213, 'beta_1': 0.9889933620972772, 'epsilon': 4.3410903680611014e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 09:38:12,254] Trial 113 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9914900795832443, 'batch_size': 21, 'attention_heads': 6, 'hidden_dimension': 121, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3831409009324843, 'global_pooling': 'sum', 'learning_rate': 0.0006108902500029883, 'weight_decay': 4.2273982437489996e-05, 'beta_0': 0.884780452895829, 'beta_1': 0.9851612359736946, 'epsilon': 1.9037387676219378e-08, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 09:58:10,702] Trial 114 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9897523742788934, 'batch_size': 16, 'attention_heads': 7, 'hidden_dimension': 112, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38864885883945594, 'global_pooling': 'sum', 'learning_rate': 0.0011719901792306049, 'weight_decay': 5.20331258781085e-05, 'beta_0': 0.8825878346512742, 'beta_1': 0.9844810334518542, 'epsilon': 1.2094105194287406e-08, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 10:16:32,312] Trial 115 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9915168175749222, 'batch_size': 20, 'attention_heads': 6, 'hidden_dimension': 105, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3411454113719233, 'global_pooling': 'sum', 'learning_rate': 0.0025642986241697302, 'weight_decay': 9.512757438200363e-05, 'beta_0': 0.8714048733958426, 'beta_1': 0.9880067372737905, 'epsilon': 2.1241196582233254e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 10:33:54,296] Trial 116 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9978327523320341, 'batch_size': 20, 'attention_heads': 6, 'hidden_dimension': 124, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33258133655549504, 'global_pooling': 'sum', 'learning_rate': 0.0009391769747090461, 'weight_decay': 4.581108751268221e-05, 'beta_0': 0.85184981398859, 'beta_1': 0.9854550056468903, 'epsilon': 1.7003950799662e-08, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 10:51:57,049] Trial 117 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9883811494468318, 'batch_size': 22, 'attention_heads': 5, 'hidden_dimension': 69, 'number_of_hidden_layers': 2, 'dropout_rate': 0.412202064053305, 'global_pooling': 'sum', 'learning_rate': 0.0037926563569731717, 'weight_decay': 2.6551766423262824e-05, 'beta_0': 0.8738619187964244, 'beta_1': 0.9857339486331347, 'epsilon': 3.238940251720022e-08, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.77 GiB. GPU 0 has a total capacity of 44.56 GiB of which 200.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 33.97 GiB is allocated by PyTorch, and 9.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 11:08:44,665] Trial 118 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9772521559773012, 'batch_size': 23, 'attention_heads': 7, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3512818690648898, 'global_pooling': 'sum', 'learning_rate': 0.0002650259511953668, 'weight_decay': 1.7321696623879105e-05, 'beta_0': 0.84511301176946, 'beta_1': 0.9903310032597815, 'epsilon': 1.4992702570244045e-08, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 11:27:01,493] Trial 119 finished with value: 0.9333333333333333 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.993708733242231, 'batch_size': 48, 'attention_heads': 4, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3683706842686773, 'global_pooling': 'sum', 'learning_rate': 0.00013166756748157733, 'weight_decay': 3.4511260346834886e-05, 'beta_0': 0.8779005406505032, 'beta_1': 0.9953749462950525, 'epsilon': 3.907264341052047e-08, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.66 GiB. GPU 0 has a total capacity of 44.56 GiB of which 468.69 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 33.83 GiB is allocated by PyTorch, and 9.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 11:43:51,278] Trial 120 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9846787961349267, 'batch_size': 19, 'attention_heads': 10, 'hidden_dimension': 130, 'number_of_hidden_layers': 3, 'dropout_rate': 0.31554742651051365, 'global_pooling': 'sum', 'learning_rate': 0.0017391265215315583, 'weight_decay': 0.0009834083301549583, 'beta_0': 0.8829600330559593, 'beta_1': 0.9872195241157801, 'epsilon': 5.790498260717596e-08, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 12:00:39,244] Trial 121 finished with value: 0.9454545454545454 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9958024363461057, 'batch_size': 26, 'attention_heads': 6, 'hidden_dimension': 89, 'number_of_hidden_layers': 1, 'dropout_rate': 0.33635727511930336, 'global_pooling': 'sum', 'learning_rate': 0.0006826120007436722, 'weight_decay': 5.3211289498826505e-05, 'beta_0': 0.8681788003022752, 'beta_1': 0.9984261056689363, 'epsilon': 2.448844541105713e-08, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 10, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 43.24 GiB memory in use. Of the allocated memory 33.38 GiB is allocated by PyTorch, and 8.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 12:17:23,994] Trial 122 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.981406686125653, 'batch_size': 24, 'attention_heads': 7, 'hidden_dimension': 118, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3002429890120085, 'global_pooling': 'sum', 'learning_rate': 0.000559211182517491, 'weight_decay': 2.0944083469051757e-05, 'beta_0': 0.8656994682333058, 'beta_1': 0.990012065635714, 'epsilon': 1.1964454033232087e-08, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 12, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 12:35:20,702] Trial 123 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9793639898948686, 'batch_size': 23, 'attention_heads': 6, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5689038018809257, 'global_pooling': 'sum', 'learning_rate': 0.0005189292287151627, 'weight_decay': 0.0004177696980906361, 'beta_0': 0.8591385905867208, 'beta_1': 0.9863376514127675, 'epsilon': 2.2135663690736792e-08, 'balanced_loss': False, 'epochs': 125, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 12:53:15,646] Trial 124 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9861436617394979, 'batch_size': 21, 'attention_heads': 5, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5352403586665998, 'global_pooling': 'sum', 'learning_rate': 0.0004402809488746173, 'weight_decay': 0.000662395387835475, 'beta_0': 0.8549090685842677, 'beta_1': 0.9894515966171933, 'epsilon': 2.6747035681963295e-08, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 13:10:54,719] Trial 125 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9865399389930382, 'batch_size': 21, 'attention_heads': 5, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5225694849436736, 'global_pooling': 'sum', 'learning_rate': 0.00042257926107608265, 'weight_decay': 6.673943101726567e-05, 'beta_0': 0.8541608077765269, 'beta_1': 0.9893120339993529, 'epsilon': 1.7522576245437807e-08, 'balanced_loss': False, 'epochs': 130, 'early_stopping_patience': 11, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 13:28:42,148] Trial 126 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9898187687244174, 'batch_size': 18, 'attention_heads': 5, 'hidden_dimension': 53, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5348500628804982, 'global_pooling': 'sum', 'learning_rate': 0.0013516462327351428, 'weight_decay': 0.000663016558576771, 'beta_0': 0.8481596543137527, 'beta_1': 0.9887730629979911, 'epsilon': 3.07385206639795e-08, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 5}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 13:46:31,479] Trial 127 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9823414309750109, 'batch_size': 22, 'attention_heads': 5, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5071281333161326, 'global_pooling': 'sum', 'learning_rate': 0.0010297117827878209, 'weight_decay': 1.5363787840380413e-05, 'beta_0': 0.8607832897122621, 'beta_1': 0.9912203425213074, 'epsilon': 4.43152165822279e-08, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 14, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 14:06:02,168] Trial 128 finished with value: 0.9515151515151515 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9875437487296912, 'batch_size': 17, 'attention_heads': 4, 'hidden_dimension': 148, 'number_of_hidden_layers': 2, 'dropout_rate': 0.347199013637522, 'global_pooling': 'sum', 'learning_rate': 0.0002309549488949466, 'weight_decay': 0.0006589901124862151, 'beta_0': 0.8558528761105416, 'beta_1': 0.9895668492979097, 'epsilon': 2.5123245573812545e-08, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 11, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 2.72 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 43.01 GiB memory in use. Of the allocated memory 34.73 GiB is allocated by PyTorch, and 7.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 14:23:07,980] Trial 129 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9748366187126649, 'batch_size': 30, 'attention_heads': 6, 'hidden_dimension': 137, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5477473510094332, 'global_pooling': 'sum', 'learning_rate': 0.0008078602867874201, 'weight_decay': 3.1926943612089953e-05, 'beta_0': 0.8504844914550963, 'beta_1': 0.9904646763847169, 'epsilon': 3.49311886075269e-08, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 14:40:57,120] Trial 130 finished with value: 0.9454545454545454 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9923057204380045, 'batch_size': 20, 'attention_heads': 5, 'hidden_dimension': 67, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5393973694745231, 'global_pooling': 'mean', 'learning_rate': 0.00015526719282011085, 'weight_decay': 1.8597288590087593e-05, 'beta_0': 0.8640202064591312, 'beta_1': 0.9876970513940135, 'epsilon': 1.8485695781198585e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 15:00:33,943] Trial 131 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9839165265349539, 'batch_size': 25, 'attention_heads': 7, 'hidden_dimension': 85, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32480119301444726, 'global_pooling': 'sum', 'learning_rate': 0.0023264671157999777, 'weight_decay': 1.3157531656952849e-05, 'beta_0': 0.8885862165917943, 'beta_1': 0.988279171696391, 'epsilon': 2.8661212922227327e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 13, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 15:19:58,573] Trial 132 finished with value: 0.9272727272727272 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9899177348605658, 'batch_size': 27, 'attention_heads': 6, 'hidden_dimension': 96, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5558112745529337, 'global_pooling': 'sum', 'learning_rate': 0.000330769370842573, 'weight_decay': 0.0008072548041951664, 'beta_0': 0.8623647173491451, 'beta_1': 0.9977123428881977, 'epsilon': 1.3537270258655802e-08, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 15:38:06,159] Trial 133 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9852986698976259, 'batch_size': 24, 'attention_heads': 6, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5639433412587616, 'global_pooling': 'sum', 'learning_rate': 0.0005034122403356375, 'weight_decay': 0.00037899322370678123, 'beta_0': 0.8397673919033578, 'beta_1': 0.9988753713272296, 'epsilon': 2.403779582896579e-08, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 15:56:35,418] Trial 134 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.979695545848198, 'batch_size': 22, 'attention_heads': 6, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5589278286813375, 'global_pooling': 'sum', 'learning_rate': 0.0004170898853156564, 'weight_decay': 0.0006111560890024593, 'beta_0': 0.85334980548547, 'beta_1': 0.9977772110499433, 'epsilon': 4.8610133979893125e-08, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 16:14:04,508] Trial 135 finished with value: 0.9212121212121213 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.977222439296428, 'batch_size': 23, 'attention_heads': 5, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5804781555456863, 'global_pooling': 'sum', 'learning_rate': 0.000637921701375857, 'weight_decay': 0.0004887795834401017, 'beta_0': 0.8470912559627516, 'beta_1': 0.9967685485812567, 'epsilon': 1.578965778134267e-08, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 11, 'plateau_patience': 23, 'plateau_divider': 4}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 2.56 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.30 GiB is free. Including non-PyTorch memory, this process has 42.26 GiB memory in use. Of the allocated memory 35.99 GiB is allocated by PyTorch, and 5.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 16:29:38,481] Trial 136 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9719872827823939, 'batch_size': 21, 'attention_heads': 5, 'hidden_dimension': 50, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5767285432298092, 'global_pooling': 'sum', 'learning_rate': 0.0011503929619326064, 'weight_decay': 0.0004019790502046279, 'beta_0': 0.8590053898527705, 'beta_1': 0.9981560302612389, 'epsilon': 1.0044330709417099e-08, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 2.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 37.22 GiB is allocated by PyTorch, and 6.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 16:40:06,878] Trial 137 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.942135278733363, 'batch_size': 19, 'attention_heads': 6, 'hidden_dimension': 78, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3556325156614351, 'global_pooling': 'mean', 'learning_rate': 0.0015195466291738932, 'weight_decay': 0.00020744125425840268, 'beta_0': 0.8693529323748239, 'beta_1': 0.9989828689610365, 'epsilon': 5.989519819934635e-07, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 13, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 824.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 192.69 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 35.06 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 16:47:00,897] Trial 138 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.913664803275008, 'batch_size': 28, 'attention_heads': 7, 'hidden_dimension': 57, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3396063317935781, 'global_pooling': 'sum', 'learning_rate': 0.0003620740103103249, 'weight_decay': 0.0002917794719187199, 'beta_0': 0.8567324768400705, 'beta_1': 0.9889028804448042, 'epsilon': 2.0618737886720087e-08, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 17:06:40,847] Trial 139 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9829282321193022, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42664522694875956, 'global_pooling': 'sum', 'learning_rate': 0.00075955241279189, 'weight_decay': 0.0005489217816444926, 'beta_0': 0.8427902716608392, 'beta_1': 0.9896187256053914, 'epsilon': 3.8733319522419236e-08, 'balanced_loss': False, 'epochs': 125, 'early_stopping_patience': 11, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 17:24:42,767] Trial 140 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9926906214557889, 'batch_size': 25, 'attention_heads': 6, 'hidden_dimension': 112, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5278460442448867, 'global_pooling': 'sum', 'learning_rate': 0.0005081074856151321, 'weight_decay': 0.0007274504738493386, 'beta_0': 0.8807019154140848, 'beta_1': 0.9900467856761173, 'epsilon': 2.488488158494976e-08, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 10, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 17:42:56,642] Trial 141 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9866462910534057, 'batch_size': 23, 'attention_heads': 5, 'hidden_dimension': 61, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5146621640993787, 'global_pooling': 'mean', 'learning_rate': 0.0031021875756326455, 'weight_decay': 2.2391376029023565e-05, 'beta_0': 0.8653599649489485, 'beta_1': 0.9870279584429945, 'epsilon': 2.978707316902049e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 754.69 MiB is free. Including non-PyTorch memory, this process has 43.82 GiB memory in use. Of the allocated memory 33.93 GiB is allocated by PyTorch, and 8.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 18:00:35,815] Trial 142 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.980786225365988, 'batch_size': 22, 'attention_heads': 7, 'hidden_dimension': 92, 'number_of_hidden_layers': 3, 'dropout_rate': 0.33092842685395896, 'global_pooling': 'sum', 'learning_rate': 0.001866114786998899, 'weight_decay': 0.00016043280481524994, 'beta_0': 0.8717788467379276, 'beta_1': 0.9842894319157289, 'epsilon': 8.566368994245478e-08, 'balanced_loss': False, 'epochs': 194, 'early_stopping_patience': 17, 'plateau_patience': 21, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 18:18:20,453] Trial 143 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9968922761443265, 'batch_size': 24, 'attention_heads': 5, 'hidden_dimension': 77, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3297503478195446, 'global_pooling': 'sum', 'learning_rate': 0.0008828239385883913, 'weight_decay': 2.791706077162109e-05, 'beta_0': 0.8785168149435756, 'beta_1': 0.9867909789641984, 'epsilon': 3.655592062324071e-08, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 10, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 18:36:01,390] Trial 144 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9985168605338722, 'batch_size': 20, 'attention_heads': 4, 'hidden_dimension': 80, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30906544537766534, 'global_pooling': 'sum', 'learning_rate': 0.0008945488724167015, 'weight_decay': 2.5389478952283826e-05, 'beta_0': 0.8752382318007838, 'beta_1': 0.9862959475533688, 'epsilon': 6.424017162454343e-08, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 10, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 18:53:52,711] Trial 145 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9952986292557041, 'batch_size': 27, 'attention_heads': 5, 'hidden_dimension': 47, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32067903953301935, 'global_pooling': 'sum', 'learning_rate': 0.001233877637058287, 'weight_decay': 2.1740692804501005e-05, 'beta_0': 0.8853278189197348, 'beta_1': 0.9853968215900202, 'epsilon': 1.7624919090149155e-08, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 11, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 868.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 190.69 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 37.11 GiB is allocated by PyTorch, and 6.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 19:02:31,070] Trial 146 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9292118586733928, 'batch_size': 29, 'attention_heads': 6, 'hidden_dimension': 73, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4567088420618015, 'global_pooling': 'sum', 'learning_rate': 0.0006178504577021582, 'weight_decay': 1.727415402804463e-05, 'beta_0': 0.8763828600423376, 'beta_1': 0.9884853045668178, 'epsilon': 4.968199295222016e-08, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 19:21:13,510] Trial 147 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9888665182522905, 'batch_size': 26, 'attention_heads': 5, 'hidden_dimension': 64, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34285745758391667, 'global_pooling': 'sum', 'learning_rate': 0.0010333723733744265, 'weight_decay': 1.1202729254337226e-05, 'beta_0': 0.8574542778840578, 'beta_1': 0.9877717158041893, 'epsilon': 3.418660011251898e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 19:39:50,543] Trial 148 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9939922035691217, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 84, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40102299956724924, 'global_pooling': 'sum', 'learning_rate': 0.0002739273066963093, 'weight_decay': 0.00033990310383465123, 'beta_0': 0.8630113925638686, 'beta_1': 0.9874213100515231, 'epsilon': 1.9998590137537416e-08, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 19:57:21,323] Trial 149 finished with value: 0.9454545454545454 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9908868356088106, 'batch_size': 23, 'attention_heads': 5, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3318527077444539, 'global_pooling': 'mean', 'learning_rate': 0.00044455374063084315, 'weight_decay': 3.081436238533179e-05, 'beta_0': 0.8789001564114205, 'beta_1': 0.9866240211663001, 'epsilon': 2.6347682227514598e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 10, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 20:15:07,450] Trial 150 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9979413549052152, 'batch_size': 21, 'attention_heads': 6, 'hidden_dimension': 89, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3150220168832457, 'global_pooling': 'sum', 'learning_rate': 0.0006888911357753317, 'weight_decay': 0.00011654850276422275, 'beta_0': 0.8735568356899913, 'beta_1': 0.9892013031977956, 'epsilon': 1.4450773978688658e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 11, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 20:33:48,101] Trial 151 finished with value: 0.9393939393939394 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9848308234554348, 'batch_size': 25, 'attention_heads': 6, 'hidden_dimension': 71, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35047875150768926, 'global_pooling': 'sum', 'learning_rate': 0.001521636531830626, 'weight_decay': 0.000996864510692378, 'beta_0': 0.8497102776650768, 'beta_1': 0.9859403932794648, 'epsilon': 4.077872000083568e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 12, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 490.69 MiB is free. Including non-PyTorch memory, this process has 44.07 GiB memory in use. Of the allocated memory 37.32 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-20 20:46:23,229] Trial 152 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9563051844171502, 'batch_size': 24, 'attention_heads': 6, 'hidden_dimension': 76, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32418677460870904, 'global_pooling': 'sum', 'learning_rate': 0.006053685305079628, 'weight_decay': 3.8501419993722556e-05, 'beta_0': 0.8664530280248085, 'beta_1': 0.9846983959386721, 'epsilon': 1.2183501922015682e-08, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 10, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 21:04:57,371] Trial 153 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9824201104259405, 'batch_size': 22, 'attention_heads': 5, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5400316731436933, 'global_pooling': 'sum', 'learning_rate': 0.0010364819659361283, 'weight_decay': 1.3896550295329028e-05, 'beta_0': 0.8622092108783118, 'beta_1': 0.9913521542385781, 'epsilon': 4.621331444078509e-08, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 14, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 21:23:44,890] Trial 154 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9788434513037334, 'batch_size': 22, 'attention_heads': 5, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5551307717255914, 'global_pooling': 'sum', 'learning_rate': 0.0008364772827383607, 'weight_decay': 2.011299105014657e-05, 'beta_0': 0.8599314107707122, 'beta_1': 0.9912838379623956, 'epsilon': 2.258811185049926e-08, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 14, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
[I 2024-12-20 21:42:07,696] Trial 155 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9823959030465919, 'batch_size': 20, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5201576384088419, 'global_pooling': 'sum', 'learning_rate': 0.0020400946515684824, 'weight_decay': 1.6291504398113585e-05, 'beta_0': 0.8612758142931037, 'beta_1': 0.9908334637655198, 'epsilon': 2.9846735494140507e-08, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 91 with value: 0.9696969696969697.
slurmstepd: error: *** JOB 14126171 ON gpu013 CANCELLED AT 2024-12-20T21:56:25 DUE TO TIME LIMIT ***
