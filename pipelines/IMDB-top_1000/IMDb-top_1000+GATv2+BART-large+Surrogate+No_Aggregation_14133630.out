[I 2024-12-22 13:18:49,465] Using an existing study with name 'IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 43.04 GiB memory in use. Of the allocated memory 31.02 GiB is allocated by PyTorch, and 10.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 13:44:02,421] Trial 212 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9800777086894545, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41388127302606703, 'global_pooling': 'mean', 'learning_rate': 8.216481971647396e-05, 'weight_decay': 0.0002460125445767187, 'beta_0': 0.8544478167629275, 'beta_1': 0.9950232728820742, 'epsilon': 1.3942064712112693e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 14:15:27,400] Trial 213 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9820808811080726, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39211430781678286, 'global_pooling': 'mean', 'learning_rate': 6.32667013180726e-05, 'weight_decay': 0.0003540862048208361, 'beta_0': 0.8574232702812252, 'beta_1': 0.9928687515225106, 'epsilon': 1.3123437730612325e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 700.69 MiB is free. Including non-PyTorch memory, this process has 43.87 GiB memory in use. Of the allocated memory 35.62 GiB is allocated by PyTorch, and 7.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 14:40:27,492] Trial 214 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9777977579789069, 'batch_size': 38, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38943205636344863, 'global_pooling': 'mean', 'learning_rate': 6.939661835800527e-05, 'weight_decay': 0.0003458315545073149, 'beta_0': 0.8576801499198918, 'beta_1': 0.9927947178492367, 'epsilon': 1.7410454542789692e-08, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 43.03 GiB memory in use. Of the allocated memory 36.19 GiB is allocated by PyTorch, and 5.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 15:05:26,098] Trial 215 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9738469536891086, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39443472161282683, 'global_pooling': 'mean', 'learning_rate': 3.9853701260587555e-05, 'weight_decay': 0.0002783017492220442, 'beta_0': 0.8539860663211875, 'beta_1': 0.9935479237557278, 'epsilon': 1.2681181791303233e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 15:36:51,937] Trial 216 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9827230948176167, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40932980461914403, 'global_pooling': 'mean', 'learning_rate': 5.7405400620232426e-05, 'weight_decay': 0.00035166628611530505, 'beta_0': 0.857804606103245, 'beta_1': 0.9931681990584248, 'epsilon': 1.3935434097315289e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 16:10:05,679] Trial 217 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9821463224852442, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3647555239993486, 'global_pooling': 'mean', 'learning_rate': 4.251395537945005e-05, 'weight_decay': 0.0004174108954058162, 'beta_0': 0.8480286382160818, 'beta_1': 0.9944202006601077, 'epsilon': 1.6653263119404317e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 16:43:22,545] Trial 218 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9812832761687332, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3659926702089447, 'global_pooling': 'mean', 'learning_rate': 4.390873579063833e-05, 'weight_decay': 0.00039880085840698096, 'beta_0': 0.8482367755834916, 'beta_1': 0.9944543562902018, 'epsilon': 1.7501552917911357e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 17:17:04,572] Trial 219 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.980349955366803, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36599887310869045, 'global_pooling': 'mean', 'learning_rate': 4.443677747780228e-05, 'weight_decay': 0.00021978122026775822, 'beta_0': 0.8482825569273506, 'beta_1': 0.9942892041234476, 'epsilon': 1.2543528033210345e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 17:50:52,628] Trial 220 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9806482037955762, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3669763649563828, 'global_pooling': 'mean', 'learning_rate': 3.388004087060855e-05, 'weight_decay': 0.00021184518235333805, 'beta_0': 0.8477375127390846, 'beta_1': 0.9945019000418447, 'epsilon': 1.2367370775659646e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 42.88 GiB memory in use. Of the allocated memory 35.21 GiB is allocated by PyTorch, and 6.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 18:16:09,636] Trial 221 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9782929908453406, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3854103933562554, 'global_pooling': 'mean', 'learning_rate': 4.307479440052833e-05, 'weight_decay': 0.00031075570718327376, 'beta_0': 0.8453287303085661, 'beta_1': 0.994131407053885, 'epsilon': 1.6845491681768353e-08, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 18:47:52,842] Trial 222 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9819459615580056, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3653885935313919, 'global_pooling': 'mean', 'learning_rate': 5.836677739382604e-05, 'weight_decay': 0.00038638376688596425, 'beta_0': 0.8479261226427101, 'beta_1': 0.9953552911771232, 'epsilon': 1.0129809991823835e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 19:21:18,298] Trial 223 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.982234285690765, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3646855819264772, 'global_pooling': 'mean', 'learning_rate': 2.459200832830323e-05, 'weight_decay': 0.0004030496132427637, 'beta_0': 0.8475806675717705, 'beta_1': 0.9954098495828361, 'epsilon': 1.0115966985247102e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 19:51:18,422] Trial 224 finished with value: 0.9515151515151515 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9768133625549588, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39229077346086205, 'global_pooling': 'mean', 'learning_rate': 5.8790427981584435e-05, 'weight_decay': 0.00035139894660772775, 'beta_0': 0.8487305162467109, 'beta_1': 0.9950328053598257, 'epsilon': 1.2721939189411542e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 20:21:59,700] Trial 225 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9821410467469217, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3691943617043025, 'global_pooling': 'mean', 'learning_rate': 6.103608523398955e-05, 'weight_decay': 0.0006149133397928408, 'beta_0': 0.8430290132289827, 'beta_1': 0.9940336957996588, 'epsilon': 1.2292691966940786e-08, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 20:54:55,078] Trial 226 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9800449963055683, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3825824300562939, 'global_pooling': 'mean', 'learning_rate': 4.855932015458496e-05, 'weight_decay': 0.0004001283031634922, 'beta_0': 0.8537211972488526, 'beta_1': 0.9944567677285007, 'epsilon': 1.5941152306686194e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 564.69 MiB is free. Including non-PyTorch memory, this process has 44.00 GiB memory in use. Of the allocated memory 29.92 GiB is allocated by PyTorch, and 12.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 21:22:14,246] Trial 227 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9797274960114551, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3835034277554973, 'global_pooling': 'mean', 'learning_rate': 4.6274033321675434e-05, 'weight_decay': 0.0003988475219305869, 'beta_0': 0.8534590038751947, 'beta_1': 0.9943749356553697, 'epsilon': 1.0046004069770991e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 21:52:51,367] Trial 228 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.982574604736714, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36122878358324223, 'global_pooling': 'mean', 'learning_rate': 8.140701551378352e-05, 'weight_decay': 0.0004106378530361959, 'beta_0': 0.8555316418794614, 'beta_1': 0.9936675933029487, 'epsilon': 1.6450028811783212e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 22:25:33,690] Trial 229 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9803191846047412, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39984057634913944, 'global_pooling': 'mean', 'learning_rate': 5.296912613902263e-05, 'weight_decay': 0.0003007072017773359, 'beta_0': 0.8482344105462218, 'beta_1': 0.9933773411671427, 'epsilon': 1.351213080945556e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 42.87 GiB memory in use. Of the allocated memory 34.93 GiB is allocated by PyTorch, and 6.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 22:51:00,951] Trial 230 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9787258087320945, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 38, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3825434253482377, 'global_pooling': 'mean', 'learning_rate': 3.580055750854914e-05, 'weight_decay': 0.0005648582019825727, 'beta_0': 0.8538014333877868, 'beta_1': 0.9947954788987875, 'epsilon': 1.2212155990005159e-08, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-22 23:21:23,740] Trial 231 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9826518916440113, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3678520532704148, 'global_pooling': 'mean', 'learning_rate': 8.816701702733977e-05, 'weight_decay': 0.00036535775624123735, 'beta_0': 0.8464890516224758, 'beta_1': 0.9955224673208041, 'epsilon': 1.6336560887721453e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 43.02 GiB memory in use. Of the allocated memory 34.97 GiB is allocated by PyTorch, and 6.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-22 23:46:54,020] Trial 232 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9754239300524078, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3881906920050976, 'global_pooling': 'mean', 'learning_rate': 7.197049874636784e-05, 'weight_decay': 0.00041526518567805085, 'beta_0': 0.8486183748814999, 'beta_1': 0.9927913929698136, 'epsilon': 1.508230569689333e-05, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 20, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 00:19:45,561] Trial 233 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9803758411814774, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 38, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35535219355734393, 'global_pooling': 'mean', 'learning_rate': 4.44186923631193e-05, 'weight_decay': 0.0005487462983723948, 'beta_0': 0.8446331768700358, 'beta_1': 0.994002701354906, 'epsilon': 1.0002437093736682e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 00:51:35,459] Trial 234 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9825512933459558, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3646389734278451, 'global_pooling': 'mean', 'learning_rate': 6.140331514876498e-05, 'weight_decay': 0.0003302538435965436, 'beta_0': 0.8559576612953493, 'beta_1': 0.9945329886074664, 'epsilon': 1.417168078178245e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 01:22:45,864] Trial 235 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9817799709755087, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39323602714113676, 'global_pooling': 'mean', 'learning_rate': 0.00012245970916860706, 'weight_decay': 0.0002771315089970531, 'beta_0': 0.8529140506675901, 'beta_1': 0.9937216603785859, 'epsilon': 1.2089392938458634e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 01:55:29,626] Trial 236 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9802849164123072, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.370289752382645, 'global_pooling': 'mean', 'learning_rate': 5.2338947481353526e-05, 'weight_decay': 0.00030701887189247857, 'beta_0': 0.8492049392373717, 'beta_1': 0.9932896156957542, 'epsilon': 1.442920050077819e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 702.69 MiB is free. Including non-PyTorch memory, this process has 43.87 GiB memory in use. Of the allocated memory 35.48 GiB is allocated by PyTorch, and 7.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 02:21:01,885] Trial 237 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9779400759832404, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40417475091147803, 'global_pooling': 'mean', 'learning_rate': 5.266202776566059e-05, 'weight_decay': 0.00037815931307643894, 'beta_0': 0.8475942379545399, 'beta_1': 0.9934382660587131, 'epsilon': 1.2591890783593387e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 02:52:00,124] Trial 238 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9807050983793435, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39907872450743515, 'global_pooling': 'mean', 'learning_rate': 7.918731624037444e-05, 'weight_decay': 0.00041504079415605926, 'beta_0': 0.8538247234250699, 'beta_1': 0.993132547772701, 'epsilon': 1.707808385962249e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 03:25:06,703] Trial 239 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9834032597238466, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38135741510132154, 'global_pooling': 'mean', 'learning_rate': 3.130027286674418e-05, 'weight_decay': 0.0006644704996760211, 'beta_0': 0.848708266001553, 'beta_1': 0.9927954387035182, 'epsilon': 1.3657308616027123e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 826.69 MiB is free. Including non-PyTorch memory, this process has 43.75 GiB memory in use. Of the allocated memory 34.72 GiB is allocated by PyTorch, and 7.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 03:37:09,089] Trial 240 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9337093177237574, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39661945582291425, 'global_pooling': 'mean', 'learning_rate': 9.178977171544759e-05, 'weight_decay': 0.000310509060652907, 'beta_0': 0.8460874450255257, 'beta_1': 0.9939536892456257, 'epsilon': 1.204877256280699e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 42.87 GiB memory in use. Of the allocated memory 34.68 GiB is allocated by PyTorch, and 7.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 04:02:40,923] Trial 241 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9789884751153841, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3909172937987975, 'global_pooling': 'mean', 'learning_rate': 4.278408174246814e-05, 'weight_decay': 0.0005304520809051404, 'beta_0': 0.8497077927596273, 'beta_1': 0.9943377812888741, 'epsilon': 1.7991216651076426e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 44.39 GiB memory in use. Of the allocated memory 36.89 GiB is allocated by PyTorch, and 6.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 04:20:36,321] Trial 242 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.950148080891, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38382106533123733, 'global_pooling': 'mean', 'learning_rate': 5.9904606158292405e-05, 'weight_decay': 0.0003541397356628466, 'beta_0': 0.8531793401041755, 'beta_1': 0.9935411097561607, 'epsilon': 1.4963957451875862e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 04:51:08,547] Trial 243 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9872746730953288, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37151472143976255, 'global_pooling': 'mean', 'learning_rate': 6.748604206165768e-05, 'weight_decay': 0.00043261554348847505, 'beta_0': 0.8570623548221479, 'beta_1': 0.9947177958033232, 'epsilon': 1.003867459332993e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
slurmstepd: error: *** JOB 14133630 ON gpu039 CANCELLED AT 2024-12-23T05:18:48 DUE TO TIME LIMIT ***
