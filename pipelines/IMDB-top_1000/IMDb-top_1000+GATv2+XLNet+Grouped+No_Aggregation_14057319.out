[I 2024-12-04 04:45:55,806] Using an existing study with name 'IMDb-top_1000-GATv2-xlnet-xlnet-base-cased-Grouped-No_Aggregation' instead of creating a new one.
[I 2024-12-04 04:55:50,313] Trial 177 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.947550976107744, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 46, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5581450423434522, 'global_pooling': 'max', 'learning_rate': 0.01063381265756562, 'weight_decay': 1.2691125489179179e-05, 'beta_0': 0.8553042061417376, 'beta_1': 0.9890594692670495, 'epsilon': 3.192284605963296e-08, 'balanced_loss': True, 'epochs': 155, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 05:05:30,611] Trial 178 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9465727359379801, 'batch_size': 42, 'attention_heads': 12, 'hidden_dimension': 42, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5811691147789827, 'global_pooling': 'max', 'learning_rate': 0.005943859447509374, 'weight_decay': 1.0738369413043921e-05, 'beta_0': 0.852373557266362, 'beta_1': 0.9885462744004363, 'epsilon': 1.6309359671714454e-08, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 05:15:37,555] Trial 179 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9418855434213318, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5407911886192647, 'global_pooling': 'max', 'learning_rate': 0.007936503629403837, 'weight_decay': 1.8741138870730608e-05, 'beta_0': 0.8471225215714543, 'beta_1': 0.9900270750633834, 'epsilon': 1.3865689740354476e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.32 GiB is free. Including non-PyTorch memory, this process has 39.24 GiB memory in use. Of the allocated memory 22.14 GiB is allocated by PyTorch, and 15.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 05:23:55,227] Trial 180 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9422605367339939, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.546271813820636, 'global_pooling': 'max', 'learning_rate': 0.011956689502394782, 'weight_decay': 1.8352480416716205e-05, 'beta_0': 0.8481624740391245, 'beta_1': 0.9900506130014475, 'epsilon': 2.1119634334849765e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 05:33:56,042] Trial 181 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9413303438243675, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5534167007278767, 'global_pooling': 'max', 'learning_rate': 0.007817795321768786, 'weight_decay': 2.3193729234967404e-05, 'beta_0': 0.8461767542624254, 'beta_1': 0.9895051029359447, 'epsilon': 1.4117861327572522e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 05:43:54,182] Trial 182 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9404178075125289, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5700825528441983, 'global_pooling': 'max', 'learning_rate': 0.008239971860446525, 'weight_decay': 2.2452154859325868e-05, 'beta_0': 0.8455604340648767, 'beta_1': 0.9879854693993466, 'epsilon': 1.3782007054228617e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 05:54:05,311] Trial 183 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9442845059912587, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5383252486524122, 'global_pooling': 'max', 'learning_rate': 0.006758359097009273, 'weight_decay': 2.541066234489788e-05, 'beta_0': 0.8412079938992634, 'beta_1': 0.9894157685693314, 'epsilon': 1.3377824721846516e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.91 GiB is free. Including non-PyTorch memory, this process has 40.64 GiB memory in use. Of the allocated memory 28.16 GiB is allocated by PyTorch, and 11.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 06:02:21,522] Trial 184 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9421369718367407, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5500355447405083, 'global_pooling': 'max', 'learning_rate': 0.005117166414915845, 'weight_decay': 2.1343744984578634e-05, 'beta_0': 0.8424211058823748, 'beta_1': 0.9890021171833999, 'epsilon': 2.5885119608605238e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 06:12:07,258] Trial 185 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9450976622617641, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5278239453938528, 'global_pooling': 'max', 'learning_rate': 0.014897467408187143, 'weight_decay': 1.8078877064070922e-05, 'beta_0': 0.8507770995402265, 'beta_1': 0.9896034815252248, 'epsilon': 1.8143074072894123e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 06:22:24,122] Trial 186 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9421288667150932, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5538523495754503, 'global_pooling': 'max', 'learning_rate': 0.006024685067249833, 'weight_decay': 1.3759202086365176e-05, 'beta_0': 0.8582851519789625, 'beta_1': 0.9887057614625455, 'epsilon': 3.265531154778997e-08, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.67 GiB is free. Including non-PyTorch memory, this process has 41.88 GiB memory in use. Of the allocated memory 28.45 GiB is allocated by PyTorch, and 12.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 06:30:40,731] Trial 187 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9399571382046004, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5777106472768692, 'global_pooling': 'max', 'learning_rate': 0.0077787311895293094, 'weight_decay': 8.759484206313969e-06, 'beta_0': 0.8488939174258825, 'beta_1': 0.9883495647694132, 'epsilon': 8.582710644029867e-07, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.78 GiB is free. Including non-PyTorch memory, this process has 39.77 GiB memory in use. Of the allocated memory 26.97 GiB is allocated by PyTorch, and 11.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 06:38:58,629] Trial 188 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9501360011502058, 'batch_size': 39, 'attention_heads': 12, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5442724824083711, 'global_pooling': 'max', 'learning_rate': 0.012748587988962789, 'weight_decay': 1.9713373727789634e-05, 'beta_0': 0.8592731570030164, 'beta_1': 0.9902712369687725, 'epsilon': 1.1993083040905312e-08, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 06:49:01,435] Trial 189 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9437183447487278, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5547091635067288, 'global_pooling': 'max', 'learning_rate': 0.009787156306776457, 'weight_decay': 2.6719904309838576e-05, 'beta_0': 0.8531528673546963, 'beta_1': 0.9893086994961677, 'epsilon': 1.428806576079452e-06, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 06:58:59,988] Trial 190 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.920855905926195, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5623061829191687, 'global_pooling': 'max', 'learning_rate': 0.010463410213908121, 'weight_decay': 2.7242527266648312e-05, 'beta_0': 0.8470520935332724, 'beta_1': 0.9888258491757654, 'epsilon': 2.513283489854971e-06, 'balanced_loss': False, 'epochs': 151, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 07:09:04,859] Trial 191 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9436841845364083, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5681794268908238, 'global_pooling': 'max', 'learning_rate': 0.008238866169446035, 'weight_decay': 1.6715040514027048e-05, 'beta_0': 0.8438055304394723, 'beta_1': 0.9899258129175674, 'epsilon': 9.871377837537145e-07, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 07:19:19,560] Trial 192 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9392135893829834, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5736210678081777, 'global_pooling': 'max', 'learning_rate': 0.010615426153997397, 'weight_decay': 2.3797520735575902e-05, 'beta_0': 0.8516877439836198, 'beta_1': 0.98915674904055, 'epsilon': 3.76822074042414e-08, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 07:29:31,789] Trial 193 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.945793264458177, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.555311698353988, 'global_pooling': 'max', 'learning_rate': 0.006612226486164856, 'weight_decay': 1.3772638093684141e-05, 'beta_0': 0.8538624704864639, 'beta_1': 0.9894265615852508, 'epsilon': 1.6386138367404835e-06, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 07:40:34,818] Trial 194 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9435460225727311, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5528545393610834, 'global_pooling': 'max', 'learning_rate': 0.005226314750458168, 'weight_decay': 1.1893204261191553e-05, 'beta_0': 0.849608428824258, 'beta_1': 0.988269692392879, 'epsilon': 2.02947359221837e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 07:50:48,619] Trial 195 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9420755866807344, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5414724680673789, 'global_pooling': 'max', 'learning_rate': 0.004416723215452173, 'weight_decay': 1.9649339888399995e-05, 'beta_0': 0.8498328708726051, 'beta_1': 0.9878470363652482, 'epsilon': 2.4877335838642576e-08, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 08:01:01,761] Trial 196 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9418891349368923, 'batch_size': 44, 'attention_heads': 12, 'hidden_dimension': 73, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5396722501026583, 'global_pooling': 'max', 'learning_rate': 0.004469647839186005, 'weight_decay': 1.1713633955917787e-05, 'beta_0': 0.8496949168521619, 'beta_1': 0.9877738812455218, 'epsilon': 1.2958226400514027e-06, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 08:11:25,993] Trial 197 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9408177385973261, 'batch_size': 46, 'attention_heads': 14, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5295377573583213, 'global_pooling': 'max', 'learning_rate': 0.01668427773970836, 'weight_decay': 1.6287956418479575e-05, 'beta_0': 0.8471002860351842, 'beta_1': 0.9880172532319822, 'epsilon': 7.567782452911901e-07, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 43.09 GiB memory in use. Of the allocated memory 27.94 GiB is allocated by PyTorch, and 14.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 08:19:44,204] Trial 198 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9436648210557771, 'batch_size': 43, 'attention_heads': 14, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5875082695452455, 'global_pooling': 'max', 'learning_rate': 0.005234633008807671, 'weight_decay': 2.7286139799853903e-05, 'beta_0': 0.8527661583805423, 'beta_1': 0.9882876813580984, 'epsilon': 2.8327847930365135e-08, 'balanced_loss': True, 'epochs': 176, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 08:29:52,508] Trial 199 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9378417471117669, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5628074285944181, 'global_pooling': 'max', 'learning_rate': 0.005044113673289651, 'weight_decay': 2.1771567016243835e-05, 'beta_0': 0.8518710339299304, 'beta_1': 0.9875781114714465, 'epsilon': 1.985197604946212e-06, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 08:40:10,905] Trial 200 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.94232222354497, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.546548040663523, 'global_pooling': 'max', 'learning_rate': 0.0068543598582353985, 'weight_decay': 0.0005743228306028058, 'beta_0': 0.848391782944046, 'beta_1': 0.9883868535835405, 'epsilon': 6.7335688907230085e-06, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 560.69 MiB is free. Including non-PyTorch memory, this process has 44.01 GiB memory in use. Of the allocated memory 28.28 GiB is allocated by PyTorch, and 14.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 08:48:28,902] Trial 201 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.941285328000003, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5412134884011948, 'global_pooling': 'max', 'learning_rate': 0.007054821631090527, 'weight_decay': 0.000799776822818057, 'beta_0': 0.8451210896136954, 'beta_1': 0.9882748259224167, 'epsilon': 7.116965922392414e-06, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 08:58:13,893] Trial 202 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9352452309555978, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5463236483643983, 'global_pooling': 'max', 'learning_rate': 0.004199561746778627, 'weight_decay': 1.348486071453359e-05, 'beta_0': 0.8490559438041385, 'beta_1': 0.9884558586553823, 'epsilon': 1.075632451489622e-06, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 09:08:27,773] Trial 203 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9433113033022633, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5575585952832536, 'global_pooling': 'max', 'learning_rate': 0.00926190409002776, 'weight_decay': 1.8035548079670047e-05, 'beta_0': 0.8390956112556283, 'beta_1': 0.9948227235629536, 'epsilon': 2.475247512280601e-06, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.63 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 43.29 GiB memory in use. Of the allocated memory 32.13 GiB is allocated by PyTorch, and 10.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 09:17:37,640] Trial 204 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.939123281596518, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 202, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5768536410192617, 'global_pooling': 'max', 'learning_rate': 0.00045946714009914195, 'weight_decay': 2.5413619475269687e-05, 'beta_0': 0.8503907943414161, 'beta_1': 0.9813248399230968, 'epsilon': 1.0822313083731e-05, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 09:27:55,856] Trial 205 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9427116810173642, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5837938094375383, 'global_pooling': 'max', 'learning_rate': 0.005635119265086254, 'weight_decay': 1.557472509814859e-05, 'beta_0': 0.8473076616555641, 'beta_1': 0.9806397533722929, 'epsilon': 5.900073506388495e-05, 'balanced_loss': True, 'epochs': 162, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 09:37:48,890] Trial 206 finished with value: 0.806060606060606 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9448252782941632, 'batch_size': 43, 'attention_heads': 14, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5662018866317432, 'global_pooling': 'max', 'learning_rate': 0.0123471574212282, 'weight_decay': 1.914277515272665e-05, 'beta_0': 0.8538487837401547, 'beta_1': 0.9888225680090372, 'epsilon': 3.698085686902307e-06, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 09:48:33,024] Trial 207 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9408056374022796, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5541737270939916, 'global_pooling': 'max', 'learning_rate': 0.0073380243539646495, 'weight_decay': 2.9149879349014233e-05, 'beta_0': 0.8484875942676597, 'beta_1': 0.9877375637120447, 'epsilon': 1.9869586504590102e-05, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 09:58:44,006] Trial 208 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9375388269184103, 'batch_size': 42, 'attention_heads': 12, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5550619071252979, 'global_pooling': 'max', 'learning_rate': 0.006954908126855871, 'weight_decay': 3.259525997793443e-05, 'beta_0': 0.8428815835539872, 'beta_1': 0.9875665568840196, 'epsilon': 3.725969635837498e-05, 'balanced_loss': True, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 10:09:13,844] Trial 209 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9392381698261655, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 75, 'number_of_hidden_layers': 0, 'dropout_rate': 0.542455290075421, 'global_pooling': 'max', 'learning_rate': 0.005945793964813044, 'weight_decay': 1.2282220099873916e-05, 'beta_0': 0.8483211713171831, 'beta_1': 0.9880552093847843, 'epsilon': 2.3088330664434195e-05, 'balanced_loss': True, 'epochs': 157, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 3.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.93 GiB is free. Including non-PyTorch memory, this process has 41.62 GiB memory in use. Of the allocated memory 27.00 GiB is allocated by PyTorch, and 13.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 10:18:24,433] Trial 210 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.940556918619143, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.534951370269158, 'global_pooling': 'max', 'learning_rate': 0.008094258411187367, 'weight_decay': 2.754190605154925e-05, 'beta_0': 0.8461980447272646, 'beta_1': 0.982178788702393, 'epsilon': 1.9052110366178568e-05, 'balanced_loss': True, 'epochs': 105, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 4.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.56 GiB is free. Including non-PyTorch memory, this process has 40.99 GiB memory in use. Of the allocated memory 31.64 GiB is allocated by PyTorch, and 8.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 10:27:29,748] Trial 211 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9431066047940425, 'batch_size': 54, 'attention_heads': 13, 'hidden_dimension': 127, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5507728783116173, 'global_pooling': 'max', 'learning_rate': 0.00538159649702696, 'weight_decay': 2.3472321973338678e-05, 'beta_0': 0.8510191561184325, 'beta_1': 0.9873714877906773, 'epsilon': 7.447957900421482e-05, 'balanced_loss': True, 'epochs': 146, 'early_stopping_patience': 12, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 10:37:30,596] Trial 212 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9483300573651471, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5602547236193662, 'global_pooling': 'max', 'learning_rate': 0.0073088648993847125, 'weight_decay': 0.0007110593221315834, 'beta_0': 0.8448545474886192, 'beta_1': 0.9896909815372882, 'epsilon': 6.488133727850069e-07, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 10:47:50,259] Trial 213 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9465167069870468, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5134653190773256, 'global_pooling': 'max', 'learning_rate': 0.010174531653130413, 'weight_decay': 2.0871618574697314e-05, 'beta_0': 0.8535953509503834, 'beta_1': 0.9891175601112455, 'epsilon': 1.47146195669874e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 10:57:51,114] Trial 214 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9416970518707023, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5475475409684853, 'global_pooling': 'max', 'learning_rate': 0.010772633519771521, 'weight_decay': 1.717839213984327e-05, 'beta_0': 0.8494139515120636, 'beta_1': 0.9884676403650278, 'epsilon': 1.2796478152161725e-05, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.92 GiB is free. Including non-PyTorch memory, this process has 40.63 GiB memory in use. Of the allocated memory 27.61 GiB is allocated by PyTorch, and 11.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 11:06:06,135] Trial 215 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.946181158866067, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5548096374943703, 'global_pooling': 'max', 'learning_rate': 0.0047195610861335, 'weight_decay': 1.4559575956819658e-05, 'beta_0': 0.852330576693843, 'beta_1': 0.9892421474789301, 'epsilon': 1.66145133030779e-08, 'balanced_loss': True, 'epochs': 158, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 11:15:39,843] Trial 216 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9734522053379714, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5424229571149621, 'global_pooling': 'max', 'learning_rate': 0.013835130819390027, 'weight_decay': 2.1958389552671024e-05, 'beta_0': 0.8541075022607385, 'beta_1': 0.9877394927731411, 'epsilon': 6.339168121776123e-06, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.00 GiB is free. Including non-PyTorch memory, this process has 39.55 GiB memory in use. Of the allocated memory 27.91 GiB is allocated by PyTorch, and 10.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 11:23:55,835] Trial 217 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9439938314912815, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5171792319065298, 'global_pooling': 'max', 'learning_rate': 0.008784444348299142, 'weight_decay': 0.0009938500885293548, 'beta_0': 0.8476515939812116, 'beta_1': 0.9902441592063496, 'epsilon': 1.3756102747680194e-06, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 11:33:44,173] Trial 218 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9298251790548084, 'batch_size': 42, 'attention_heads': 7, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5124070894145709, 'global_pooling': 'max', 'learning_rate': 0.0063537848994822095, 'weight_decay': 2.9922679994968105e-05, 'beta_0': 0.8509223601680158, 'beta_1': 0.9891540446797508, 'epsilon': 1.4228155844223953e-08, 'balanced_loss': True, 'epochs': 175, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 11:43:34,934] Trial 219 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9495499078518833, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5638771428643323, 'global_pooling': 'max', 'learning_rate': 0.00746480013977023, 'weight_decay': 1.5602325063697525e-05, 'beta_0': 0.8528408461869232, 'beta_1': 0.9896767451177133, 'epsilon': 2.9010731039485384e-05, 'balanced_loss': True, 'epochs': 77, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 11:56:49,760] Trial 220 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9468291409237388, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5504328803746347, 'global_pooling': 'max', 'learning_rate': 0.00022773738381659208, 'weight_decay': 2.4180867430209444e-05, 'beta_0': 0.8560939877466099, 'beta_1': 0.9885352644469065, 'epsilon': 1.232409882972455e-08, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 12:07:52,180] Trial 221 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9411890704575775, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4391265811554568, 'global_pooling': 'sum', 'learning_rate': 0.003696300545785445, 'weight_decay': 0.00048279017154333003, 'beta_0': 0.8495869340577936, 'beta_1': 0.9880364660305161, 'epsilon': 1.805262855226342e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 12:18:07,321] Trial 222 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9412585890279933, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43940633942035395, 'global_pooling': 'max', 'learning_rate': 0.003808304446424079, 'weight_decay': 0.0005759874509684205, 'beta_0': 0.849138566192933, 'beta_1': 0.9878688946665126, 'epsilon': 1.733144192880732e-08, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 12:27:57,981] Trial 223 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9816411541482487, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5274740697359461, 'global_pooling': 'sum', 'learning_rate': 0.00500350943514814, 'weight_decay': 0.0004686320631153968, 'beta_0': 0.8469333336253996, 'beta_1': 0.9882592980339975, 'epsilon': 2.5342565043947587e-08, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 12:38:06,370] Trial 224 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9433933862939397, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5265577237539956, 'global_pooling': 'sum', 'learning_rate': 0.009372690863740804, 'weight_decay': 0.0004238124257095789, 'beta_0': 0.8468223173253321, 'beta_1': 0.9886203405450915, 'epsilon': 2.3169094454992685e-08, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.54 GiB is free. Including non-PyTorch memory, this process has 40.01 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 16.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 12:46:25,194] Trial 225 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9826098008940128, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.541078966191967, 'global_pooling': 'sum', 'learning_rate': 0.003510773663135957, 'weight_decay': 0.0005474738022775619, 'beta_0': 0.8444863254108489, 'beta_1': 0.9879947648550456, 'epsilon': 1.4860805849558094e-08, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.15 GiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 21.87 GiB is allocated by PyTorch, and 16.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 12:54:41,756] Trial 226 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.988978023122706, 'batch_size': 41, 'attention_heads': 12, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5289929544220754, 'global_pooling': 'sum', 'learning_rate': 0.0056867217473631085, 'weight_decay': 0.0005212028975418618, 'beta_0': 0.8504334014584982, 'beta_1': 0.9881138482320361, 'epsilon': 1.8436459221572554e-08, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 13:04:24,820] Trial 227 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.985470210535003, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5346796682374568, 'global_pooling': 'sum', 'learning_rate': 0.006873382809203925, 'weight_decay': 0.0006550477776466978, 'beta_0': 0.848047527116708, 'beta_1': 0.988938637856894, 'epsilon': 2.5739613785709896e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.32 GiB is free. Including non-PyTorch memory, this process has 39.24 GiB memory in use. Of the allocated memory 22.40 GiB is allocated by PyTorch, and 15.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 13:12:42,447] Trial 228 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9404041673633586, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 164, 'number_of_hidden_layers': 0, 'dropout_rate': 0.45656695248454565, 'global_pooling': 'max', 'learning_rate': 0.01069037944480837, 'weight_decay': 0.00035132024424180665, 'beta_0': 0.8546974439230165, 'beta_1': 0.9872635259775925, 'epsilon': 3.946295397309063e-08, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 13:23:03,003] Trial 229 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9441877089053301, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5573816660579823, 'global_pooling': 'max', 'learning_rate': 0.004418426878022406, 'weight_decay': 0.00045707606758046596, 'beta_0': 0.8520110232579239, 'beta_1': 0.9883769338033123, 'epsilon': 2.5810039852445915e-08, 'balanced_loss': False, 'epochs': 94, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.99 GiB is free. Including non-PyTorch memory, this process has 38.56 GiB memory in use. Of the allocated memory 27.80 GiB is allocated by PyTorch, and 9.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 13:31:22,659] Trial 230 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.944747848627676, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5582723516044603, 'global_pooling': 'max', 'learning_rate': 0.004412829689129361, 'weight_decay': 0.00039001533433191313, 'beta_0': 0.852892446190781, 'beta_1': 0.9882942138025601, 'epsilon': 3.0392140677017106e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 13:41:43,843] Trial 231 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9473617336878646, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5482350821622802, 'global_pooling': 'max', 'learning_rate': 0.0050576772156672575, 'weight_decay': 0.00045189579909314973, 'beta_0': 0.8500528045695178, 'beta_1': 0.9875914536582838, 'epsilon': 1.9616574550031895e-08, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.99 GiB is free. Including non-PyTorch memory, this process has 38.57 GiB memory in use. Of the allocated memory 28.04 GiB is allocated by PyTorch, and 9.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 13:50:02,986] Trial 232 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9429916946736593, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5661414835067512, 'global_pooling': 'max', 'learning_rate': 0.003600806541199958, 'weight_decay': 0.0006237557664737084, 'beta_0': 0.8455801103825462, 'beta_1': 0.9870698965976027, 'epsilon': 4.362980474789814e-08, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 13:59:33,430] Trial 233 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9820394363950001, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5545628092159373, 'global_pooling': 'max', 'learning_rate': 0.008043808967672208, 'weight_decay': 2.0213727841727353e-05, 'beta_0': 0.8518242089078182, 'beta_1': 0.988588212033773, 'epsilon': 2.4080788585914806e-08, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 14:09:47,269] Trial 234 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9411899421238278, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5444891842643599, 'global_pooling': 'max', 'learning_rate': 0.006368114866405061, 'weight_decay': 0.0004500137678554815, 'beta_0': 0.8476097796378099, 'beta_1': 0.9953557777504706, 'epsilon': 2.9762389961679095e-08, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 14:20:03,199] Trial 235 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9460030560245105, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5538489428897508, 'global_pooling': 'max', 'learning_rate': 0.008484524654729405, 'weight_decay': 0.0005122167099946288, 'beta_0': 0.854596243986406, 'beta_1': 0.9891625923629431, 'epsilon': 1.413368992559561e-08, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.75 GiB is free. Including non-PyTorch memory, this process has 38.80 GiB memory in use. Of the allocated memory 23.26 GiB is allocated by PyTorch, and 14.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 14:28:22,582] Trial 236 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9790537656275152, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5611820695169191, 'global_pooling': 'max', 'learning_rate': 0.005680430699347444, 'weight_decay': 0.0007803405886760423, 'beta_0': 0.8799099758302195, 'beta_1': 0.988229067617968, 'epsilon': 2.571812405906863e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 14:38:31,776] Trial 237 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9433115567751396, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5372726519627452, 'global_pooling': 'max', 'learning_rate': 0.004137161501572107, 'weight_decay': 1.753364419244461e-05, 'beta_0': 0.8495911740068144, 'beta_1': 0.9889240310390766, 'epsilon': 1.9550188793609097e-08, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.21 GiB is free. Including non-PyTorch memory, this process has 40.34 GiB memory in use. Of the allocated memory 28.58 GiB is allocated by PyTorch, and 10.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 14:46:51,937] Trial 238 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9388877526431618, 'batch_size': 36, 'attention_heads': 14, 'hidden_dimension': 149, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5696451679081875, 'global_pooling': 'max', 'learning_rate': 0.012973579977105701, 'weight_decay': 1.2106231556733685e-05, 'beta_0': 0.8519772907096907, 'beta_1': 0.9895589142844861, 'epsilon': 3.5382657601732814e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 14:57:32,778] Trial 239 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9446918218634042, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5462504132931844, 'global_pooling': 'sum', 'learning_rate': 0.018688429744395427, 'weight_decay': 2.1233335724816892e-05, 'beta_0': 0.8573011472350925, 'beta_1': 0.9885841170997925, 'epsilon': 1.652941082068441e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 15:07:16,958] Trial 240 finished with value: 0.8727272727272727 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9424764741121014, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.48172132535181095, 'global_pooling': 'max', 'learning_rate': 0.007214151340406693, 'weight_decay': 2.9831465469333578e-05, 'beta_0': 0.8419369242751427, 'beta_1': 0.9879505777707378, 'epsilon': 2.1782169803173072e-08, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 15:17:08,404] Trial 241 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9479030967864174, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5597873585130834, 'global_pooling': 'max', 'learning_rate': 0.009888912442640306, 'weight_decay': 1.3660340786720115e-05, 'beta_0': 0.853931851016896, 'beta_1': 0.9958955101027821, 'epsilon': 1.1779013711943552e-08, 'balanced_loss': True, 'epochs': 113, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.16 GiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 21.91 GiB is allocated by PyTorch, and 16.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 15:25:27,154] Trial 242 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9886655042192891, 'batch_size': 39, 'attention_heads': 12, 'hidden_dimension': 76, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5515521121058834, 'global_pooling': 'max', 'learning_rate': 0.004803180852927626, 'weight_decay': 1.7411709846825105e-05, 'beta_0': 0.846674656856016, 'beta_1': 0.9891689800248872, 'epsilon': 2.5912954741663932e-08, 'balanced_loss': True, 'epochs': 125, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.05 GiB is free. Including non-PyTorch memory, this process has 41.50 GiB memory in use. Of the allocated memory 28.93 GiB is allocated by PyTorch, and 11.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 15:33:45,840] Trial 243 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9364366768328088, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5408369726574008, 'global_pooling': 'sum', 'learning_rate': 0.004547661654207812, 'weight_decay': 2.5188446859362335e-05, 'beta_0': 0.8507360579372713, 'beta_1': 0.9900213180790196, 'epsilon': 8.983665641730324e-07, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 15:44:16,615] Trial 244 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9407296446520308, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 39, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5562300097995381, 'global_pooling': 'sum', 'learning_rate': 0.005799631336853157, 'weight_decay': 1.5384316194988592e-05, 'beta_0': 0.8784604648795707, 'beta_1': 0.9887318939710739, 'epsilon': 1.539064882113847e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.00 GiB is free. Including non-PyTorch memory, this process has 39.55 GiB memory in use. Of the allocated memory 27.72 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 15:52:36,494] Trial 245 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9453735003239472, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.41236625379520336, 'global_pooling': 'sum', 'learning_rate': 0.006860890438142845, 'weight_decay': 0.00047326197399984865, 'beta_0': 0.8762489282280708, 'beta_1': 0.9966017784746722, 'epsilon': 7.292529197027763e-07, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.98 GiB is free. Including non-PyTorch memory, this process has 38.57 GiB memory in use. Of the allocated memory 28.12 GiB is allocated by PyTorch, and 9.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 16:00:56,003] Trial 246 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9425440877393516, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5484729457731172, 'global_pooling': 'sum', 'learning_rate': 0.0030899458776782783, 'weight_decay': 1.1218878404657826e-05, 'beta_0': 0.8562909707283478, 'beta_1': 0.9877494343041443, 'epsilon': 4.305601053313785e-05, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.76 GiB is free. Including non-PyTorch memory, this process has 40.80 GiB memory in use. Of the allocated memory 28.61 GiB is allocated by PyTorch, and 11.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 16:09:15,615] Trial 247 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9387648453711425, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 35, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5701725136685741, 'global_pooling': 'sum', 'learning_rate': 0.005041153393902546, 'weight_decay': 2.2023668431484333e-05, 'beta_0': 0.848911945677377, 'beta_1': 0.9883195685648382, 'epsilon': 1.157679897872677e-06, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.00 GiB is free. Including non-PyTorch memory, this process has 39.55 GiB memory in use. Of the allocated memory 27.90 GiB is allocated by PyTorch, and 10.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 16:17:35,103] Trial 248 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9440047862138176, 'batch_size': 38, 'attention_heads': 12, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5039176339539547, 'global_pooling': 'max', 'learning_rate': 0.008519884970755159, 'weight_decay': 1.965656583627734e-05, 'beta_0': 0.8819311219022733, 'beta_1': 0.9894530102134171, 'epsilon': 1.9265620917568028e-08, 'balanced_loss': True, 'epochs': 160, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 16:28:42,067] Trial 249 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.946618817883216, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 69, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5622719783149682, 'global_pooling': 'max', 'learning_rate': 0.006068860587788655, 'weight_decay': 3.488132570326779e-05, 'beta_0': 0.8517630892624001, 'beta_1': 0.9891391853322383, 'epsilon': 3.22908823973663e-08, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 16:38:10,248] Trial 250 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9801042275923132, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.422002048516275, 'global_pooling': 'max', 'learning_rate': 0.003791463517334524, 'weight_decay': 9.618053257461231e-06, 'beta_0': 0.8538863402290654, 'beta_1': 0.9882122293395413, 'epsilon': 1.2184794003333897e-08, 'balanced_loss': True, 'epochs': 90, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 16:48:10,496] Trial 251 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9400654512957366, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5926531968098869, 'global_pooling': 'max', 'learning_rate': 0.010717209584728368, 'weight_decay': 1.8357180178483255e-05, 'beta_0': 0.8483548912361759, 'beta_1': 0.9887749379793985, 'epsilon': 5.494785445169821e-07, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 16:58:10,071] Trial 252 finished with value: 0.8909090909090909 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9417262835365963, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5531300680692053, 'global_pooling': 'max', 'learning_rate': 0.006993031154294494, 'weight_decay': 0.0005916563635894715, 'beta_0': 0.8436056180464766, 'beta_1': 0.9828332818301473, 'epsilon': 6.223793673101858e-05, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.61 GiB is free. Including non-PyTorch memory, this process has 41.95 GiB memory in use. Of the allocated memory 25.58 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 17:07:22,260] Trial 253 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9507873180904416, 'batch_size': 43, 'attention_heads': 12, 'hidden_dimension': 110, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5435914363684564, 'global_pooling': 'sum', 'learning_rate': 0.007965845075509275, 'weight_decay': 1.4374606307501528e-05, 'beta_0': 0.8456846966833399, 'beta_1': 0.9898194179619124, 'epsilon': 2.3237466989655972e-08, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.99 GiB is free. Including non-PyTorch memory, this process has 38.56 GiB memory in use. Of the allocated memory 27.83 GiB is allocated by PyTorch, and 9.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 17:15:41,529] Trial 254 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.944727177959086, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.442408259921245, 'global_pooling': 'max', 'learning_rate': 0.015523107172533263, 'weight_decay': 2.7103772725581216e-05, 'beta_0': 0.850270053875083, 'beta_1': 0.9874003895474516, 'epsilon': 4.889159341117544e-06, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 17:25:39,404] Trial 255 finished with value: 0.8848484848484849 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9840498718135451, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 184, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3932205898754848, 'global_pooling': 'max', 'learning_rate': 0.011723528299061895, 'weight_decay': 0.00037342339602966865, 'beta_0': 0.8531400397175599, 'beta_1': 0.9878734254510023, 'epsilon': 1.6915056164272995e-08, 'balanced_loss': True, 'epochs': 178, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 17:35:54,895] Trial 256 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9466365315023775, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.536475465385685, 'global_pooling': 'max', 'learning_rate': 0.004469696100401133, 'weight_decay': 1.657701971217135e-05, 'beta_0': 0.8559608111199306, 'beta_1': 0.9884756484597371, 'epsilon': 2.7538663692696167e-08, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 17:46:18,153] Trial 257 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9486456459674432, 'batch_size': 45, 'attention_heads': 14, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5581696825515771, 'global_pooling': 'max', 'learning_rate': 0.0092717089315225, 'weight_decay': 2.420799355166744e-05, 'beta_0': 0.8176644792957083, 'beta_1': 0.9930486679753051, 'epsilon': 1.4258497558214066e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 17:57:15,459] Trial 258 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9337147794758927, 'batch_size': 42, 'attention_heads': 12, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5244315710431172, 'global_pooling': 'sum', 'learning_rate': 0.005500307785616756, 'weight_decay': 1.8649688920595745e-05, 'beta_0': 0.8473831001714738, 'beta_1': 0.9889867585022164, 'epsilon': 1.997162869618476e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 12, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 18:07:10,210] Trial 259 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9379627795885109, 'batch_size': 36, 'attention_heads': 13, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5322504654310063, 'global_pooling': 'max', 'learning_rate': 0.023806288965651668, 'weight_decay': 3.113643509381596e-05, 'beta_0': 0.8514993937416041, 'beta_1': 0.9936313728531893, 'epsilon': 1.7157892084836086e-06, 'balanced_loss': True, 'epochs': 151, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 3.03 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.38 GiB is free. Including non-PyTorch memory, this process has 43.17 GiB memory in use. Of the allocated memory 27.59 GiB is allocated by PyTorch, and 14.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 18:16:20,174] Trial 260 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9163767927234191, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.549579421635041, 'global_pooling': 'max', 'learning_rate': 0.006572728964935519, 'weight_decay': 1.3085600827063662e-05, 'beta_0': 0.879720251667909, 'beta_1': 0.9944796732686729, 'epsilon': 2.4139681895987032e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 18:26:19,355] Trial 261 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9433626113635694, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.563795306302344, 'global_pooling': 'max', 'learning_rate': 0.007964345201822232, 'weight_decay': 0.0003028045848650861, 'beta_0': 0.8759782905843685, 'beta_1': 0.9816151219856641, 'epsilon': 3.494897707034661e-08, 'balanced_loss': True, 'epochs': 108, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.77 GiB is free. Including non-PyTorch memory, this process has 39.79 GiB memory in use. Of the allocated memory 27.93 GiB is allocated by PyTorch, and 10.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 18:34:38,699] Trial 262 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9438186779467186, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.570217071423386, 'global_pooling': 'max', 'learning_rate': 0.008324783602887006, 'weight_decay': 0.0002681123108558415, 'beta_0': 0.8756481728411216, 'beta_1': 0.9894667677543406, 'epsilon': 3.4850634360277306e-08, 'balanced_loss': True, 'epochs': 107, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 18:44:40,573] Trial 263 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9415221836759436, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 35, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5641875797651242, 'global_pooling': 'max', 'learning_rate': 0.011784649901370715, 'weight_decay': 0.0004634702738681943, 'beta_0': 0.8746422642853656, 'beta_1': 0.990381920531519, 'epsilon': 4.7431113183824377e-08, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 18:54:38,547] Trial 264 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9410405045415581, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 34, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5629567681426123, 'global_pooling': 'max', 'learning_rate': 0.012539992598598611, 'weight_decay': 0.0005003717172234823, 'beta_0': 0.87738473227012, 'beta_1': 0.9901381596728847, 'epsilon': 4.106256103606454e-08, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 19:04:31,234] Trial 265 finished with value: 0.8848484848484849 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.942422326975014, 'batch_size': 43, 'attention_heads': 14, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5734663898847757, 'global_pooling': 'max', 'learning_rate': 0.009715973384710185, 'weight_decay': 0.00043505502347089564, 'beta_0': 0.8757985979378423, 'beta_1': 0.9899464638458204, 'epsilon': 3.0547113676309084e-08, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.99 GiB is free. Including non-PyTorch memory, this process has 39.57 GiB memory in use. Of the allocated memory 28.52 GiB is allocated by PyTorch, and 9.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 19:12:50,254] Trial 266 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9395303870883501, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5648346703348663, 'global_pooling': 'max', 'learning_rate': 0.007817894668895776, 'weight_decay': 0.0003277952606710455, 'beta_0': 0.8734525391264213, 'beta_1': 0.9903994912013789, 'epsilon': 3.4790001870244654e-08, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 19:22:55,936] Trial 267 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9449530696574827, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 44, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5561089609243697, 'global_pooling': 'max', 'learning_rate': 0.01579322375600797, 'weight_decay': 0.0003935214167039752, 'beta_0': 0.8491719355058948, 'beta_1': 0.9907586921875857, 'epsilon': 4.517776342671991e-08, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 19:33:22,609] Trial 268 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9420955957216618, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5785925234852901, 'global_pooling': 'max', 'learning_rate': 0.0062027251122472316, 'weight_decay': 0.00031684074453123157, 'beta_0': 0.8544680678175295, 'beta_1': 0.9988561726166049, 'epsilon': 5.721708873764453e-08, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 19:43:02,644] Trial 269 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9466756373983751, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.565071377543154, 'global_pooling': 'max', 'learning_rate': 0.009618490011344497, 'weight_decay': 0.0006666403943819851, 'beta_0': 0.8778511046338592, 'beta_1': 0.9908973815213425, 'epsilon': 2.490961953501292e-08, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 19:52:58,098] Trial 270 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9442657525777829, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5530875921260642, 'global_pooling': 'max', 'learning_rate': 0.011966966448390369, 'weight_decay': 0.0005213830886406609, 'beta_0': 0.8575381942117967, 'beta_1': 0.989535764784677, 'epsilon': 2.7465576591248757e-08, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 20:03:11,279] Trial 271 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9491011049214579, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5450447181815395, 'global_pooling': 'max', 'learning_rate': 0.007564081082185754, 'weight_decay': 0.0005620018768751807, 'beta_0': 0.8615011629937891, 'beta_1': 0.9881796111555473, 'epsilon': 1.8490569370518448e-08, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1020.69 MiB is free. Including non-PyTorch memory, this process has 43.56 GiB memory in use. Of the allocated memory 28.36 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-04 20:11:29,375] Trial 272 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9405983357417651, 'batch_size': 43, 'attention_heads': 11, 'hidden_dimension': 38, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5585791095692505, 'global_pooling': 'max', 'learning_rate': 0.005593957537940793, 'weight_decay': 1.61519058318149e-05, 'beta_0': 0.8393614478851716, 'beta_1': 0.9888344719055293, 'epsilon': 5.089018658598901e-08, 'balanced_loss': True, 'epochs': 104, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 20:21:36,099] Trial 273 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9430936962222195, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5179023203312275, 'global_pooling': 'max', 'learning_rate': 0.008695807302892681, 'weight_decay': 2.096126084355543e-05, 'beta_0': 0.8527076455054134, 'beta_1': 0.991532917284213, 'epsilon': 3.83414594293283e-06, 'balanced_loss': True, 'epochs': 54, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 20:31:54,801] Trial 274 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.938476969088759, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5741979090900291, 'global_pooling': 'max', 'learning_rate': 0.01859105264717366, 'weight_decay': 7.75632136064778e-06, 'beta_0': 0.8748575997597301, 'beta_1': 0.9904311282332039, 'epsilon': 2.151492979700073e-08, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-04 20:41:47,127] Trial 275 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9458588385438507, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 46, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5508229854295192, 'global_pooling': 'max', 'learning_rate': 0.0067691088493582385, 'weight_decay': 2.1209614765920003e-06, 'beta_0': 0.8448554590038501, 'beta_1': 0.9979152843085327, 'epsilon': 2.98159671064877e-08, 'balanced_loss': True, 'epochs': 113, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
slurmstepd: error: *** JOB 14057319 ON gpu045 CANCELLED AT 2024-12-04T20:46:00 DUE TO TIME LIMIT ***
