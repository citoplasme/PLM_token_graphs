[I 2024-12-25 04:05:15,122] Using an existing study with name 'IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
[I 2024-12-25 04:35:38,607] Trial 314 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9821053580137404, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3584851035938134, 'global_pooling': 'mean', 'learning_rate': 7.088995070809059e-05, 'weight_decay': 0.00048193615647789567, 'beta_0': 0.8539114282953632, 'beta_1': 0.9939835204485898, 'epsilon': 1.4759722003775365e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 05:07:33,625] Trial 315 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9890155307833897, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4143209764164224, 'global_pooling': 'mean', 'learning_rate': 2.966664494843379e-05, 'weight_decay': 0.0002951960297727206, 'beta_0': 0.8510960999933209, 'beta_1': 0.99313768537269, 'epsilon': 1.2165675294122202e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 21, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 05:38:14,809] Trial 316 finished with value: 0.9575757575757575 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9762918328147397, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4392499488134854, 'global_pooling': 'mean', 'learning_rate': 4.7386488604788604e-05, 'weight_decay': 0.0003749652462065677, 'beta_0': 0.8562725299445365, 'beta_1': 0.9914191908121628, 'epsilon': 1.9944946095879428e-08, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 06:09:40,259] Trial 317 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9803573742640972, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3730592095014941, 'global_pooling': 'mean', 'learning_rate': 5.9911246602711636e-05, 'weight_decay': 0.000751550951614101, 'beta_0': 0.849300587122629, 'beta_1': 0.9947156461729073, 'epsilon': 1.4115564016977992e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 06:40:35,679] Trial 318 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9850828957893867, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3882179972402695, 'global_pooling': 'mean', 'learning_rate': 7.971256985075913e-05, 'weight_decay': 0.0004300036289681605, 'beta_0': 0.851933069462864, 'beta_1': 0.9922635847679478, 'epsilon': 3.3584310308021153e-07, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 07:14:26,965] Trial 319 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.982718814427003, 'batch_size': 28, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4282452402593278, 'global_pooling': 'mean', 'learning_rate': 3.5791646543540545e-05, 'weight_decay': 0.0002579072955401955, 'beta_0': 0.8543193499166761, 'beta_1': 0.993691780152599, 'epsilon': 1.1291686888699461e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.81 GiB is free. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 34.88 GiB is allocated by PyTorch, and 6.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 07:39:37,004] Trial 320 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9787309420643746, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40456733551232, 'global_pooling': 'mean', 'learning_rate': 9.944664445348936e-05, 'weight_decay': 0.0005533279529091011, 'beta_0': 0.8462044498812535, 'beta_1': 0.9961810217338385, 'epsilon': 1.7416799769531883e-08, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 6}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 08:10:19,672] Trial 321 finished with value: 0.9696969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9815423523006618, 'batch_size': 42, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3792019660528421, 'global_pooling': 'mean', 'learning_rate': 6.627666962562975e-05, 'weight_decay': 0.00031914208448403664, 'beta_0': 0.8593233148743781, 'beta_1': 0.994170462635712, 'epsilon': 1.3768797517039504e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 08:40:20,552] Trial 322 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9862571222541322, 'batch_size': 34, 'attention_heads': 5, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36498722098454545, 'global_pooling': 'mean', 'learning_rate': 0.00013605961880792822, 'weight_decay': 0.00040007080496200665, 'beta_0': 0.8503441230470644, 'beta_1': 0.992890185159878, 'epsilon': 1.1623547835702758e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 09:12:14,558] Trial 323 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.983699574151119, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3939989979301161, 'global_pooling': 'mean', 'learning_rate': 4.9111623990591366e-05, 'weight_decay': 0.00048824217085536144, 'beta_0': 0.8568730938409171, 'beta_1': 0.9817872127587747, 'epsilon': 1.0066088644858318e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 690.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 29.50 GiB is allocated by PyTorch, and 13.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 09:39:27,550] Trial 324 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9802029820430561, 'batch_size': 27, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37286566787800923, 'global_pooling': 'mean', 'learning_rate': 8.33738239270253e-05, 'weight_decay': 0.0006437246406547471, 'beta_0': 0.8527760626428132, 'beta_1': 0.9932921582804961, 'epsilon': 1.6084148675734782e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 42.78 GiB memory in use. Of the allocated memory 35.76 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 10:04:39,387] Trial 325 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9776171175650539, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3531164812684018, 'global_pooling': 'max', 'learning_rate': 0.00010393400914086273, 'weight_decay': 0.0004363320483003847, 'beta_0': 0.8481143812701334, 'beta_1': 0.9925046421491832, 'epsilon': 2.256037692812252e-08, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 104.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 37.94 GiB is allocated by PyTorch, and 5.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 10:29:50,903] Trial 326 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9747979321428966, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 53, 'number_of_hidden_layers': 2, 'dropout_rate': 0.384371967830525, 'global_pooling': 'mean', 'learning_rate': 4.2249860148836795e-05, 'weight_decay': 0.0003633510809381208, 'beta_0': 0.8407253956188083, 'beta_1': 0.992012586482278, 'epsilon': 1.3458516438250431e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 41.77 GiB memory in use. Of the allocated memory 31.34 GiB is allocated by PyTorch, and 9.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 10:50:22,054] Trial 327 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9675070170352587, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.399277591555026, 'global_pooling': 'mean', 'learning_rate': 5.898713903016731e-05, 'weight_decay': 0.00030921397925962296, 'beta_0': 0.8551013458018368, 'beta_1': 0.993757837923687, 'epsilon': 1.808599895247588e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 11:20:30,817] Trial 328 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9828721258638831, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3612827916929014, 'global_pooling': 'mean', 'learning_rate': 7.347934196292932e-05, 'weight_decay': 0.0005371803437301516, 'beta_0': 0.8523784172305987, 'beta_1': 0.9943510690868217, 'epsilon': 1.2289978043209193e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 11:51:39,543] Trial 329 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9875046207322896, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37685561882363017, 'global_pooling': 'mean', 'learning_rate': 5.359720530995243e-05, 'weight_decay': 0.00046033185948127073, 'beta_0': 0.8501936614133975, 'beta_1': 0.9948816052023023, 'epsilon': 1.5111596918386123e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 12:21:21,297] Trial 330 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9809012783514133, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3695902303208697, 'global_pooling': 'mean', 'learning_rate': 0.00012253809696856704, 'weight_decay': 0.00023871685984515827, 'beta_0': 0.8443627805979383, 'beta_1': 0.9934229334451784, 'epsilon': 1.0052548111487102e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 824.69 MiB is free. Including non-PyTorch memory, this process has 43.75 GiB memory in use. Of the allocated memory 36.13 GiB is allocated by PyTorch, and 6.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 12:40:33,966] Trial 331 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9633010694749309, 'batch_size': 30, 'attention_heads': 5, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38974168414359184, 'global_pooling': 'mean', 'learning_rate': 6.607727878760219e-05, 'weight_decay': 0.00039027841065566294, 'beta_0': 0.8479315427202929, 'beta_1': 0.9916946893516642, 'epsilon': 1.2110172641531552e-08, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 13:11:58,934] Trial 332 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9855695901524065, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3811029907330293, 'global_pooling': 'mean', 'learning_rate': 3.849793638174707e-05, 'weight_decay': 0.00033409618536938475, 'beta_0': 0.8574235004364464, 'beta_1': 0.9929623508093319, 'epsilon': 1.5690965786767357e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 13:44:19,037] Trial 333 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9840762509103805, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3683142937910028, 'global_pooling': 'mean', 'learning_rate': 2.6169360230812732e-05, 'weight_decay': 0.0005848639627200379, 'beta_0': 0.853914282975144, 'beta_1': 0.9940378187122965, 'epsilon': 1.8225180539349686e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 14:13:46,537] Trial 334 finished with value: 0.9575757575757575 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9787837825766338, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37664601016085447, 'global_pooling': 'mean', 'learning_rate': 9.184231135895055e-05, 'weight_decay': 0.00046452786301649204, 'beta_0': 0.8501161614776297, 'beta_1': 0.9934440606451043, 'epsilon': 1.3493216253479816e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 20, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 14:43:39,452] Trial 335 finished with value: 0.9515151515151515 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9819841326239879, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3878054169648094, 'global_pooling': 'mean', 'learning_rate': 4.627651746534709e-05, 'weight_decay': 0.00028036154610182557, 'beta_0': 0.8466402684602486, 'beta_1': 0.9926023919616567, 'epsilon': 7.86031101449287e-06, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 15:17:30,464] Trial 336 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9843939712719445, 'batch_size': 38, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35643490003854916, 'global_pooling': 'mean', 'learning_rate': 3.2059390162936795e-05, 'weight_decay': 0.000399978481163559, 'beta_0': 0.8556235084237066, 'beta_1': 0.9945972350402955, 'epsilon': 1.1633384754162937e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 690.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 29.49 GiB is allocated by PyTorch, and 13.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 15:44:41,498] Trial 337 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9802560620883618, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 198, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4095880444110234, 'global_pooling': 'mean', 'learning_rate': 7.720049285517236e-05, 'weight_decay': 0.0005095272831588994, 'beta_0': 0.852206221121201, 'beta_1': 0.9937382581076607, 'epsilon': 2.097512729973151e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 16:13:50,693] Trial 338 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9888742361913863, 'batch_size': 28, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36312502389287604, 'global_pooling': 'mean', 'learning_rate': 0.00015869155611578897, 'weight_decay': 0.0003439382933467549, 'beta_0': 0.8608672837082817, 'beta_1': 0.9910994440439111, 'epsilon': 1.5509206207479628e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 16:46:04,340] Trial 339 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.982340386969769, 'batch_size': 52, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39698328701262053, 'global_pooling': 'mean', 'learning_rate': 6.177658211121123e-05, 'weight_decay': 2.7599256736628646e-06, 'beta_0': 0.8428275437565961, 'beta_1': 0.9931741234756852, 'epsilon': 1.151874453102767e-08, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.77 GiB is free. Including non-PyTorch memory, this process has 42.78 GiB memory in use. Of the allocated memory 35.93 GiB is allocated by PyTorch, and 5.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 17:11:28,738] Trial 340 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9773605024554997, 'batch_size': 31, 'attention_heads': 5, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38320856801884884, 'global_pooling': 'mean', 'learning_rate': 0.00011428705054760149, 'weight_decay': 0.00042996324455760805, 'beta_0': 0.849033718340829, 'beta_1': 0.9922276562642219, 'epsilon': 1.377648811007419e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 542.69 MiB is free. Including non-PyTorch memory, this process has 44.02 GiB memory in use. Of the allocated memory 33.58 GiB is allocated by PyTorch, and 9.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 17:39:23,039] Trial 341 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9858092742930826, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 166, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37290720796290744, 'global_pooling': 'mean', 'learning_rate': 9.31024404176374e-05, 'weight_decay': 0.0006565486292905475, 'beta_0': 0.8581353069562659, 'beta_1': 0.9951148059659126, 'epsilon': 1.7541230908929293e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 18:11:17,721] Trial 342 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9833655938594367, 'batch_size': 27, 'attention_heads': 4, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4020974388502613, 'global_pooling': 'mean', 'learning_rate': 5.5643824730238945e-05, 'weight_decay': 0.00038104567908895336, 'beta_0': 0.8532121397770676, 'beta_1': 0.9941366073770274, 'epsilon': 1.0037898401591791e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 18:42:20,105] Trial 343 finished with value: 0.9636363636363636 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9796438545360533, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3932261611387244, 'global_pooling': 'mean', 'learning_rate': 4.405932746760385e-05, 'weight_decay': 0.00027650463032605324, 'beta_0': 0.8519095385210507, 'beta_1': 0.9928986603361593, 'epsilon': 1.3622128654875295e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 19:12:34,681] Trial 344 finished with value: 0.9393939393939394 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9818901587934101, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36656406980161377, 'global_pooling': 'max', 'learning_rate': 6.930284589108109e-05, 'weight_decay': 0.0004880420124999566, 'beta_0': 0.8466712906832199, 'beta_1': 0.9934754075531567, 'epsilon': 2.2978400168131703e-08, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 19:46:10,862] Trial 345 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9870264529533399, 'batch_size': 31, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37790481177142843, 'global_pooling': 'mean', 'learning_rate': 3.587461756075853e-05, 'weight_decay': 0.00032830734801728487, 'beta_0': 0.8559508400640525, 'beta_1': 0.9944364654940313, 'epsilon': 1.0037369473354252e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 20:17:30,765] Trial 346 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9847516655069742, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35027472582476943, 'global_pooling': 'mean', 'learning_rate': 8.2173701882772e-05, 'weight_decay': 0.00041746543049091435, 'beta_0': 0.8506692661306732, 'beta_1': 0.9955316620063247, 'epsilon': 1.5604034810571163e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 20:50:32,982] Trial 347 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9807501354593815, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3841314318293737, 'global_pooling': 'mean', 'learning_rate': 5.3929164295955236e-05, 'weight_decay': 0.0005702934112845676, 'beta_0': 0.8544834044786719, 'beta_1': 0.9925668571697402, 'epsilon': 1.1855428530594703e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 21:19:33,621] Trial 348 finished with value: 0.9575757575757575 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9784057691827495, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39002835185494583, 'global_pooling': 'mean', 'learning_rate': 0.0001060242164337156, 'weight_decay': 0.0007893284971042259, 'beta_0': 0.848305189332289, 'beta_1': 0.993814653349803, 'epsilon': 1.9048837632371e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 176.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 37.27 GiB is allocated by PyTorch, and 5.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 21:45:00,504] Trial 349 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9756489579317092, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3714844747659549, 'global_pooling': 'mean', 'learning_rate': 6.681676926107475e-05, 'weight_decay': 4.768969728318131e-06, 'beta_0': 0.8447917633554667, 'beta_1': 0.9916616380111918, 'epsilon': 1.3617895917024722e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 22:17:57,243] Trial 350 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9842785909866031, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36052760709156867, 'global_pooling': 'mean', 'learning_rate': 4.747281799948032e-05, 'weight_decay': 0.000360042504572425, 'beta_0': 0.8510658993299891, 'beta_1': 0.9801763627706476, 'epsilon': 1.5828557162031695e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 41.84 GiB memory in use. Of the allocated memory 33.44 GiB is allocated by PyTorch, and 7.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 22:44:53,556] Trial 351 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9823761051351005, 'batch_size': 57, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4644095270889577, 'global_pooling': 'mean', 'learning_rate': 8.085672357327261e-05, 'weight_decay': 0.00045217546998103545, 'beta_0': 0.8536495680269749, 'beta_1': 0.9931031357340194, 'epsilon': 1.2067585731595753e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 23:15:44,288] Trial 352 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9801872439493109, 'batch_size': 30, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4477160565783752, 'global_pooling': 'mean', 'learning_rate': 0.00013188904879762146, 'weight_decay': 0.0002969336389275548, 'beta_0': 0.8489142957493767, 'beta_1': 0.9921147208031665, 'epsilon': 1.0004719152689063e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-25 23:46:16,493] Trial 353 finished with value: 0.9030303030303031 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9869768888893378, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38075489656581485, 'global_pooling': 'mean', 'learning_rate': 2.0168371982903485e-05, 'weight_decay': 0.00021414165899253363, 'beta_0': 0.8584089475194597, 'beta_1': 0.9940381082213888, 'epsilon': 1.4059575418171453e-08, 'balanced_loss': False, 'epochs': 84, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.25 GiB is free. Including non-PyTorch memory, this process has 43.30 GiB memory in use. Of the allocated memory 34.88 GiB is allocated by PyTorch, and 7.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-25 23:56:20,860] Trial 354 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9262275737698105, 'batch_size': 28, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36781806887366125, 'global_pooling': 'mean', 'learning_rate': 3.983128681687445e-05, 'weight_decay': 0.0005156416370205668, 'beta_0': 0.8555075939222209, 'beta_1': 0.9934530946268267, 'epsilon': 1.692452599448732e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 5}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 00:27:17,664] Trial 355 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9833584566629301, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37452275944822455, 'global_pooling': 'mean', 'learning_rate': 9.906556658719718e-05, 'weight_decay': 0.00039432751559845416, 'beta_0': 0.8520036492458177, 'beta_1': 0.9927601943908374, 'epsilon': 6.223986976313601e-07, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 820.69 MiB is free. Including non-PyTorch memory, this process has 43.75 GiB memory in use. Of the allocated memory 35.57 GiB is allocated by PyTorch, and 7.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 00:52:44,949] Trial 356 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9778705088160913, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39609484028304687, 'global_pooling': 'mean', 'learning_rate': 5.920529524712094e-05, 'weight_decay': 0.0006950231139323323, 'beta_0': 0.8460283678748818, 'beta_1': 0.9937834058772285, 'epsilon': 1.2155433040997059e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 158.69 MiB is free. Including non-PyTorch memory, this process has 44.40 GiB memory in use. Of the allocated memory 37.35 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 01:18:10,259] Trial 357 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9723345126573324, 'batch_size': 31, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38695243976635507, 'global_pooling': 'mean', 'learning_rate': 5.247410865676578e-05, 'weight_decay': 0.0005756643087119795, 'beta_0': 0.8494148933434155, 'beta_1': 0.994491960781745, 'epsilon': 2.033875493760024e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 01:48:59,384] Trial 358 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9816701597174984, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40573149072281117, 'global_pooling': 'mean', 'learning_rate': 8.744038179288292e-05, 'weight_decay': 0.0002561745768290393, 'beta_0': 0.8535291677785468, 'beta_1': 0.9949350129426261, 'epsilon': 1.1794226543602949e-08, 'balanced_loss': True, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 02:19:48,503] Trial 359 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9854711902361535, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3575431090524165, 'global_pooling': 'mean', 'learning_rate': 7.184588487008933e-05, 'weight_decay': 0.0003395823696797523, 'beta_0': 0.8569865575115073, 'beta_1': 0.9932357645427441, 'epsilon': 4.5571518518581273e-07, 'balanced_loss': False, 'epochs': 198, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 02:52:05,511] Trial 360 finished with value: 0.9696969696969697 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9887318367907099, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 50, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3793980556751552, 'global_pooling': 'mean', 'learning_rate': 3.301230914786017e-05, 'weight_decay': 0.0004483022627975304, 'beta_0': 0.8473935138039044, 'beta_1': 0.992603818307427, 'epsilon': 1.5476425875075686e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 03:21:22,646] Trial 361 finished with value: 0.9515151515151515 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9801859227933393, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3650837255871168, 'global_pooling': 'mean', 'learning_rate': 4.2043800986159795e-05, 'weight_decay': 0.00038021738284235804, 'beta_0': 0.8509853014870575, 'beta_1': 0.9930318848756571, 'epsilon': 1.3774170602372763e-07, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 14, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 03:52:06,361] Trial 362 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9833316025952703, 'batch_size': 27, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3900521324999321, 'global_pooling': 'mean', 'learning_rate': 6.377587598178137e-05, 'weight_decay': 0.0005032341007390568, 'beta_0': 0.8550623672765557, 'beta_1': 0.9942254350301634, 'epsilon': 1.0047447860331518e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 04:24:30,829] Trial 363 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9814537744185095, 'batch_size': 39, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37042496739741765, 'global_pooling': 'mean', 'learning_rate': 4.949870346526198e-05, 'weight_decay': 0.0002919461399088505, 'beta_0': 0.8525916049912031, 'beta_1': 0.9967269152594854, 'epsilon': 1.3037770773751864e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.56 GiB of which 466.69 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 37.41 GiB is allocated by PyTorch, and 5.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 04:42:50,363] Trial 364 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9537007750601187, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4007302827765526, 'global_pooling': 'mean', 'learning_rate': 0.00011414587198333077, 'weight_decay': 0.0004233666647614394, 'beta_0': 0.8598416544546456, 'beta_1': 0.9920523062374165, 'epsilon': 1.7278174810914513e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 776.69 MiB is free. Including non-PyTorch memory, this process has 43.79 GiB memory in use. Of the allocated memory 36.55 GiB is allocated by PyTorch, and 6.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 05:08:14,905] Trial 365 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9765891154628905, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3767627752583195, 'global_pooling': 'mean', 'learning_rate': 7.639267462920991e-05, 'weight_decay': 0.00033910421831887915, 'beta_0': 0.8496069428019907, 'beta_1': 0.9936955728294579, 'epsilon': 2.948810224120871e-05, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.81 GiB is free. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 34.65 GiB is allocated by PyTorch, and 6.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 05:33:37,816] Trial 366 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9790807471112432, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 47, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38449757150562747, 'global_pooling': 'max', 'learning_rate': 8.99253530238867e-05, 'weight_decay': 0.0005964106298642936, 'beta_0': 0.8447705387882609, 'beta_1': 0.9947423277817825, 'epsilon': 1.38983935486227e-08, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 886.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 36.89 GiB is allocated by PyTorch, and 5.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 05:43:04,022] Trial 367 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9112110341732346, 'batch_size': 33, 'attention_heads': 5, 'hidden_dimension': 229, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3633088252930147, 'global_pooling': 'mean', 'learning_rate': 0.00013674638990498981, 'weight_decay': 0.0004775716748566243, 'beta_0': 0.8509667589935668, 'beta_1': 0.9923261684082124, 'epsilon': 1.1783747899059154e-08, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 06:13:53,778] Trial 368 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9860918723915445, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3918639769356017, 'global_pooling': 'mean', 'learning_rate': 6.314366583466479e-05, 'weight_decay': 0.0003861174883644797, 'beta_0': 0.846984573823395, 'beta_1': 0.9915049279441801, 'epsilon': 2.191929327552372e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 06:44:49,514] Trial 369 finished with value: 0.9696969696969697 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.984336682941437, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3727771471752273, 'global_pooling': 'mean', 'learning_rate': 4.842594560160232e-05, 'weight_decay': 0.000314778085876874, 'beta_0': 0.8540110476030263, 'beta_1': 0.993327200940497, 'epsilon': 1.4944150445765345e-08, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 07:18:59,107] Trial 370 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9820615790959265, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41611796842468846, 'global_pooling': 'mean', 'learning_rate': 2.8490105443729465e-05, 'weight_decay': 0.00023964863792587794, 'beta_0': 0.8421166642177362, 'beta_1': 0.9940244393022669, 'epsilon': 1.87031035312488e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.81 GiB is free. Including non-PyTorch memory, this process has 42.75 GiB memory in use. Of the allocated memory 34.62 GiB is allocated by PyTorch, and 6.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 07:44:24,288] Trial 371 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9790811683655514, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38184824493102953, 'global_pooling': 'mean', 'learning_rate': 1.4201887701045446e-05, 'weight_decay': 1.4911601232263379e-06, 'beta_0': 0.8245974596685991, 'beta_1': 0.9929008998242298, 'epsilon': 1.1887581437425311e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 08:18:04,072] Trial 372 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9833130133168853, 'batch_size': 37, 'attention_heads': 5, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3503426304662276, 'global_pooling': 'mean', 'learning_rate': 3.963080557128461e-05, 'weight_decay': 0.0005225015525075279, 'beta_0': 0.8569234687916917, 'beta_1': 0.9936454687443739, 'epsilon': 1.3799285204213843e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 08:48:38,400] Trial 373 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9871112295103673, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 56, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39587946603082214, 'global_pooling': 'mean', 'learning_rate': 9.976980896407014e-05, 'weight_decay': 0.0004411840197898986, 'beta_0': 0.8486483383683051, 'beta_1': 0.9957455680686355, 'epsilon': 1.727795715466537e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 09:20:30,226] Trial 374 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9808568905739506, 'batch_size': 28, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42299680348193425, 'global_pooling': 'mean', 'learning_rate': 5.559188812591042e-05, 'weight_decay': 0.00037684118844643846, 'beta_0': 0.8519263607240334, 'beta_1': 0.9943446699992549, 'epsilon': 1.1648451427263909e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.92 GiB. GPU 0 has a total capacity of 44.56 GiB of which 650.69 MiB is free. Including non-PyTorch memory, this process has 43.92 GiB memory in use. Of the allocated memory 34.27 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 09:27:59,393] Trial 375 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9010554075031962, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36530969121846113, 'global_pooling': 'mean', 'learning_rate': 7.857272248480835e-05, 'weight_decay': 0.0006293123316129994, 'beta_0': 0.8551044821132812, 'beta_1': 0.9925962535771268, 'epsilon': 1.0065853791929611e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 09:57:46,737] Trial 376 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9847874808979454, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3592796679323732, 'global_pooling': 'mean', 'learning_rate': 0.0001630309579872099, 'weight_decay': 0.0004247940290532149, 'beta_0': 0.8500126372878131, 'beta_1': 0.995326637180474, 'epsilon': 1.5474722291692083e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 10:30:50,094] Trial 377 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.982744995996586, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3759215307323879, 'global_pooling': 'mean', 'learning_rate': 6.828137655559781e-05, 'weight_decay': 0.00031633193342516967, 'beta_0': 0.8530897206258381, 'beta_1': 0.9933815720495334, 'epsilon': 1.3441859124548037e-08, 'balanced_loss': True, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 10:59:56,353] Trial 378 finished with value: 0.9575757575757575 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9781372055665495, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38608944058329303, 'global_pooling': 'mean', 'learning_rate': 0.00010926399010845008, 'weight_decay': 0.0005218746530483955, 'beta_0': 0.8476638188918281, 'beta_1': 0.9930220930508402, 'epsilon': 2.387888332577198e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 6}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 11:32:45,960] Trial 379 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9807149296131951, 'batch_size': 28, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40606557387051645, 'global_pooling': 'mean', 'learning_rate': 4.5808543631686656e-05, 'weight_decay': 0.00016890560318962564, 'beta_0': 0.8579597204066778, 'beta_1': 0.9939371596882416, 'epsilon': 1.8512211255658666e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 12:03:49,524] Trial 380 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.98520254152727, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37214400801221503, 'global_pooling': 'mean', 'learning_rate': 5.749168452869e-05, 'weight_decay': 0.00036602171853037513, 'beta_0': 0.843632747767741, 'beta_1': 0.9945826319643418, 'epsilon': 2.6411942014427054e-06, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 12:36:00,789] Trial 381 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9886818186223455, 'batch_size': 48, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38054871674573404, 'global_pooling': 'mean', 'learning_rate': 3.444888053273698e-05, 'weight_decay': 0.0002749628750330647, 'beta_0': 0.8559165538926998, 'beta_1': 0.9923468338021477, 'epsilon': 1.0055180158641743e-08, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 176.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 37.31 GiB is allocated by PyTorch, and 5.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 13:01:23,084] Trial 382 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9756169123108602, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 4, 'dropout_rate': 0.38984095804126995, 'global_pooling': 'mean', 'learning_rate': 8.824744844213909e-05, 'weight_decay': 0.0004728933688682003, 'beta_0': 0.8120476476253868, 'beta_1': 0.9918247617103895, 'epsilon': 1.34234591468762e-08, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 13:32:34,753] Trial 383 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9827084219916884, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35462181906716683, 'global_pooling': 'mean', 'learning_rate': 7.035697509966712e-05, 'weight_decay': 0.0007220811148908738, 'beta_0': 0.8520023123908993, 'beta_1': 0.9935453681273019, 'epsilon': 2.036676961878384e-07, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 210.69 MiB is free. Including non-PyTorch memory, this process has 44.35 GiB memory in use. Of the allocated memory 30.10 GiB is allocated by PyTorch, and 13.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 14:00:00,881] Trial 384 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9794957576886659, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36808726524218305, 'global_pooling': 'mean', 'learning_rate': 5.409798392762875e-05, 'weight_decay': 0.00034902240841473166, 'beta_0': 0.8600042810855372, 'beta_1': 0.9928935164564988, 'epsilon': 1.1937565833305946e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 14:29:29,353] Trial 385 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9862320904885835, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4004105749734838, 'global_pooling': 'mean', 'learning_rate': 0.00012286499841832535, 'weight_decay': 0.0004167348402327358, 'beta_0': 0.8282353544521925, 'beta_1': 0.99508550564434, 'epsilon': 1.536831629107968e-08, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 20, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 15:01:03,533] Trial 386 finished with value: 0.9575757575757575 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9814126381737347, 'batch_size': 27, 'attention_heads': 4, 'hidden_dimension': 51, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3953664016010724, 'global_pooling': 'mean', 'learning_rate': 4.0413305461023975e-05, 'weight_decay': 0.0005596858425623455, 'beta_0': 0.8455507496145689, 'beta_1': 0.9940460952961858, 'epsilon': 2.0128872892132915e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 15:31:11,046] Trial 387 finished with value: 0.9454545454545454 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9839315078708907, 'batch_size': 35, 'attention_heads': 5, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3759801692981561, 'global_pooling': 'max', 'learning_rate': 8.283267398252274e-05, 'weight_decay': 0.000467316619912408, 'beta_0': 0.8493139844776374, 'beta_1': 0.9932608074540554, 'epsilon': 1.1762040510397193e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 42.87 GiB memory in use. Of the allocated memory 36.12 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 15:56:09,794] Trial 388 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9771552501366215, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 154, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36291929451354005, 'global_pooling': 'mean', 'learning_rate': 6.385788697960296e-05, 'weight_decay': 0.0003080332981255421, 'beta_0': 0.8540162051014547, 'beta_1': 0.9924669221277864, 'epsilon': 1.5341251955334162e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 730.69 MiB is free. Including non-PyTorch memory, this process has 43.84 GiB memory in use. Of the allocated memory 35.01 GiB is allocated by PyTorch, and 7.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 16:05:25,712] Trial 389 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9058994788619632, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38061436680769467, 'global_pooling': 'mean', 'learning_rate': 0.00010370083826085317, 'weight_decay': 0.00040291123630377857, 'beta_0': 0.8506125539791163, 'beta_1': 0.9936915856731666, 'epsilon': 1.3125015143426709e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 686.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 13.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 16:32:28,272] Trial 390 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9800654636334348, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38649915956746134, 'global_pooling': 'mean', 'learning_rate': 5.107580361768816e-05, 'weight_decay': 0.00025864123671972915, 'beta_0': 0.8474864378724277, 'beta_1': 0.9944375926260199, 'epsilon': 1.7489078754010472e-08, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 6}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 24.69 MiB is free. Including non-PyTorch memory, this process has 44.53 GiB memory in use. Of the allocated memory 36.08 GiB is allocated by PyTorch, and 7.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 16:57:29,017] Trial 391 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9740005397046028, 'batch_size': 32, 'attention_heads': 5, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36914517774245753, 'global_pooling': 'mean', 'learning_rate': 7.33356720894352e-05, 'weight_decay': 0.0005559992376480716, 'beta_0': 0.8068612829933263, 'beta_1': 0.9948180223984014, 'epsilon': 1.006492337644179e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 76.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 34.89 GiB is allocated by PyTorch, and 8.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 17:09:17,050] Trial 392 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9377905116323546, 'batch_size': 28, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3583009486157108, 'global_pooling': 'mean', 'learning_rate': 4.230777071530969e-05, 'weight_decay': 0.0003347209079154592, 'beta_0': 0.8532923200236884, 'beta_1': 0.991306641942784, 'epsilon': 1.370343917804485e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 17:39:33,785] Trial 393 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9840426197415648, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37510365911780374, 'global_pooling': 'mean', 'learning_rate': 9.830029097871212e-05, 'weight_decay': 0.00046778938379629366, 'beta_0': 0.8564466736808695, 'beta_1': 0.9928214214501634, 'epsilon': 2.321517675355403e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 3.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.53 GiB is free. Including non-PyTorch memory, this process has 42.02 GiB memory in use. Of the allocated memory 32.84 GiB is allocated by PyTorch, and 8.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 18:05:13,887] Trial 394 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9818636903014352, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39416973913830056, 'global_pooling': 'mean', 'learning_rate': 3.384036073123747e-05, 'weight_decay': 0.0003766042890072052, 'beta_0': 0.8504400274642843, 'beta_1': 0.9919684475701165, 'epsilon': 1.17818185689278e-08, 'balanced_loss': False, 'epochs': 189, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 18:34:12,929] Trial 395 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9898000885821573, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38510809785793254, 'global_pooling': 'mean', 'learning_rate': 0.00013999996113406413, 'weight_decay': 0.00022135658361070685, 'beta_0': 0.848179603442847, 'beta_1': 0.9941272338744608, 'epsilon': 1.6211503673070135e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 19:02:59,963] Trial 396 finished with value: 0.9696969696969697 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9873178324788473, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3673315709369109, 'global_pooling': 'mean', 'learning_rate': 5.835468204708853e-05, 'weight_decay': 0.0006489709960361311, 'beta_0': 0.8545004078868014, 'beta_1': 0.9932484055753592, 'epsilon': 1.1849296133555284e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 886.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 35.07 GiB is allocated by PyTorch, and 7.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 19:28:21,222] Trial 397 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9784887268325356, 'batch_size': 34, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4084194306382225, 'global_pooling': 'mean', 'learning_rate': 8.07684309319297e-05, 'weight_decay': 0.00040528422549354593, 'beta_0': 0.8585953274177447, 'beta_1': 0.9937234913647353, 'epsilon': 1.8847482328971425e-08, 'balanced_loss': True, 'epochs': 97, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 20:01:36,683] Trial 398 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9849879290767274, 'batch_size': 27, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3799691264934687, 'global_pooling': 'mean', 'learning_rate': 2.5917612911297263e-05, 'weight_decay': 0.0005090159562911577, 'beta_0': 0.8523352307990626, 'beta_1': 0.9927713350727952, 'epsilon': 1.0053504525823518e-08, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 20:31:40,623] Trial 399 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9824245107161026, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3722087895849033, 'global_pooling': 'mean', 'learning_rate': 6.591386939798812e-05, 'weight_decay': 0.00031460693604405096, 'beta_0': 0.8458733573066832, 'beta_1': 0.9932394252347209, 'epsilon': 1.2401916122408801e-06, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 21:00:49,804] Trial 400 finished with value: 0.9515151515151515 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9815132555255415, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3910836080816443, 'global_pooling': 'mean', 'learning_rate': 5.075957601910078e-05, 'weight_decay': 0.0004360951107890719, 'beta_0': 0.850060588969739, 'beta_1': 0.9941479245525728, 'epsilon': 1.4533709738031157e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 3.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.49 GiB is free. Including non-PyTorch memory, this process has 42.06 GiB memory in use. Of the allocated memory 33.58 GiB is allocated by PyTorch, and 7.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 21:27:13,661] Trial 401 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9856929764767334, 'batch_size': 33, 'attention_heads': 13, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4015302498690642, 'global_pooling': 'mean', 'learning_rate': 9.134920470112603e-05, 'weight_decay': 0.0002790049144647025, 'beta_0': 0.8616430655600511, 'beta_1': 0.9936547383936297, 'epsilon': 1.3534601513474232e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 212.69 MiB is free. Including non-PyTorch memory, this process has 44.35 GiB memory in use. Of the allocated memory 29.86 GiB is allocated by PyTorch, and 13.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 21:54:10,412] Trial 402 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9798073652330419, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3612097821675706, 'global_pooling': 'mean', 'learning_rate': 4.466722530588632e-05, 'weight_decay': 0.00034898854463181843, 'beta_0': 0.855386298276567, 'beta_1': 0.992420316260127, 'epsilon': 1.56856012239485e-08, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 22:22:42,072] Trial 403 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9832398039586897, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 49, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3833618467454214, 'global_pooling': 'mean', 'learning_rate': 0.012683947452078494, 'weight_decay': 0.0006011268973637328, 'beta_0': 0.8514901512515508, 'beta_1': 0.9946495291312027, 'epsilon': 1.1614971561613419e-08, 'balanced_loss': False, 'epochs': 193, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 754.69 MiB is free. Including non-PyTorch memory, this process has 43.82 GiB memory in use. Of the allocated memory 34.51 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 22:41:56,843] Trial 404 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9699037995390427, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37686269465904254, 'global_pooling': 'mean', 'learning_rate': 0.00012017986922723978, 'weight_decay': 0.0003820329274368946, 'beta_0': 0.8484017272549866, 'beta_1': 0.9908452469278568, 'epsilon': 2.093467144641551e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 788.69 MiB is free. Including non-PyTorch memory, this process has 43.78 GiB memory in use. Of the allocated memory 36.39 GiB is allocated by PyTorch, and 6.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-26 23:06:54,155] Trial 405 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9768109960478836, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3536615737085421, 'global_pooling': 'mean', 'learning_rate': 7.29980243643165e-05, 'weight_decay': 0.000491627124654017, 'beta_0': 0.8535204512198152, 'beta_1': 0.9920664298496762, 'epsilon': 1.000903607575194e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-26 23:39:41,459] Trial 406 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9802594999240924, 'batch_size': 35, 'attention_heads': 5, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37038003464019253, 'global_pooling': 'mean', 'learning_rate': 6.049753169752857e-05, 'weight_decay': 0.00044299825374894894, 'beta_0': 0.8566097006093658, 'beta_1': 0.9929980931867938, 'epsilon': 1.3629249356632958e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-27 00:12:30,122] Trial 407 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9837567491023793, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3899230708231572, 'global_pooling': 'mean', 'learning_rate': 3.580825790112843e-05, 'weight_decay': 8.782602816634485e-05, 'beta_0': 0.8518532828451629, 'beta_1': 0.9939521321493774, 'epsilon': 1.7965630557348766e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.

[TRIAL] 198 [VALIDATION PERFORMANCE] 0.9757575757575757 [TRAINING LOSS] 0.03725554604898207 [VALIDATION LOSS] 0.11455988326391282 

number                                     198
value                                 0.975758
params_threshold                      0.982602
params_attention_heads                       4
params_balanced_loss                     False
params_embedding_pooling_operation         min
params_attention_pooling_operation        mean
params_batch_size                           32
params_dropout_rate                   0.366046
params_early_stopping_patience              15
params_epochs                              176
params_global_pooling                     mean
params_hidden_dimension                     32
params_learning_rate                  0.000125
params_number_of_hidden_layers               2
params_plateau_divider                       8
params_plateau_patience                     25
params_weight_decay                   0.000569
params_beta_0                         0.853714
params_beta_1                         0.991779
params_epsilon                             0.0
user_attrs_epoch                          57.0
user_attrs_training_loss              0.037256
user_attrs_validation_loss             0.11456
params_left_stride                         256
params_right_stride                         32
Name: 198, dtype: object
37 Val: 0.9696969696969697 Test: 0.9522388059701492
38 Val: 0.9757575757575757 Test: 0.9492537313432836
39 Val: 0.9636363636363636 Test: 0.9522388059701492
40 Val: 0.9696969696969697 Test: 0.9522388059701492
41 Val: 0.9636363636363636 Test: 0.9522388059701492
42 Val: 0.9757575757575757 Test: 0.9492537313432836
43 Val: 0.9696969696969697 Test: 0.9522388059701492
slurmstepd: error: *** JOB 14149145 ON gpu050 CANCELLED AT 2024-12-27T04:05:03 DUE TO TIME LIMIT ***
