Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2024-12-18 06:16:12,766] A new study created in RDB with name: IMDb-top_1000-GATv2-FacebookAI-roberta-large-Surrogate-No_Aggregation
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 43.27 GiB memory in use. Of the allocated memory 36.62 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:28:24,504] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9304211818735242, 'batch_size': 41, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
[I 2024-12-18 06:49:27,229] Trial 1 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9939404991670033, 'batch_size': 59, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 1 with value: 0.9030303030303031.
[I 2024-12-18 07:12:45,891] Trial 2 finished with value: 0.9272727272727272 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.988712402130207, 'batch_size': 39, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 124.69 MiB is free. Including non-PyTorch memory, this process has 44.43 GiB memory in use. Of the allocated memory 37.68 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:19:59,625] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9227912369025688, 'batch_size': 36, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 976.69 MiB is free. Including non-PyTorch memory, this process has 43.60 GiB memory in use. Of the allocated memory 37.30 GiB is allocated by PyTorch, and 5.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:28:59,205] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9367746354405981, 'batch_size': 46, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 11.90 GiB. GPU 0 has a total capacity of 44.56 GiB of which 6.39 GiB is free. Including non-PyTorch memory, this process has 38.17 GiB memory in use. Of the allocated memory 35.17 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:45:56,472] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9725883083302352, 'batch_size': 59, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 92.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 38.60 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:58:13,686] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9576846194237897, 'batch_size': 40, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-18 07:58:14,532] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9359455202104634, 'batch_size': 30, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 7.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.03 GiB is free. Including non-PyTorch memory, this process has 41.53 GiB memory in use. Of the allocated memory 36.73 GiB is allocated by PyTorch, and 3.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:14:50,218] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9726018724589289, 'batch_size': 63, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 940.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 66.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 35.33 GiB is allocated by PyTorch, and 8.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:25:20,684] Trial 9 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9491566713529321, 'batch_size': 39, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 08:56:37,000] Trial 10 finished with value: 0.5333333333333333 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9983501014500715, 'batch_size': 17, 'attention_heads': 4, 'hidden_dimension': 48, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5215216634175063, 'global_pooling': 'sum', 'learning_rate': 0.0002656836893415814, 'weight_decay': 8.178412772916804e-06, 'beta_0': 0.8049297522472849, 'beta_1': 0.9840036011509948, 'epsilon': 1.3261882354835817e-05, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 10, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 09:28:59,790] Trial 11 finished with value: 0.9151515151515152 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9990142201899019, 'batch_size': 52, 'attention_heads': 9, 'hidden_dimension': 254, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3965390541849626, 'global_pooling': 'max', 'learning_rate': 4.0083099308315954e-05, 'weight_decay': 1.9948878206620123e-05, 'beta_0': 0.8229997949552795, 'beta_1': 0.9846023396580135, 'epsilon': 1.2304350108398785e-06, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.87 GiB. GPU 0 has a total capacity of 44.56 GiB of which 406.69 MiB is free. Including non-PyTorch memory, this process has 44.16 GiB memory in use. Of the allocated memory 38.64 GiB is allocated by PyTorch, and 4.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:47:02,407] Trial 12 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9836090729914889, 'batch_size': 50, 'attention_heads': 8, 'hidden_dimension': 171, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40659960874307727, 'global_pooling': 'max', 'learning_rate': 1.8370630218209106e-05, 'weight_decay': 1.0332855569626058e-06, 'beta_0': 0.8174825199905924, 'beta_1': 0.9836077130257564, 'epsilon': 9.576134503618629e-07, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1010.69 MiB is free. Including non-PyTorch memory, this process has 43.57 GiB memory in use. Of the allocated memory 39.42 GiB is allocated by PyTorch, and 3.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:05:04,839] Trial 13 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9817569136080533, 'batch_size': 51, 'attention_heads': 7, 'hidden_dimension': 256, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4279039998282282, 'global_pooling': 'mean', 'learning_rate': 5.410300946087553e-05, 'weight_decay': 1.2654238466832892e-05, 'beta_0': 0.8172660649220073, 'beta_1': 0.9829768654405543, 'epsilon': 1.5474111718848809e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 15, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 10:35:51,972] Trial 14 finished with value: 0.5757575757575758 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9999172473279013, 'batch_size': 27, 'attention_heads': 5, 'hidden_dimension': 170, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4809979469424628, 'global_pooling': 'sum', 'learning_rate': 0.0006021958076155605, 'weight_decay': 2.6125225565537756e-05, 'beta_0': 0.8154296187798512, 'beta_1': 0.986552157703884, 'epsilon': 4.2962994729145973e-07, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 7}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 4.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.56 GiB is free. Including non-PyTorch memory, this process has 41.99 GiB memory in use. Of the allocated memory 35.23 GiB is allocated by PyTorch, and 5.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:41:31,956] Trial 15 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9073474392808745, 'batch_size': 51, 'attention_heads': 8, 'hidden_dimension': 254, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3987286688945941, 'global_pooling': 'mean', 'learning_rate': 3.579580328401167e-05, 'weight_decay': 4.074623760133868e-06, 'beta_0': 0.8981752259338325, 'beta_1': 0.9815902362392467, 'epsilon': 6.943484420885042e-06, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 43.30 GiB memory in use. Of the allocated memory 39.69 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:55:24,054] Trial 16 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.960237178472283, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 181, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4574189735526221, 'global_pooling': 'max', 'learning_rate': 0.0007292256933226278, 'weight_decay': 5.108721527149817e-05, 'beta_0': 0.8472355824830708, 'beta_1': 0.99438024478171, 'epsilon': 4.042966932312565e-05, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 292.69 MiB is free. Including non-PyTorch memory, this process has 44.27 GiB memory in use. Of the allocated memory 39.09 GiB is allocated by PyTorch, and 4.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 11:13:44,833] Trial 17 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9856755545289968, 'batch_size': 21, 'attention_heads': 12, 'hidden_dimension': 229, 'number_of_hidden_layers': 2, 'dropout_rate': 0.50832887599105, 'global_pooling': 'max', 'learning_rate': 7.526930870396907e-05, 'weight_decay': 1.150301972489828e-06, 'beta_0': 0.8246123377934255, 'beta_1': 0.9873753253447174, 'epsilon': 3.1470271214203135e-06, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 12, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.73 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.59 GiB is free. Including non-PyTorch memory, this process has 41.96 GiB memory in use. Of the allocated memory 37.85 GiB is allocated by PyTorch, and 2.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 11:30:26,654] Trial 18 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.971276110188116, 'batch_size': 47, 'attention_heads': 9, 'hidden_dimension': 140, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4362665054703549, 'global_pooling': 'mean', 'learning_rate': 0.0002816725408590892, 'weight_decay': 1.0109352208416367e-05, 'beta_0': 0.8088092947015806, 'beta_1': 0.982221457061805, 'epsilon': 3.8237763506282656e-07, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 44.56 GiB of which 410.69 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 33.78 GiB is allocated by PyTorch, and 9.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:01:50,216] Trial 19 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9908435705382247, 'batch_size': 56, 'attention_heads': 6, 'hidden_dimension': 201, 'number_of_hidden_layers': 3, 'dropout_rate': 0.36622570171919266, 'global_pooling': 'sum', 'learning_rate': 0.008360916142616125, 'weight_decay': 3.0151280635279086e-06, 'beta_0': 0.846333398569798, 'beta_1': 0.9851415874349125, 'epsilon': 1.6860727609902753e-05, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process has 43.28 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:15:58,853] Trial 20 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9613613541543788, 'batch_size': 45, 'attention_heads': 16, 'hidden_dimension': 36, 'number_of_hidden_layers': 1, 'dropout_rate': 0.416333368158282, 'global_pooling': 'max', 'learning_rate': 2.806531963620131e-05, 'weight_decay': 1.8173435229581596e-05, 'beta_0': 0.8257347785877511, 'beta_1': 0.9817639347896109, 'epsilon': 3.28940203016142e-06, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 12:37:28,094] Trial 21 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.995074301114471, 'batch_size': 57, 'attention_heads': 12, 'hidden_dimension': 234, 'number_of_hidden_layers': 0, 'dropout_rate': 0.36152052130283363, 'global_pooling': 'max', 'learning_rate': 0.00010048141196024432, 'weight_decay': 0.000225841758603841, 'beta_0': 0.8368234808433618, 'beta_1': 0.9855390421451264, 'epsilon': 1.1907832507216262e-06, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 8.47 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.02 GiB is free. Including non-PyTorch memory, this process has 43.53 GiB memory in use. Of the allocated memory 38.30 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:55:14,586] Trial 22 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9785671787577688, 'batch_size': 55, 'attention_heads': 12, 'hidden_dimension': 242, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3845053586588828, 'global_pooling': 'max', 'learning_rate': 0.00010636684809927412, 'weight_decay': 0.00010926137092191429, 'beta_0': 0.8408684477191014, 'beta_1': 0.9857488057957113, 'epsilon': 7.516536015539293e-07, 'balanced_loss': False, 'epochs': 76, 'early_stopping_patience': 23, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 4.20 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.39 GiB is free. Including non-PyTorch memory, this process has 43.16 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:12:22,783] Trial 23 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9903005710353747, 'batch_size': 64, 'attention_heads': 13, 'hidden_dimension': 220, 'number_of_hidden_layers': 1, 'dropout_rate': 0.342104147343975, 'global_pooling': 'max', 'learning_rate': 0.001146765394472029, 'weight_decay': 5.8397640007537835e-05, 'beta_0': 0.8119654541727195, 'beta_1': 0.9841991103822492, 'epsilon': 1.716173564302197e-07, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 14, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 13:38:38,028] Trial 24 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9996667332132508, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 189, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4603976261265119, 'global_pooling': 'max', 'learning_rate': 0.00016023523030508276, 'weight_decay': 0.00017714783729476892, 'beta_0': 0.824910502813953, 'beta_1': 0.987509735538255, 'epsilon': 2.898372828351585e-06, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 24, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 828.69 MiB is free. Including non-PyTorch memory, this process has 43.74 GiB memory in use. Of the allocated memory 36.89 GiB is allocated by PyTorch, and 5.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 14:02:43,387] Trial 25 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9893346309956067, 'batch_size': 34, 'attention_heads': 10, 'hidden_dimension': 191, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4837634738884747, 'global_pooling': 'mean', 'learning_rate': 0.0002947102219128196, 'weight_decay': 7.090190564514144e-06, 'beta_0': 0.8247012855944147, 'beta_1': 0.9885859274121704, 'epsilon': 3.632814595811304e-06, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 14:33:38,876] Trial 26 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9995588233004048, 'batch_size': 27, 'attention_heads': 8, 'hidden_dimension': 159, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5145809377609565, 'global_pooling': 'max', 'learning_rate': 0.0011834947096087446, 'weight_decay': 5.390020727388527e-05, 'beta_0': 0.8570653954727818, 'beta_1': 0.991144506426741, 'epsilon': 1.4987184749222735e-05, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.40 GiB. GPU 0 has a total capacity of 44.56 GiB of which 226.69 MiB is free. Including non-PyTorch memory, this process has 44.33 GiB memory in use. Of the allocated memory 39.92 GiB is allocated by PyTorch, and 3.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 14:50:31,229] Trial 27 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9776194417197456, 'batch_size': 24, 'attention_heads': 8, 'hidden_dimension': 156, 'number_of_hidden_layers': 3, 'dropout_rate': 0.530607449255703, 'global_pooling': 'max', 'learning_rate': 0.0018667053826717797, 'weight_decay': 4.6307399123514263e-05, 'beta_0': 0.8545753568576869, 'beta_1': 0.9939968811605641, 'epsilon': 1.9399200976869e-05, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.66 GiB. GPU 0 has a total capacity of 44.56 GiB of which 968.69 MiB is free. Including non-PyTorch memory, this process has 43.61 GiB memory in use. Of the allocated memory 38.67 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:06:17,948] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9673253875559835, 'batch_size': 26, 'attention_heads': 7, 'hidden_dimension': 110, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5778378523151202, 'global_pooling': 'mean', 'learning_rate': 0.0005222314481959689, 'weight_decay': 1.8662276460913876e-05, 'beta_0': 0.8563153207343507, 'beta_1': 0.9985045210799033, 'epsilon': 6.974965908739194e-05, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 43.35 GiB memory in use. Of the allocated memory 33.55 GiB is allocated by PyTorch, and 8.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:13:49,390] Trial 29 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9013794237547178, 'batch_size': 42, 'attention_heads': 9, 'hidden_dimension': 82, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5039164679577534, 'global_pooling': 'sum', 'learning_rate': 0.01387995939330834, 'weight_decay': 4.858445107124492e-06, 'beta_0': 0.864405102967954, 'beta_1': 0.9913361385778628, 'epsilon': 8.416253053060569e-06, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 12, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 970.69 MiB is free. Including non-PyTorch memory, this process has 43.61 GiB memory in use. Of the allocated memory 38.16 GiB is allocated by PyTorch, and 4.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:33:18,363] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9525736052741787, 'batch_size': 17, 'attention_heads': 5, 'hidden_dimension': 128, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5510839891654924, 'global_pooling': 'max', 'learning_rate': 0.00880257442976611, 'weight_decay': 2.5682674759824535e-06, 'beta_0': 0.8001058441780993, 'beta_1': 0.9908126336371739, 'epsilon': 2.5055238623456908e-05, 'balanced_loss': False, 'epochs': 126, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 16:07:31,412] Trial 31 finished with value: 0.9212121212121213 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9994062621998191, 'batch_size': 35, 'attention_heads': 10, 'hidden_dimension': 164, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4710526537346568, 'global_pooling': 'max', 'learning_rate': 0.001052993123427486, 'weight_decay': 0.00016816096764289774, 'beta_0': 0.819954260471711, 'beta_1': 0.9883146785427022, 'epsilon': 1.149120533623223e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 24, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 44.56 GiB of which 810.69 MiB is free. Including non-PyTorch memory, this process has 43.76 GiB memory in use. Of the allocated memory 38.10 GiB is allocated by PyTorch, and 4.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:31:38,375] Trial 32 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9883183177017039, 'batch_size': 37, 'attention_heads': 11, 'hidden_dimension': 166, 'number_of_hidden_layers': 4, 'dropout_rate': 0.47285164205309105, 'global_pooling': 'max', 'learning_rate': 0.0010663746694489584, 'weight_decay': 7.901489803172158e-05, 'beta_0': 0.8105849087817313, 'beta_1': 0.9927285652731492, 'epsilon': 1.9239721637829556e-08, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 24, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 17:05:00,918] Trial 33 finished with value: 0.9272727272727272 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9933766327072511, 'batch_size': 31, 'attention_heads': 9, 'hidden_dimension': 139, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43573178772322396, 'global_pooling': 'max', 'learning_rate': 0.0016521862636292516, 'weight_decay': 0.00033515367090255783, 'beta_0': 0.8299274635469311, 'beta_1': 0.9890867937092697, 'epsilon': 1.2227930638094405e-08, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 24, 'plateau_patience': 22, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 17:43:35,917] Trial 34 finished with value: 0.9212121212121213 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9932177204393354, 'batch_size': 32, 'attention_heads': 11, 'hidden_dimension': 132, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4379555688321385, 'global_pooling': 'max', 'learning_rate': 0.0018103440447932019, 'weight_decay': 0.00093191913774659, 'beta_0': 0.8304944573080592, 'beta_1': 0.9889944419378862, 'epsilon': 1.1641180005180234e-08, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 24, 'plateau_patience': 23, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 206.69 MiB is free. Including non-PyTorch memory, this process has 44.35 GiB memory in use. Of the allocated memory 35.75 GiB is allocated by PyTorch, and 7.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 18:05:27,108] Trial 35 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9808355672878223, 'batch_size': 28, 'attention_heads': 7, 'hidden_dimension': 155, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5039355184536205, 'global_pooling': 'max', 'learning_rate': 0.005306192871523381, 'weight_decay': 0.0004343222469359283, 'beta_0': 0.8544625757481293, 'beta_1': 0.9908052245174568, 'epsilon': 1.9124691515546688e-08, 'balanced_loss': False, 'epochs': 84, 'early_stopping_patience': 25, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 18:37:39,957] Trial 36 finished with value: 0.9151515151515152 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9944649331834361, 'batch_size': 36, 'attention_heads': 8, 'hidden_dimension': 144, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5339065426301994, 'global_pooling': 'mean', 'learning_rate': 0.0013480718114344998, 'weight_decay': 0.00014415915754767135, 'beta_0': 0.8190882691398711, 'beta_1': 0.9951491769091582, 'epsilon': 9.192048413221175e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 712.69 MiB is free. Including non-PyTorch memory, this process has 43.86 GiB memory in use. Of the allocated memory 39.25 GiB is allocated by PyTorch, and 3.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 18:56:21,532] Trial 37 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9859757906537427, 'batch_size': 22, 'attention_heads': 10, 'hidden_dimension': 114, 'number_of_hidden_layers': 4, 'dropout_rate': 0.45193745577280064, 'global_pooling': 'max', 'learning_rate': 0.0005256574068083214, 'weight_decay': 0.0002547760705225377, 'beta_0': 0.8426779829686798, 'beta_1': 0.9920167796460191, 'epsilon': 2.7498603231231934e-08, 'balanced_loss': True, 'epochs': 108, 'early_stopping_patience': 23, 'plateau_patience': 24, 'plateau_divider': 5}. Best is trial 2 with value: 0.9272727272727272.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-18 18:56:22,442] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9761753003842241, 'batch_size': 43, 'attention_heads': 9, 'hidden_dimension': 86, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5966048419514559, 'global_pooling': 'max', 'learning_rate': 0.0027740279555670475, 'weight_decay': 0.0004045731812263393, 'beta_0': 0.8309588373415022, 'beta_1': 0.9899366978371966, 'epsilon': 1.0611191218922006e-08, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 22, 'plateau_patience': 21, 'plateau_divider': 6}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1002.69 MiB is free. Including non-PyTorch memory, this process has 43.57 GiB memory in use. Of the allocated memory 36.35 GiB is allocated by PyTorch, and 6.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:11:40,565] Trial 39 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9657173745556672, 'batch_size': 38, 'attention_heads': 5, 'hidden_dimension': 177, 'number_of_hidden_layers': 4, 'dropout_rate': 0.46747282758028835, 'global_pooling': 'mean', 'learning_rate': 0.02390078730774726, 'weight_decay': 7.548809150910362e-05, 'beta_0': 0.8696817216781626, 'beta_1': 0.9961575139907141, 'epsilon': 1.084769683452037e-05, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 25, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 2.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 36.73 GiB is allocated by PyTorch, and 6.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:21:09,824] Trial 40 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9393411085284503, 'batch_size': 29, 'attention_heads': 11, 'hidden_dimension': 208, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5525409957807512, 'global_pooling': 'sum', 'learning_rate': 0.005159096124515859, 'weight_decay': 0.0001983255502409713, 'beta_0': 0.807407755139052, 'beta_1': 0.9931248064521062, 'epsilon': 7.357304537266892e-08, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 972.69 MiB is free. Including non-PyTorch memory, this process has 43.60 GiB memory in use. Of the allocated memory 38.20 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:45:31,399] Trial 41 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.992963962744387, 'batch_size': 33, 'attention_heads': 11, 'hidden_dimension': 130, 'number_of_hidden_layers': 4, 'dropout_rate': 0.42775640241109775, 'global_pooling': 'max', 'learning_rate': 0.001803430887661121, 'weight_decay': 0.0009142754921018032, 'beta_0': 0.8328933895493785, 'beta_1': 0.9887645621782087, 'epsilon': 1.0490069954814846e-08, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 24, 'plateau_patience': 23, 'plateau_divider': 4}. Best is trial 2 with value: 0.9272727272727272.
[I 2024-12-18 20:24:19,181] Trial 42 finished with value: 0.9333333333333333 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9945639537098113, 'batch_size': 32, 'attention_heads': 13, 'hidden_dimension': 144, 'number_of_hidden_layers': 4, 'dropout_rate': 0.44218376001256415, 'global_pooling': 'max', 'learning_rate': 0.0009312725517436095, 'weight_decay': 0.0009946185229000495, 'beta_0': 0.8304860637310261, 'beta_1': 0.9891798606966712, 'epsilon': 3.339317445785037e-08, 'balanced_loss': False, 'epochs': 199, 'early_stopping_patience': 24, 'plateau_patience': 24, 'plateau_divider': 4}. Best is trial 42 with value: 0.9333333333333333.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-18 20:24:19,982] Trial 43 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9957447873747992, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 163, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4929688142439509, 'global_pooling': 'max', 'learning_rate': 0.0003633082616245644, 'weight_decay': 0.0005523976220646152, 'beta_0': 0.8383399016342902, 'beta_1': 0.9901845639336001, 'epsilon': 2.7871373604083816e-08, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 24, 'plateau_patience': 25, 'plateau_divider': 3}. Best is trial 42 with value: 0.9333333333333333.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.29 GiB is free. Including non-PyTorch memory, this process has 40.26 GiB memory in use. Of the allocated memory 30.44 GiB is allocated by PyTorch, and 8.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:46:13,463] Trial 44 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9855935763985877, 'batch_size': 25, 'attention_heads': 13, 'hidden_dimension': 150, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4431826246818211, 'global_pooling': 'max', 'learning_rate': 0.0008715932134136418, 'weight_decay': 0.0006853975618991855, 'beta_0': 0.8140878583231, 'beta_1': 0.9865830366131308, 'epsilon': 2.9945454425215007e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 22, 'plateau_patience': 24, 'plateau_divider': 5}. Best is trial 42 with value: 0.9333333333333333.
CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 44.56 GiB of which 338.69 MiB is free. Including non-PyTorch memory, this process has 44.22 GiB memory in use. Of the allocated memory 36.85 GiB is allocated by PyTorch, and 6.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:53:21,480] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9206221012945047, 'batch_size': 30, 'attention_heads': 7, 'hidden_dimension': 184, 'number_of_hidden_layers': 3, 'dropout_rate': 0.41937204983373616, 'global_pooling': 'max', 'learning_rate': 0.00016764199107143602, 'weight_decay': 0.00034033210642227046, 'beta_0': 0.8041425362194349, 'beta_1': 0.9879821617698092, 'epsilon': 1.2202568930329602e-07, 'balanced_loss': False, 'epochs': 193, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 4}. Best is trial 42 with value: 0.9333333333333333.
[I 2024-12-18 21:14:12,896] Trial 46 finished with value: 0.806060606060606 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9998647521062514, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 123, 'number_of_hidden_layers': 4, 'dropout_rate': 0.46818176153027474, 'global_pooling': 'mean', 'learning_rate': 0.00039122050950305277, 'weight_decay': 8.238789585344233e-05, 'beta_0': 0.830125025972873, 'beta_1': 0.9894301203536424, 'epsilon': 5.6908177920397934e-08, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 42 with value: 0.9333333333333333.
CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 230.69 MiB is free. Including non-PyTorch memory, this process has 44.33 GiB memory in use. Of the allocated memory 33.45 GiB is allocated by PyTorch, and 9.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 21:36:06,743] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9828760788229333, 'batch_size': 34, 'attention_heads': 16, 'hidden_dimension': 144, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4475377345744357, 'global_pooling': 'max', 'learning_rate': 0.0032542872144763133, 'weight_decay': 0.000600443573125101, 'beta_0': 0.8200252652034389, 'beta_1': 0.9918575828196067, 'epsilon': 1.5525922331011368e-08, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 11, 'plateau_patience': 21, 'plateau_divider': 6}. Best is trial 42 with value: 0.9333333333333333.
slurmstepd: error: *** JOB 14110900 ON gpu046 CANCELLED AT 2024-12-18T22:16:13 DUE TO TIME LIMIT ***
