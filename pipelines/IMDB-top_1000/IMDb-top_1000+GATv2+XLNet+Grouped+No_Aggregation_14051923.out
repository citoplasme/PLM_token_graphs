[I 2024-12-03 05:08:14,029] Using an existing study with name 'IMDb-top_1000-GATv2-xlnet-xlnet-base-cased-Grouped-No_Aggregation' instead of creating a new one.
[I 2024-12-03 05:21:09,990] Trial 103 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9984609671145198, 'batch_size': 31, 'attention_heads': 12, 'hidden_dimension': 212, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33158813177294844, 'global_pooling': 'mean', 'learning_rate': 0.006444260588216466, 'weight_decay': 3.0638485731595e-05, 'beta_0': 0.8813745877825381, 'beta_1': 0.9882896554542113, 'epsilon': 4.5200959937525705e-07, 'balanced_loss': True, 'epochs': 162, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 05:34:55,252] Trial 104 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9862974001101031, 'batch_size': 25, 'attention_heads': 14, 'hidden_dimension': 168, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3185728536504865, 'global_pooling': 'mean', 'learning_rate': 0.005278164600712502, 'weight_decay': 2.4053799592199816e-05, 'beta_0': 0.8785185586127129, 'beta_1': 0.9865546878912452, 'epsilon': 7.654524038480981e-07, 'balanced_loss': True, 'epochs': 180, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 05:48:06,569] Trial 105 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9764730953363667, 'batch_size': 29, 'attention_heads': 4, 'hidden_dimension': 207, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30871518273140225, 'global_pooling': 'mean', 'learning_rate': 0.0017667065947386013, 'weight_decay': 8.371495487359704e-05, 'beta_0': 0.8681974861095704, 'beta_1': 0.9830473731749448, 'epsilon': 6.388354212482504e-07, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 16, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.34 GiB is free. Including non-PyTorch memory, this process has 39.21 GiB memory in use. Of the allocated memory 26.33 GiB is allocated by PyTorch, and 11.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 05:58:32,047] Trial 106 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9916064278427584, 'batch_size': 33, 'attention_heads': 8, 'hidden_dimension': 44, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3006536145166815, 'global_pooling': 'mean', 'learning_rate': 0.025922482513070996, 'weight_decay': 3.330238156087226e-05, 'beta_0': 0.8746830799396006, 'beta_1': 0.9870951289196184, 'epsilon': 3.5695663067741094e-07, 'balanced_loss': True, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 06:12:28,850] Trial 107 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9837799660176711, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 155, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3277677791671513, 'global_pooling': 'max', 'learning_rate': 0.004234625820468511, 'weight_decay': 5.567616678768047e-05, 'beta_0': 0.8620350635333444, 'beta_1': 0.9832566112000105, 'epsilon': 2.0073657982546006e-07, 'balanced_loss': True, 'epochs': 107, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 41.98 GiB memory in use. Of the allocated memory 27.72 GiB is allocated by PyTorch, and 13.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 06:23:59,002] Trial 108 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9453300011700482, 'batch_size': 32, 'attention_heads': 12, 'hidden_dimension': 133, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35366844486457316, 'global_pooling': 'mean', 'learning_rate': 0.008300216828422349, 'weight_decay': 4.743797278477795e-05, 'beta_0': 0.8721281852258923, 'beta_1': 0.9838585073924461, 'epsilon': 1.3863799433570917e-08, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 06:37:23,625] Trial 109 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9813336115813235, 'batch_size': 36, 'attention_heads': 14, 'hidden_dimension': 93, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3204135993395564, 'global_pooling': 'mean', 'learning_rate': 0.0027599435884825816, 'weight_decay': 0.00010954149196038602, 'beta_0': 0.8637288131257608, 'beta_1': 0.9858327545700399, 'epsilon': 2.93298717167981e-07, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 174.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 28.61 GiB is allocated by PyTorch, and 14.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 06:48:51,797] Trial 110 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9388168451514042, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30492429172428764, 'global_pooling': 'mean', 'learning_rate': 0.010009390908416111, 'weight_decay': 3.728103748516454e-05, 'beta_0': 0.8550431911115145, 'beta_1': 0.9926254545312158, 'epsilon': 1.0567094673404104e-06, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 07:03:06,541] Trial 111 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.932929546832715, 'batch_size': 30, 'attention_heads': 15, 'hidden_dimension': 39, 'number_of_hidden_layers': 0, 'dropout_rate': 0.38447239450500525, 'global_pooling': 'sum', 'learning_rate': 0.004760216621465471, 'weight_decay': 7.075395317420266e-05, 'beta_0': 0.8867674938348838, 'beta_1': 0.9819788225360012, 'epsilon': 5.115365566131107e-05, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 834.69 MiB is free. Including non-PyTorch memory, this process has 43.74 GiB memory in use. Of the allocated memory 30.31 GiB is allocated by PyTorch, and 12.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 07:14:35,428] Trial 112 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9263864182969643, 'batch_size': 26, 'attention_heads': 15, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3833309379268296, 'global_pooling': 'sum', 'learning_rate': 0.004672079578998336, 'weight_decay': 6.814410612489654e-05, 'beta_0': 0.8920463389144588, 'beta_1': 0.982221127888173, 'epsilon': 4.4337181507454194e-05, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 07:29:11,959] Trial 113 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9314284698683347, 'batch_size': 30, 'attention_heads': 16, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3918581521203987, 'global_pooling': 'sum', 'learning_rate': 0.0033759789306150915, 'weight_decay': 9.560812667423658e-05, 'beta_0': 0.8869366076980597, 'beta_1': 0.9818670083196785, 'epsilon': 9.324556997210214e-05, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.37 GiB is free. Including non-PyTorch memory, this process has 43.19 GiB memory in use. Of the allocated memory 33.99 GiB is allocated by PyTorch, and 8.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 07:41:53,022] Trial 114 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9331648752274688, 'batch_size': 39, 'attention_heads': 15, 'hidden_dimension': 177, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4063383171251994, 'global_pooling': 'sum', 'learning_rate': 0.015120994094942585, 'weight_decay': 6.0058348607595997e-05, 'beta_0': 0.8805167457349338, 'beta_1': 0.9816826523067308, 'epsilon': 5.18977210781598e-06, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 07:55:33,494] Trial 115 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9411774069109435, 'batch_size': 31, 'attention_heads': 14, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34269861773276117, 'global_pooling': 'sum', 'learning_rate': 0.019655838360090362, 'weight_decay': 0.00013552134497324598, 'beta_0': 0.8824062202836873, 'beta_1': 0.9895253553800575, 'epsilon': 3.285731734713685e-05, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 19, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 08:09:15,355] Trial 116 finished with value: 0.8484848484848485 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9436624745317224, 'batch_size': 28, 'attention_heads': 14, 'hidden_dimension': 42, 'number_of_hidden_layers': 0, 'dropout_rate': 0.37040062060711937, 'global_pooling': 'sum', 'learning_rate': 6.849905332388105e-05, 'weight_decay': 4.41082772640272e-05, 'beta_0': 0.8951191360493677, 'beta_1': 0.9811658363855749, 'epsilon': 8.691205876746762e-08, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 44.56 GiB of which 522.69 MiB is free. Including non-PyTorch memory, this process has 44.04 GiB memory in use. Of the allocated memory 26.59 GiB is allocated by PyTorch, and 16.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:21:56,082] Trial 117 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9384904699989355, 'batch_size': 37, 'attention_heads': 15, 'hidden_dimension': 59, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3144980059158963, 'global_pooling': 'mean', 'learning_rate': 0.006293707632216202, 'weight_decay': 1.60732271681574e-05, 'beta_0': 0.8771557334733833, 'beta_1': 0.9886121934148191, 'epsilon': 2.923571642456275e-07, 'balanced_loss': True, 'epochs': 176, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.52 GiB is free. Including non-PyTorch memory, this process has 41.04 GiB memory in use. Of the allocated memory 29.03 GiB is allocated by PyTorch, and 10.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:33:25,711] Trial 118 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9354477752012215, 'batch_size': 58, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3288187662653941, 'global_pooling': 'max', 'learning_rate': 0.000354927075457581, 'weight_decay': 9.754006877869917e-06, 'beta_0': 0.859468053135998, 'beta_1': 0.9804217147738356, 'epsilon': 2.134162213338919e-07, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 39.45 GiB memory in use. Of the allocated memory 27.50 GiB is allocated by PyTorch, and 10.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:44:55,749] Trial 119 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9468630633335111, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33697499367327977, 'global_pooling': 'mean', 'learning_rate': 0.008345012207927488, 'weight_decay': 0.00021543336272189425, 'beta_0': 0.8837706737498072, 'beta_1': 0.9876130782325782, 'epsilon': 1.5862372801127616e-05, 'balanced_loss': True, 'epochs': 50, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.86 GiB is free. Including non-PyTorch memory, this process has 38.69 GiB memory in use. Of the allocated memory 27.88 GiB is allocated by PyTorch, and 9.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:56:25,270] Trial 120 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9442888180662551, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 119, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3651295569940664, 'global_pooling': 'mean', 'learning_rate': 0.030546827592181094, 'weight_decay': 7.79180008851313e-05, 'beta_0': 0.8898817753622441, 'beta_1': 0.9957406100154429, 'epsilon': 9.999700912903879e-06, 'balanced_loss': True, 'epochs': 155, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 09:09:44,254] Trial 121 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9960783139549328, 'batch_size': 29, 'attention_heads': 16, 'hidden_dimension': 184, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42783426110472467, 'global_pooling': 'mean', 'learning_rate': 0.00102811929704259, 'weight_decay': 0.00011638040782675289, 'beta_0': 0.8322515642032632, 'beta_1': 0.9852735187760184, 'epsilon': 2.7261241777460893e-08, 'balanced_loss': True, 'epochs': 101, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 852.69 MiB is free. Including non-PyTorch memory, this process has 43.72 GiB memory in use. Of the allocated memory 33.14 GiB is allocated by PyTorch, and 9.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 09:23:16,566] Trial 122 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9576715881000291, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 51, 'number_of_hidden_layers': 1, 'dropout_rate': 0.35236752983138, 'global_pooling': 'mean', 'learning_rate': 0.011976730469127369, 'weight_decay': 4.565889052091647e-05, 'beta_0': 0.8795603756112621, 'beta_1': 0.9858322126893578, 'epsilon': 1.296852456199336e-08, 'balanced_loss': True, 'epochs': 180, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 43.51 GiB memory in use. Of the allocated memory 31.25 GiB is allocated by PyTorch, and 11.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 09:36:48,376] Trial 123 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9714960558488491, 'batch_size': 36, 'attention_heads': 16, 'hidden_dimension': 61, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34550993834830285, 'global_pooling': 'mean', 'learning_rate': 0.014181447424998044, 'weight_decay': 2.501059841463208e-05, 'beta_0': 0.8866952753554229, 'beta_1': 0.9846576522797733, 'epsilon': 1.0778178687125723e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.56 GiB is free. Including non-PyTorch memory, this process has 39.99 GiB memory in use. Of the allocated memory 33.83 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 09:50:20,307] Trial 124 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9525146730136945, 'batch_size': 33, 'attention_heads': 15, 'hidden_dimension': 37, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3792899113999234, 'global_pooling': 'mean', 'learning_rate': 0.043944983690134734, 'weight_decay': 6.340567407017046e-05, 'beta_0': 0.8749607562415943, 'beta_1': 0.9869890539950146, 'epsilon': 2.3486204938385836e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.56 GiB is free. Including non-PyTorch memory, this process has 41.00 GiB memory in use. Of the allocated memory 29.13 GiB is allocated by PyTorch, and 10.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 10:00:46,774] Trial 125 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9643682694413229, 'batch_size': 31, 'attention_heads': 14, 'hidden_dimension': 55, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3597515007023986, 'global_pooling': 'mean', 'learning_rate': 0.0015396867275086093, 'weight_decay': 1.8819944766336562e-05, 'beta_0': 0.8674231280747173, 'beta_1': 0.9899997909493998, 'epsilon': 1.9114213001099664e-08, 'balanced_loss': True, 'epochs': 188, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 638.69 MiB is free. Including non-PyTorch memory, this process has 43.93 GiB memory in use. Of the allocated memory 28.74 GiB is allocated by PyTorch, and 14.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 10:14:17,157] Trial 126 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9377976883233787, 'batch_size': 38, 'attention_heads': 15, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33373566003564104, 'global_pooling': 'mean', 'learning_rate': 0.010513197472040174, 'weight_decay': 3.281890633256649e-05, 'beta_0': 0.8776576770840548, 'beta_1': 0.9863034071413272, 'epsilon': 1.1607712233340146e-08, 'balanced_loss': True, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 10:27:38,981] Trial 127 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9547875408041255, 'batch_size': 35, 'attention_heads': 11, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.313404697189695, 'global_pooling': 'mean', 'learning_rate': 0.005433834960736263, 'weight_decay': 8.751582254545872e-05, 'beta_0': 0.8712223507433418, 'beta_1': 0.9881173294608672, 'epsilon': 3.2943204911144293e-08, 'balanced_loss': True, 'epochs': 177, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 10:41:06,400] Trial 128 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9601981613804833, 'batch_size': 26, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35006484326900317, 'global_pooling': 'mean', 'learning_rate': 0.0038975294577204295, 'weight_decay': 4.015441880763208e-05, 'beta_0': 0.8801695998148901, 'beta_1': 0.9914341535096232, 'epsilon': 2.437912678437533e-05, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.89 GiB is free. Including non-PyTorch memory, this process has 40.66 GiB memory in use. Of the allocated memory 24.63 GiB is allocated by PyTorch, and 14.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 10:52:35,685] Trial 129 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.968037770683606, 'batch_size': 30, 'attention_heads': 16, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5557223781267985, 'global_pooling': 'max', 'learning_rate': 0.007171841774080036, 'weight_decay': 7.46465561201142e-05, 'beta_0': 0.8736622031926765, 'beta_1': 0.9854416472556466, 'epsilon': 4.565404411261106e-07, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 16, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.61 GiB is free. Including non-PyTorch memory, this process has 40.94 GiB memory in use. Of the allocated memory 31.51 GiB is allocated by PyTorch, and 8.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 11:03:03,257] Trial 130 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9413805245994786, 'batch_size': 27, 'attention_heads': 14, 'hidden_dimension': 245, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3057883092217047, 'global_pooling': 'sum', 'learning_rate': 0.0031957752872551312, 'weight_decay': 4.7080556395826225e-05, 'beta_0': 0.8859700120563722, 'beta_1': 0.9848817464012511, 'epsilon': 4.981406610433736e-08, 'balanced_loss': True, 'epochs': 80, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 11:16:46,061] Trial 131 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9495721607226401, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3239582319542556, 'global_pooling': 'mean', 'learning_rate': 0.002171689035365421, 'weight_decay': 0.0001846778630193048, 'beta_0': 0.8825582083640524, 'beta_1': 0.9841270753447875, 'epsilon': 1.553521614750089e-08, 'balanced_loss': True, 'epochs': 87, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 11:31:51,575] Trial 132 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9583622789472009, 'batch_size': 25, 'attention_heads': 16, 'hidden_dimension': 61, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3583508762849407, 'global_pooling': 'mean', 'learning_rate': 0.0016059367056827044, 'weight_decay': 0.00016232548265293382, 'beta_0': 0.891907658691918, 'beta_1': 0.9906555006994424, 'epsilon': 1.0767733288433127e-07, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 11:46:04,204] Trial 133 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9474777251326226, 'batch_size': 24, 'attention_heads': 16, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.34785125371428033, 'global_pooling': 'mean', 'learning_rate': 0.00186297038758391, 'weight_decay': 0.00015862928397617325, 'beta_0': 0.8698433794764507, 'beta_1': 0.9920370564007154, 'epsilon': 1.464952265764004e-07, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.44 GiB is free. Including non-PyTorch memory, this process has 41.11 GiB memory in use. Of the allocated memory 28.60 GiB is allocated by PyTorch, and 11.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 11:58:46,290] Trial 134 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9428802324635002, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 143, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3694751289213167, 'global_pooling': 'mean', 'learning_rate': 0.0012378439942511068, 'weight_decay': 0.00023264815308150004, 'beta_0': 0.8844181294202024, 'beta_1': 0.9890924552647294, 'epsilon': 1.2228065709420638e-07, 'balanced_loss': True, 'epochs': 177, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 12:13:47,267] Trial 135 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9624688427023063, 'batch_size': 28, 'attention_heads': 15, 'hidden_dimension': 76, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5779497373034495, 'global_pooling': 'mean', 'learning_rate': 0.0026246406024673288, 'weight_decay': 0.00010598126533657503, 'beta_0': 0.8647785361805419, 'beta_1': 0.9903309338427272, 'epsilon': 1.7770640194639414e-07, 'balanced_loss': True, 'epochs': 173, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 12:27:27,973] Trial 136 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9454725129939835, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3393719498940173, 'global_pooling': 'mean', 'learning_rate': 0.004910870594926151, 'weight_decay': 1.2883181389354908e-05, 'beta_0': 0.8890910318274343, 'beta_1': 0.989594756079892, 'epsilon': 2.4769495895892944e-07, 'balanced_loss': True, 'epochs': 95, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.82 GiB is free. Including non-PyTorch memory, this process has 40.73 GiB memory in use. Of the allocated memory 26.95 GiB is allocated by PyTorch, and 12.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 12:38:58,157] Trial 137 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9511003874038761, 'batch_size': 26, 'attention_heads': 15, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31995428998969094, 'global_pooling': 'mean', 'learning_rate': 0.02242422572548989, 'weight_decay': 5.354083968426195e-05, 'beta_0': 0.8977103313065811, 'beta_1': 0.9910797074873774, 'epsilon': 7.035583251519167e-08, 'balanced_loss': True, 'epochs': 162, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 12:53:27,277] Trial 138 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9599576228779586, 'batch_size': 23, 'attention_heads': 16, 'hidden_dimension': 40, 'number_of_hidden_layers': 1, 'dropout_rate': 0.35762561616866345, 'global_pooling': 'mean', 'learning_rate': 0.017822450345657916, 'weight_decay': 0.00013691315310421952, 'beta_0': 0.8509799109402665, 'beta_1': 0.9960568957392457, 'epsilon': 3.3875220656611155e-07, 'balanced_loss': True, 'epochs': 126, 'early_stopping_patience': 19, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 13:07:22,575] Trial 139 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9400637555071347, 'batch_size': 35, 'attention_heads': 15, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3000387837022259, 'global_pooling': 'mean', 'learning_rate': 0.0006324886619748406, 'weight_decay': 2.7467880051666835e-05, 'beta_0': 0.8925325451499626, 'beta_1': 0.9826927983340706, 'epsilon': 1.7113734421358974e-07, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.82 GiB is free. Including non-PyTorch memory, this process has 39.73 GiB memory in use. Of the allocated memory 29.75 GiB is allocated by PyTorch, and 8.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 13:18:52,303] Trial 140 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9303854206848852, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5896086992722468, 'global_pooling': 'mean', 'learning_rate': 0.0041895555562074405, 'weight_decay': 0.0004095241935911351, 'beta_0': 0.8763162322755847, 'beta_1': 0.9951149437131547, 'epsilon': 1.3663159547246605e-07, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 1.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 42.62 GiB memory in use. Of the allocated memory 34.75 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 13:31:33,596] Trial 141 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9348117677690568, 'batch_size': 29, 'attention_heads': 14, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3646667927678289, 'global_pooling': 'max', 'learning_rate': 0.009289343813156672, 'weight_decay': 7.523157340882215e-06, 'beta_0': 0.8563743003959262, 'beta_1': 0.988811696218235, 'epsilon': 2.680433785081568e-07, 'balanced_loss': True, 'epochs': 157, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 13:44:46,335] Trial 142 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9536600439227656, 'batch_size': 36, 'attention_heads': 12, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.38616388302901317, 'global_pooling': 'mean', 'learning_rate': 0.013280624796147207, 'weight_decay': 5.57168611763507e-05, 'beta_0': 0.8677776720020998, 'beta_1': 0.98754127671203, 'epsilon': 3.659918318111149e-08, 'balanced_loss': True, 'epochs': 123, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.93 GiB is free. Including non-PyTorch memory, this process has 39.62 GiB memory in use. Of the allocated memory 26.21 GiB is allocated by PyTorch, and 12.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 13:56:15,530] Trial 143 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.956707308145478, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3756082553603345, 'global_pooling': 'mean', 'learning_rate': 0.02105322207092846, 'weight_decay': 9.251373929072009e-05, 'beta_0': 0.8625737325650935, 'beta_1': 0.9866094512579049, 'epsilon': 6.086123881219433e-08, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 14:09:32,333] Trial 144 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9551279895851469, 'batch_size': 35, 'attention_heads': 14, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.49839878616047506, 'global_pooling': 'mean', 'learning_rate': 0.032813360328616085, 'weight_decay': 6.755249827531614e-05, 'beta_0': 0.8461034144250773, 'beta_1': 0.9940302487499147, 'epsilon': 2.1871885822385726e-07, 'balanced_loss': True, 'epochs': 111, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 14:22:51,463] Trial 145 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9487912501246701, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.39886220800532446, 'global_pooling': 'mean', 'learning_rate': 0.01680566282115888, 'weight_decay': 0.00019458826615900777, 'beta_0': 0.866535458189056, 'beta_1': 0.9872710261512678, 'epsilon': 5.633019580441949e-08, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 14:36:48,927] Trial 146 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9437691315239974, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.373037685164277, 'global_pooling': 'mean', 'learning_rate': 0.007501938641779944, 'weight_decay': 0.0002933619120698364, 'beta_0': 0.861435589269239, 'beta_1': 0.989864176464145, 'epsilon': 8.524621851262817e-08, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 14:50:13,124] Trial 147 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9372300369372566, 'batch_size': 27, 'attention_heads': 12, 'hidden_dimension': 44, 'number_of_hidden_layers': 0, 'dropout_rate': 0.38167899768584507, 'global_pooling': 'mean', 'learning_rate': 0.027368360734216758, 'weight_decay': 3.62434500602858e-05, 'beta_0': 0.8787934777334885, 'beta_1': 0.9876833477131822, 'epsilon': 4.1503908821001497e-08, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 15:03:48,498] Trial 148 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9591974470887659, 'batch_size': 20, 'attention_heads': 13, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3152287912532145, 'global_pooling': 'sum', 'learning_rate': 0.005901892263763123, 'weight_decay': 5.013626011376955e-05, 'beta_0': 0.871332055695672, 'beta_1': 0.9835102492142541, 'epsilon': 3.8840850697301343e-07, 'balanced_loss': True, 'epochs': 134, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 594.69 MiB is free. Including non-PyTorch memory, this process has 43.97 GiB memory in use. Of the allocated memory 30.21 GiB is allocated by PyTorch, and 12.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 15:15:17,861] Trial 149 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9269753665556171, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 224, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33093554219596677, 'global_pooling': 'mean', 'learning_rate': 0.0025713841374218724, 'weight_decay': 7.899426912310223e-05, 'beta_0': 0.8876867584082035, 'beta_1': 0.9867868044732494, 'epsilon': 1.0263652565498493e-07, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 15:29:01,997] Trial 150 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9522712060141931, 'batch_size': 37, 'attention_heads': 15, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32470960238318985, 'global_pooling': 'mean', 'learning_rate': 0.011024511241794589, 'weight_decay': 5.518944434716013e-06, 'beta_0': 0.8813515191932473, 'beta_1': 0.9881183272393425, 'epsilon': 5.492145083795457e-07, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.78 GiB is free. Including non-PyTorch memory, this process has 39.78 GiB memory in use. Of the allocated memory 29.39 GiB is allocated by PyTorch, and 9.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 15:39:29,017] Trial 151 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.961720335968264, 'batch_size': 30, 'attention_heads': 14, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4757325219289564, 'global_pooling': 'mean', 'learning_rate': 0.003178795224744259, 'weight_decay': 0.00011591024043364154, 'beta_0': 0.8689748177939176, 'beta_1': 0.9893258073652562, 'epsilon': 1.724459564270837e-08, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 15:53:32,231] Trial 152 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9407281423348537, 'batch_size': 18, 'attention_heads': 11, 'hidden_dimension': 78, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5993227148872271, 'global_pooling': 'mean', 'learning_rate': 0.004915938228978711, 'weight_decay': 4.026119214215965e-05, 'beta_0': 0.8736917110077482, 'beta_1': 0.9861875742460146, 'epsilon': 3.227417783100335e-07, 'balanced_loss': True, 'epochs': 112, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 16:07:13,043] Trial 153 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9328806887970104, 'batch_size': 25, 'attention_heads': 11, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5664287753502087, 'global_pooling': 'mean', 'learning_rate': 0.002096846086517852, 'weight_decay': 3.846618208297627e-06, 'beta_0': 0.8658423980185679, 'beta_1': 0.9870841494934303, 'epsilon': 4.2779784603925847e-07, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 422.69 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 28.61 GiB is allocated by PyTorch, and 14.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 16:18:42,726] Trial 154 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.93886156845291, 'batch_size': 23, 'attention_heads': 12, 'hidden_dimension': 72, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5213587315144408, 'global_pooling': 'mean', 'learning_rate': 0.003936849979686362, 'weight_decay': 6.193290412878167e-05, 'beta_0': 0.8726788858933598, 'beta_1': 0.9858641396301352, 'epsilon': 1.9345533784116726e-07, 'balanced_loss': True, 'epochs': 66, 'early_stopping_patience': 18, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 16:33:02,013] Trial 155 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9366706095023433, 'batch_size': 33, 'attention_heads': 10, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3103701727785745, 'global_pooling': 'mean', 'learning_rate': 0.0008802632427272092, 'weight_decay': 4.956566764681746e-06, 'beta_0': 0.8770171544439425, 'beta_1': 0.9905756119219528, 'epsilon': 3.1480727753300265e-07, 'balanced_loss': True, 'epochs': 73, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 50.69 MiB is free. Including non-PyTorch memory, this process has 44.50 GiB memory in use. Of the allocated memory 32.77 GiB is allocated by PyTorch, and 10.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 16:44:31,523] Trial 156 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9078352696917492, 'batch_size': 24, 'attention_heads': 12, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5901123896020873, 'global_pooling': 'mean', 'learning_rate': 0.013376668911512062, 'weight_decay': 3.120363723526497e-05, 'beta_0': 0.8689144749748695, 'beta_1': 0.9867181935424638, 'epsilon': 2.5680815191832887e-07, 'balanced_loss': True, 'epochs': 130, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 16:58:07,811] Trial 157 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9453203650955377, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5838734419921238, 'global_pooling': 'max', 'learning_rate': 0.0063324953350154785, 'weight_decay': 2.277531668250243e-05, 'beta_0': 0.8758015008150687, 'beta_1': 0.9885300625031217, 'epsilon': 8.071494237810712e-07, 'balanced_loss': True, 'epochs': 113, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 17:11:39,516] Trial 158 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.947530612786696, 'batch_size': 36, 'attention_heads': 9, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5732791609659371, 'global_pooling': 'max', 'learning_rate': 0.0066845112977160824, 'weight_decay': 2.2604568862983126e-05, 'beta_0': 0.8835953668788381, 'beta_1': 0.9884189433737788, 'epsilon': 1.0035900577147266e-08, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 17:25:29,414] Trial 159 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9452582668808213, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5811554311315397, 'global_pooling': 'max', 'learning_rate': 0.009625730512366287, 'weight_decay': 2.0057172642607124e-05, 'beta_0': 0.8592967736056617, 'beta_1': 0.9888898041680348, 'epsilon': 2.1613822089480637e-08, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 17:39:06,910] Trial 160 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9453216241690103, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5475415729196431, 'global_pooling': 'max', 'learning_rate': 0.00922550026202551, 'weight_decay': 1.5019645574849691e-05, 'beta_0': 0.8579076572282723, 'beta_1': 0.9889774462097062, 'epsilon': 2.27991425955705e-08, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 17:52:52,165] Trial 161 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9452811131270327, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.551018907070949, 'global_pooling': 'max', 'learning_rate': 0.008085759147166253, 'weight_decay': 1.95364707859434e-05, 'beta_0': 0.8583032739849635, 'beta_1': 0.9887979155629615, 'epsilon': 2.1704613752265413e-08, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.87 GiB is free. Including non-PyTorch memory, this process has 38.68 GiB memory in use. Of the allocated memory 27.76 GiB is allocated by PyTorch, and 9.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 18:04:22,346] Trial 162 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9451011575412488, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5437766386244851, 'global_pooling': 'max', 'learning_rate': 0.009124631198512721, 'weight_decay': 1.462865089201617e-05, 'beta_0': 0.8581234258747826, 'beta_1': 0.9887099903668971, 'epsilon': 2.2099794801679448e-08, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 18:18:26,299] Trial 163 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9422915491296913, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5591602608953438, 'global_pooling': 'max', 'learning_rate': 0.00782127398194508, 'weight_decay': 1.618688790374396e-05, 'beta_0': 0.854088220513104, 'beta_1': 0.9889274409587516, 'epsilon': 3.0865712476312455e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 18:32:07,095] Trial 164 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9425055224183081, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5560017784250146, 'global_pooling': 'max', 'learning_rate': 0.007839168175502061, 'weight_decay': 1.612128647164736e-05, 'beta_0': 0.8524710852098771, 'beta_1': 0.9890912788355675, 'epsilon': 2.8743251563752537e-08, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 18:45:48,081] Trial 165 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.94261164879369, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5574782572414869, 'global_pooling': 'max', 'learning_rate': 0.008080373801390369, 'weight_decay': 1.7079560906613308e-05, 'beta_0': 0.8494210525277366, 'beta_1': 0.9888267889251015, 'epsilon': 2.825514733805698e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 18:59:25,419] Trial 166 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9463772802524834, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5497733097099632, 'global_pooling': 'max', 'learning_rate': 0.00604027321850602, 'weight_decay': 1.5449144027764145e-05, 'beta_0': 0.8537392995616268, 'beta_1': 0.989143628222065, 'epsilon': 2.0463810375357065e-08, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 19:13:04,600] Trial 167 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9463715520123579, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5508606509832721, 'global_pooling': 'max', 'learning_rate': 0.0062137663872681905, 'weight_decay': 1.0120623035254093e-05, 'beta_0': 0.8555991362454886, 'beta_1': 0.9893334109931451, 'epsilon': 1.9933048473723378e-08, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 19:26:51,743] Trial 168 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9452540983469485, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5475789223663974, 'global_pooling': 'max', 'learning_rate': 0.0060670356385075595, 'weight_decay': 1.9224165616398814e-05, 'beta_0': 0.8542005048371173, 'beta_1': 0.9893353247283365, 'epsilon': 2.0932118111337944e-08, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 19:40:24,195] Trial 169 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9459190271626503, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.534652849303388, 'global_pooling': 'max', 'learning_rate': 0.010757320847259009, 'weight_decay': 1.0106524771566258e-05, 'beta_0': 0.8526186120786319, 'beta_1': 0.9890458967437936, 'epsilon': 2.9683382200544577e-08, 'balanced_loss': True, 'epochs': 153, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 19:53:52,359] Trial 170 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9501032457247616, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5663487969368698, 'global_pooling': 'max', 'learning_rate': 0.007345907666091808, 'weight_decay': 1.2000656267446331e-05, 'beta_0': 0.8506944860186909, 'beta_1': 0.9884865333090274, 'epsilon': 2.3690590763054906e-08, 'balanced_loss': True, 'epochs': 149, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 20:07:15,475] Trial 171 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9475854577972702, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5594637771070593, 'global_pooling': 'max', 'learning_rate': 0.009256395173325877, 'weight_decay': 1.5531836225686533e-05, 'beta_0': 0.8557659895172933, 'beta_1': 0.9897557070540148, 'epsilon': 1.6366501269390233e-08, 'balanced_loss': True, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 20:20:45,656] Trial 172 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9475295911328416, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5585640109090009, 'global_pooling': 'max', 'learning_rate': 0.008864656825743483, 'weight_decay': 1.5225535086597629e-05, 'beta_0': 0.8556147314252752, 'beta_1': 0.9898071689908973, 'epsilon': 1.6162914056354716e-08, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 20:34:15,487] Trial 173 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9476198068053102, 'batch_size': 46, 'attention_heads': 13, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5615182295258354, 'global_pooling': 'max', 'learning_rate': 0.009037174643151033, 'weight_decay': 1.582487630063466e-05, 'beta_0': 0.8561259222001065, 'beta_1': 0.9893986415918329, 'epsilon': 1.541159852735027e-08, 'balanced_loss': True, 'epochs': 157, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 20:47:47,713] Trial 174 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9489254724822236, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.533320168313539, 'global_pooling': 'max', 'learning_rate': 0.009248514802025726, 'weight_decay': 2.0909214289777735e-05, 'beta_0': 0.8537690527312791, 'beta_1': 0.9898215107646245, 'epsilon': 1.9607257388854357e-08, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-03 21:01:23,144] Trial 175 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9441085964537048, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5502612122597673, 'global_pooling': 'max', 'learning_rate': 0.0072080788395178785, 'weight_decay': 1.479919633306471e-05, 'beta_0': 0.857155608536015, 'beta_1': 0.9881115402407941, 'epsilon': 2.44813638422922e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
slurmstepd: error: *** JOB 14051923 ON gpu041 CANCELLED AT 2024-12-03T21:08:05 DUE TO TIME LIMIT ***
