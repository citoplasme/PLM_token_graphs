[I 2024-12-23 05:19:53,242] Using an existing study with name 'IMDb-top_1000-GATv2-facebook-bart-large-Surrogate-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors
[I 2024-12-23 05:50:11,593] Trial 245 finished with value: 0.9515151515151515 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9835158230658628, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4180711664872537, 'global_pooling': 'mean', 'learning_rate': 4.925920562834034e-05, 'weight_decay': 0.0002311440669639107, 'beta_0': 0.8544396104404853, 'beta_1': 0.9929091412607028, 'epsilon': 1.1945043064076627e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 15, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 06:20:52,447] Trial 246 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9811950263942442, 'batch_size': 39, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40480052649801535, 'global_pooling': 'mean', 'learning_rate': 8.208379168095286e-05, 'weight_decay': 0.0004337635113391184, 'beta_0': 0.8525408236798115, 'beta_1': 0.9937124508591636, 'epsilon': 1.4166996425042432e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.61 GiB is free. Including non-PyTorch memory, this process has 41.94 GiB memory in use. Of the allocated memory 33.61 GiB is allocated by PyTorch, and 7.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 06:46:13,521] Trial 247 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9803962916653693, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3723491521089547, 'global_pooling': 'mean', 'learning_rate': 5.1947055395399235e-05, 'weight_decay': 0.0002826738994208179, 'beta_0': 0.8481084878975323, 'beta_1': 0.9933934531938312, 'epsilon': 1.4640350533185039e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 07:19:16,157] Trial 248 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9831873790673044, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3626069838099222, 'global_pooling': 'mean', 'learning_rate': 3.9530620607660656e-05, 'weight_decay': 0.0002989070463949916, 'beta_0': 0.849895112797106, 'beta_1': 0.9932179683065364, 'epsilon': 1.8187136443196698e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 33.27 GiB is allocated by PyTorch, and 9.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 07:45:03,064] Trial 249 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9795755944657274, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37011275135279204, 'global_pooling': 'mean', 'learning_rate': 5.680182813378435e-05, 'weight_decay': 0.0003419243996094718, 'beta_0': 0.849672942669845, 'beta_1': 0.9941796450478904, 'epsilon': 1.1717767174150804e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 43.00 GiB memory in use. Of the allocated memory 36.50 GiB is allocated by PyTorch, and 5.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 08:10:25,517] Trial 250 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.976646367600103, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3575422968940153, 'global_pooling': 'mean', 'learning_rate': 7.163863687203881e-05, 'weight_decay': 0.0005223941137762986, 'beta_0': 0.8447920109697206, 'beta_1': 0.9927581522388828, 'epsilon': 1.3876494242226184e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 41.93 GiB memory in use. Of the allocated memory 33.08 GiB is allocated by PyTorch, and 7.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 08:36:00,252] Trial 251 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9810707243242658, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38453774769675436, 'global_pooling': 'mean', 'learning_rate': 9.794024485377662e-05, 'weight_decay': 0.0003756212262697397, 'beta_0': 0.8559294506329291, 'beta_1': 0.9951793392538048, 'epsilon': 1.5273522120412194e-08, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 09:06:41,088] Trial 252 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9837149392853326, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3796175342655763, 'global_pooling': 'mean', 'learning_rate': 0.00011507078431551669, 'weight_decay': 0.00031761361611428455, 'beta_0': 0.8527755792101264, 'beta_1': 0.9932201161206679, 'epsilon': 1.1667686831097835e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 09:38:02,552] Trial 253 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9862571222541322, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36498722098454545, 'global_pooling': 'mean', 'learning_rate': 5.277699791447047e-05, 'weight_decay': 0.00043345727911813143, 'beta_0': 0.8472096223792235, 'beta_1': 0.9938184025817766, 'epsilon': 1.784659199947619e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 10:12:50,293] Trial 254 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9818692632181111, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3909413079987634, 'global_pooling': 'mean', 'learning_rate': 2.5700604977396847e-05, 'weight_decay': 0.0002590512000992079, 'beta_0': 0.8498024354423475, 'beta_1': 0.9958083784547728, 'epsilon': 1.0066088644858318e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 10:41:14,465] Trial 255 finished with value: 0.9454545454545454 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9787435573460914, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37174070384968194, 'global_pooling': 'mean', 'learning_rate': 0.09285939748894982, 'weight_decay': 0.00036265812815396457, 'beta_0': 0.8586739464840708, 'beta_1': 0.9946933242010684, 'epsilon': 1.3888158394281637e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 11:12:24,697] Trial 256 finished with value: 0.9636363636363636 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9835249591662812, 'batch_size': 38, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3972839089727645, 'global_pooling': 'mean', 'learning_rate': 3.435851415793875e-05, 'weight_decay': 0.0006124278222349847, 'beta_0': 0.8550939505291374, 'beta_1': 0.9925653463698089, 'epsilon': 1.2159138978200683e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 42.87 GiB memory in use. Of the allocated memory 33.54 GiB is allocated by PyTorch, and 8.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 11:38:12,240] Trial 257 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9805216231313955, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 183, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37979003809604495, 'global_pooling': 'mean', 'learning_rate': 7.743766836447856e-05, 'weight_decay': 0.0005090414668663664, 'beta_0': 0.8527887513563993, 'beta_1': 0.9933199622317648, 'epsilon': 1.6189826108524524e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.63 GiB. GPU 0 has a total capacity of 44.56 GiB of which 434.69 MiB is free. Including non-PyTorch memory, this process has 44.13 GiB memory in use. Of the allocated memory 32.47 GiB is allocated by PyTorch, and 10.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 12:06:25,472] Trial 258 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9862345395064479, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 205, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3872611551163514, 'global_pooling': 'max', 'learning_rate': 4.327319500521709e-05, 'weight_decay': 0.00046451645410920195, 'beta_0': 0.8496181545987488, 'beta_1': 0.99410874071935, 'epsilon': 1.1855644502536193e-08, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.35 GiB is free. Including non-PyTorch memory, this process has 42.20 GiB memory in use. Of the allocated memory 35.94 GiB is allocated by PyTorch, and 5.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 12:34:36,790] Trial 259 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.982546542413164, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 250, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3703676326054534, 'global_pooling': 'mean', 'learning_rate': 6.461841086751095e-05, 'weight_decay': 0.0003947968413196765, 'beta_0': 0.8458956845474634, 'beta_1': 0.9931101010580617, 'epsilon': 1.8708998410662482e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.48 GiB is free. Including non-PyTorch memory, this process has 42.08 GiB memory in use. Of the allocated memory 33.58 GiB is allocated by PyTorch, and 7.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 13:00:13,118] Trial 260 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9772181235049829, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35695697236076723, 'global_pooling': 'mean', 'learning_rate': 0.00013153788886768533, 'weight_decay': 0.00030533853520643185, 'beta_0': 0.8414687022513504, 'beta_1': 0.9937422718080224, 'epsilon': 1.0098209088679531e-08, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 13:30:30,765] Trial 261 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9841613397272163, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36326389083483795, 'global_pooling': 'mean', 'learning_rate': 9.30958104758367e-05, 'weight_decay': 0.00044280304184620903, 'beta_0': 0.8515109306018188, 'beta_1': 0.9926922151959511, 'epsilon': 1.4231721664962635e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.67 GiB is free. Including non-PyTorch memory, this process has 42.88 GiB memory in use. Of the allocated memory 34.33 GiB is allocated by PyTorch, and 7.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 13:56:05,680] Trial 262 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9794922585960217, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3792173625592649, 'global_pooling': 'mean', 'learning_rate': 4.995164325043024e-05, 'weight_decay': 0.0005471929272987514, 'beta_0': 0.8571140600903965, 'beta_1': 0.9944650193889696, 'epsilon': 2.0444725077060348e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 14:24:33,357] Trial 263 finished with value: 0.9636363636363636 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9877570727514198, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39668745413482803, 'global_pooling': 'mean', 'learning_rate': 6.521921167140036e-05, 'weight_decay': 0.00033409618536938475, 'beta_0': 0.8476045314736781, 'beta_1': 0.9934827852444993, 'epsilon': 1.3370112295867392e-08, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 14:56:45,419] Trial 264 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9816338203845246, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38867787722403047, 'global_pooling': 'mean', 'learning_rate': 7.98483737520968e-05, 'weight_decay': 0.0006998779172654459, 'beta_0': 0.8549028674887489, 'beta_1': 0.9922183932906183, 'epsilon': 1.6074242835029087e-08, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 15:29:41,106] Trial 265 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9854971030666538, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 51, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40322860818192785, 'global_pooling': 'mean', 'learning_rate': 3.836611571638176e-05, 'weight_decay': 0.00020095957790326657, 'beta_0': 0.8501161614776297, 'beta_1': 0.9929196658286736, 'epsilon': 1.1986934945697337e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 41.94 GiB memory in use. Of the allocated memory 33.42 GiB is allocated by PyTorch, and 7.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 15:55:12,907] Trial 266 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9806075188903309, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 220, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37145543227211186, 'global_pooling': 'mean', 'learning_rate': 5.539296071454689e-05, 'weight_decay': 0.00037125943547398324, 'beta_0': 0.8525315495195654, 'beta_1': 0.9942368174547926, 'epsilon': 1.0002372885391829e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 288.69 MiB is free. Including non-PyTorch memory, this process has 44.27 GiB memory in use. Of the allocated memory 36.58 GiB is allocated by PyTorch, and 6.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 16:23:15,258] Trial 267 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9831200570414693, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3831987393267471, 'global_pooling': 'mean', 'learning_rate': 0.00011182126495669844, 'weight_decay': 0.0002554369637826114, 'beta_0': 0.8437534157222839, 'beta_1': 0.9948514797859088, 'epsilon': 1.5424517245810247e-08, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 42.89 GiB memory in use. Of the allocated memory 35.01 GiB is allocated by PyTorch, and 6.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 16:48:47,602] Trial 268 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9785806303220005, 'batch_size': 32, 'attention_heads': 4, 'hidden_dimension': 38, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3661327668529188, 'global_pooling': 'mean', 'learning_rate': 2.9528904303319776e-05, 'weight_decay': 0.0004369002213354439, 'beta_0': 0.8484615088388635, 'beta_1': 0.9938988444258924, 'epsilon': 1.3132305624543355e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 17, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 17:22:13,349] Trial 269 finished with value: 0.9575757575757575 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9847356722275865, 'batch_size': 30, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39280341532695945, 'global_pooling': 'mean', 'learning_rate': 4.7118907702207104e-05, 'weight_decay': 0.000492781083037843, 'beta_0': 0.8549054285798463, 'beta_1': 0.993273299502418, 'epsilon': 4.8719043206201657e-05, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 6}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 17:53:49,884] Trial 270 finished with value: 0.9696969696969697 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9815998154709067, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3754813505126193, 'global_pooling': 'mean', 'learning_rate': 8.808890635273994e-05, 'weight_decay': 0.0006014801773818152, 'beta_0': 0.8510773540890944, 'beta_1': 0.9963645054109649, 'epsilon': 2.4275748474147773e-07, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 898.69 MiB is free. Including non-PyTorch memory, this process has 43.68 GiB memory in use. Of the allocated memory 35.24 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 18:19:23,958] Trial 271 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9750869723872303, 'batch_size': 34, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35696917353910496, 'global_pooling': 'mean', 'learning_rate': 7.052982728766509e-05, 'weight_decay': 0.00039078809038328613, 'beta_0': 0.8583943653811426, 'beta_1': 0.9925628159260514, 'epsilon': 1.9862585518222583e-08, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 18:50:58,163] Trial 272 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.987841714091215, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.38110347746979867, 'global_pooling': 'mean', 'learning_rate': 6.040376610168041e-05, 'weight_decay': 0.0002923805827458577, 'beta_0': 0.8459551926151183, 'beta_1': 0.9935450734883695, 'epsilon': 1.1710240915007198e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 19:21:59,533] Trial 273 finished with value: 0.9636363636363636 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9835426820938431, 'batch_size': 31, 'attention_heads': 4, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.407460694988618, 'global_pooling': 'mean', 'learning_rate': 9.472371172303325e-05, 'weight_decay': 0.0005312665539400094, 'beta_0': 0.8531100660927253, 'beta_1': 0.9930013393238479, 'epsilon': 1.6045987133884882e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 33.22 GiB is allocated by PyTorch, and 9.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 19:47:55,111] Trial 274 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9796438545360533, 'batch_size': 33, 'attention_heads': 4, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36920763900235315, 'global_pooling': 'mean', 'learning_rate': 0.0001317238897019424, 'weight_decay': 0.0003319670247844836, 'beta_0': 0.8563792983303515, 'beta_1': 0.9939017157675333, 'epsilon': 1.3551844959761731e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 177 with value: 0.9757575757575757.
[I 2024-12-23 20:20:20,227] Trial 275 finished with value: 0.9757575757575757 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9859766188582519, 'batch_size': 40, 'attention_heads': 4, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3867325538677943, 'global_pooling': 'mean', 'learning_rate': 3.8219462335253694e-05, 'weight_decay': 0.00042898607919769334, 'beta_0': 0.8490223805015451, 'beta_1': 0.994371125581801, 'epsilon': 1.1588263080451891e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 16, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 458.69 MiB is free. Including non-PyTorch memory, this process has 44.11 GiB memory in use. Of the allocated memory 34.79 GiB is allocated by PyTorch, and 8.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 20:48:26,540] Trial 276 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9819984612565623, 'batch_size': 30, 'attention_heads': 12, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36217961352440325, 'global_pooling': 'mean', 'learning_rate': 4.779247586955706e-05, 'weight_decay': 0.0003636450514916147, 'beta_0': 0.8517617979502821, 'beta_1': 0.9923067853730041, 'epsilon': 1.7490161021195476e-06, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 20, 'plateau_patience': 25, 'plateau_divider': 8}. Best is trial 177 with value: 0.9757575757575757.
CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 44.56 GiB of which 52.69 MiB is free. Including non-PyTorch memory, this process has 44.50 GiB memory in use. Of the allocated memory 35.08 GiB is allocated by PyTorch, and 8.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-23 21:15:26,771] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.976956003887213, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.39765953676570753, 'global_pooling': 'mean', 'learning_rate': 7.60366837935155e-05, 'weight_decay': 0.00023273923801525872, 'beta_0': 0.8541516431343621, 'beta_1': 0.9950874411984812, 'epsilon': 1.910077646364566e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 15, 'plateau_patience': 24, 'plateau_divider': 10}. Best is trial 177 with value: 0.9757575757575757.
slurmstepd: error: *** JOB 14142049 ON gpu039 CANCELLED AT 2024-12-23T21:19:23 DUE TO TIME LIMIT ***
