[I 2024-12-05 04:51:10,129] Using an existing study with name 'IMDb-top_1000-GATv2-xlnet-xlnet-base-cased-Grouped-No_Aggregation' instead of creating a new one.
[I 2024-12-05 05:07:45,585] Trial 277 finished with value: 0.8121212121212121 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9416966793099633, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5393950051559443, 'global_pooling': 'max', 'learning_rate': 1.1290337641647065e-05, 'weight_decay': 0.00024337291132639255, 'beta_0': 0.8502135704306347, 'beta_1': 0.9877137357587943, 'epsilon': 1.0911687021836364e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 05:17:22,269] Trial 278 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.968946764746053, 'batch_size': 41, 'attention_heads': 12, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5585317100475433, 'global_pooling': 'max', 'learning_rate': 0.01438727035893165, 'weight_decay': 1.1378553829301439e-05, 'beta_0': 0.8474942375372491, 'beta_1': 0.9892127966604558, 'epsilon': 2.996894878961025e-06, 'balanced_loss': True, 'epochs': 93, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 05:27:37,485] Trial 279 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9437480846614678, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.568578976337989, 'global_pooling': 'max', 'learning_rate': 0.003428307906144288, 'weight_decay': 1.4787825285581366e-05, 'beta_0': 0.8550798955044229, 'beta_1': 0.9885367599598498, 'epsilon': 3.787686845216032e-08, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 05:37:35,849] Trial 280 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9512880115321186, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5491829120331433, 'global_pooling': 'max', 'learning_rate': 0.01128508306982834, 'weight_decay': 0.00046559520649238755, 'beta_0': 0.8510838945292332, 'beta_1': 0.9950263752041384, 'epsilon': 8.834831635757252e-06, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.73 GiB is free. Including non-PyTorch memory, this process has 40.82 GiB memory in use. Of the allocated memory 27.40 GiB is allocated by PyTorch, and 12.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 05:45:55,981] Trial 281 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9475474145289471, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5631465235507729, 'global_pooling': 'max', 'learning_rate': 0.0052275596355444804, 'weight_decay': 2.4710781669166872e-05, 'beta_0': 0.8721807437317257, 'beta_1': 0.9897711136192143, 'epsilon': 1.3141044276145465e-08, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 05:59:23,073] Trial 282 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9362137448240891, 'batch_size': 46, 'attention_heads': 13, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5546321224532713, 'global_pooling': 'max', 'learning_rate': 0.007399134903147308, 'weight_decay': 0.0003718672349363883, 'beta_0': 0.8488277107128395, 'beta_1': 0.9880702234841324, 'epsilon': 1.5693545012442625e-08, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.39 GiB is free. Including non-PyTorch memory, this process has 40.17 GiB memory in use. Of the allocated memory 31.67 GiB is allocated by PyTorch, and 7.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 06:07:10,759] Trial 283 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9397853035186918, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5450402141475239, 'global_pooling': 'max', 'learning_rate': 0.009263390632994995, 'weight_decay': 0.0007517125899821119, 'beta_0': 0.8101761635869574, 'beta_1': 0.9872829601051041, 'epsilon': 2.193907340427946e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.86 GiB is free. Including non-PyTorch memory, this process has 40.69 GiB memory in use. Of the allocated memory 27.73 GiB is allocated by PyTorch, and 11.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 06:15:30,976] Trial 284 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9454009733861183, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.561183577269859, 'global_pooling': 'max', 'learning_rate': 2.1454709998875967e-05, 'weight_decay': 1.7779711704864652e-05, 'beta_0': 0.8534498100541559, 'beta_1': 0.9890292927227187, 'epsilon': 1.4382629285067428e-06, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 06:25:15,877] Trial 285 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9423429899699446, 'batch_size': 38, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5394927180893752, 'global_pooling': 'max', 'learning_rate': 0.006369608746845209, 'weight_decay': 2.21162215265113e-05, 'beta_0': 0.8456581537927984, 'beta_1': 0.9885176677712205, 'epsilon': 1.7432971851611e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 06:35:27,562] Trial 286 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9406941838682235, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5467552259723882, 'global_pooling': 'max', 'learning_rate': 0.004618885583151173, 'weight_decay': 2.0042889725516294e-05, 'beta_0': 0.8569347189275193, 'beta_1': 0.9894423493693792, 'epsilon': 2.7753600520829723e-08, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.93 GiB is free. Including non-PyTorch memory, this process has 38.63 GiB memory in use. Of the allocated memory 27.88 GiB is allocated by PyTorch, and 9.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 06:43:48,693] Trial 287 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9439821851094379, 'batch_size': 41, 'attention_heads': 12, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5826900626203835, 'global_pooling': 'max', 'learning_rate': 0.002691984747932379, 'weight_decay': 1.2821268343255804e-05, 'beta_0': 0.8515061084528986, 'beta_1': 0.9876850090405561, 'epsilon': 3.6140504421591975e-08, 'balanced_loss': True, 'epochs': 64, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 06:54:21,379] Trial 288 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9486808293359728, 'batch_size': 36, 'attention_heads': 13, 'hidden_dimension': 79, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5668941244895447, 'global_pooling': 'max', 'learning_rate': 0.003787984347723448, 'weight_decay': 1.0474426141394909e-05, 'beta_0': 0.880617704829638, 'beta_1': 0.9901683624688161, 'epsilon': 1.5835908858375975e-07, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.91 GiB is free. Including non-PyTorch memory, this process has 39.65 GiB memory in use. Of the allocated memory 28.83 GiB is allocated by PyTorch, and 9.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 07:02:41,731] Trial 289 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9371030531990141, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5551022753482249, 'global_pooling': 'max', 'learning_rate': 0.010042470670725771, 'weight_decay': 0.0006152144965539288, 'beta_0': 0.8607189829441684, 'beta_1': 0.988713957500306, 'epsilon': 2.1893229337011017e-08, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 07:12:56,612] Trial 290 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9465124621837607, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5750617791796546, 'global_pooling': 'max', 'learning_rate': 0.007484138023572105, 'weight_decay': 1.5426986752510282e-05, 'beta_0': 0.8263166852336423, 'beta_1': 0.9883968775534796, 'epsilon': 1.767027845526157e-08, 'balanced_loss': True, 'epochs': 107, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.92 GiB is free. Including non-PyTorch memory, this process has 38.63 GiB memory in use. Of the allocated memory 28.07 GiB is allocated by PyTorch, and 9.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 07:21:16,204] Trial 291 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9427826327981543, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 41, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5322191261758888, 'global_pooling': 'max', 'learning_rate': 0.013600955036876581, 'weight_decay': 2.7402115908477375e-05, 'beta_0': 0.8783043476608738, 'beta_1': 0.9922928148810352, 'epsilon': 2.837704776242754e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.24 GiB is free. Including non-PyTorch memory, this process has 39.31 GiB memory in use. Of the allocated memory 22.53 GiB is allocated by PyTorch, and 15.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 07:29:35,500] Trial 292 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9393670117192228, 'batch_size': 37, 'attention_heads': 13, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4866188812941073, 'global_pooling': 'max', 'learning_rate': 0.005767718446811789, 'weight_decay': 0.0004317298032295941, 'beta_0': 0.8465026234259448, 'beta_1': 0.995618505052539, 'epsilon': 1.4256864095480242e-08, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 07:39:42,604] Trial 293 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9445416512027076, 'batch_size': 42, 'attention_heads': 12, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5537500979875313, 'global_pooling': 'max', 'learning_rate': 0.008264338444838037, 'weight_decay': 1.728670198270802e-05, 'beta_0': 0.8482234681814336, 'beta_1': 0.9891737610334117, 'epsilon': 4.4367202690504694e-08, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 07:49:06,120] Trial 294 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9964523138037811, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 72, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5595359664179568, 'global_pooling': 'max', 'learning_rate': 0.010903732641635988, 'weight_decay': 0.0002844869969806416, 'beta_0': 0.842749110464494, 'beta_1': 0.989819525016015, 'epsilon': 1.8169382497646379e-06, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 07:58:31,226] Trial 295 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9991705009048847, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 73, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5615607511381062, 'global_pooling': 'max', 'learning_rate': 0.00011154341843528441, 'weight_decay': 0.00026726713716653043, 'beta_0': 0.840627011108456, 'beta_1': 0.9898413074792458, 'epsilon': 1.933640192722286e-06, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 08:08:32,831] Trial 296 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9408310612293148, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5693370641875543, 'global_pooling': 'max', 'learning_rate': 0.010748000411927399, 'weight_decay': 0.0003149710274592582, 'beta_0': 0.8443987364832081, 'beta_1': 0.9905606535302611, 'epsilon': 1.5890197457508042e-06, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 08:18:01,251] Trial 297 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9919899184995444, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5468280561991388, 'global_pooling': 'max', 'learning_rate': 0.009147005488421418, 'weight_decay': 0.00021941335223767176, 'beta_0': 0.8418897001635361, 'beta_1': 0.9897186965775792, 'epsilon': 2.0762968815884472e-07, 'balanced_loss': False, 'epochs': 155, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 768.69 MiB is free. Including non-PyTorch memory, this process has 43.80 GiB memory in use. Of the allocated memory 21.65 GiB is allocated by PyTorch, and 21.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 08:26:22,006] Trial 298 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9909142658450822, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 83, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5456781802787637, 'global_pooling': 'max', 'learning_rate': 0.011750434386067024, 'weight_decay': 0.0002878831588099198, 'beta_0': 0.8425280579403359, 'beta_1': 0.9897109998823014, 'epsilon': 1.3513245323434978e-06, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.46 GiB is free. Including non-PyTorch memory, this process has 40.10 GiB memory in use. Of the allocated memory 20.64 GiB is allocated by PyTorch, and 18.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 08:34:42,666] Trial 299 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9967443064545856, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 76, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5388980555531353, 'global_pooling': 'max', 'learning_rate': 0.008963219908002902, 'weight_decay': 0.00034774046402912136, 'beta_0': 0.84976385840531, 'beta_1': 0.9895682323619622, 'epsilon': 1.0203621478185134e-06, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 42.17 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 19.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 08:43:03,771] Trial 300 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9930289911752954, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5506464046448504, 'global_pooling': 'max', 'learning_rate': 0.004342894507169773, 'weight_decay': 0.00020690354668017005, 'beta_0': 0.841765642516884, 'beta_1': 0.9899180578847183, 'epsilon': 2.2334428260444883e-07, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 08:52:27,343] Trial 301 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9960613853509062, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 69, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5574013575180785, 'global_pooling': 'max', 'learning_rate': 0.006495599363254039, 'weight_decay': 0.00023579080379742276, 'beta_0': 0.8392441699888684, 'beta_1': 0.9911679605364884, 'epsilon': 2.287936783759441e-06, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:01:59,887] Trial 302 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9892069845766212, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 73, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5450631220805721, 'global_pooling': 'max', 'learning_rate': 0.00979922273879647, 'weight_decay': 0.00029498764890406115, 'beta_0': 0.8439210206046782, 'beta_1': 0.9902491353000623, 'epsilon': 1.2279664288626922e-06, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:11:16,836] Trial 303 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9998587602217824, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5871516696587281, 'global_pooling': 'max', 'learning_rate': 0.013038260632941275, 'weight_decay': 0.0005293663797018189, 'beta_0': 0.843463897372373, 'beta_1': 0.989317434936327, 'epsilon': 5.848577129157094e-06, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:20:49,608] Trial 304 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9920115666722457, 'batch_size': 43, 'attention_heads': 14, 'hidden_dimension': 78, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5601489777330424, 'global_pooling': 'max', 'learning_rate': 0.007746497660070273, 'weight_decay': 0.0002108113500792407, 'beta_0': 0.8460571948471957, 'beta_1': 0.9906945594406539, 'epsilon': 1.0392804575319554e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:30:20,607] Trial 305 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9944326642534298, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 88, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5634641399071791, 'global_pooling': 'max', 'learning_rate': 0.007905064283014391, 'weight_decay': 0.00023269159066612713, 'beta_0': 0.8459808607599769, 'beta_1': 0.9908846838713908, 'epsilon': 1.2668091912488747e-08, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:39:43,897] Trial 306 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9973457527186701, 'batch_size': 45, 'attention_heads': 14, 'hidden_dimension': 80, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5568099084251009, 'global_pooling': 'max', 'learning_rate': 0.010687607899356777, 'weight_decay': 0.0008750406816886869, 'beta_0': 0.8408837843992801, 'beta_1': 0.9905643970442471, 'epsilon': 1.078609907951369e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.76 GiB is free. Including non-PyTorch memory, this process has 38.80 GiB memory in use. Of the allocated memory 14.80 GiB is allocated by PyTorch, and 22.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 09:48:06,133] Trial 307 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9957827311104043, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 72, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5526209096253544, 'global_pooling': 'max', 'learning_rate': 0.007158863831570888, 'weight_decay': 0.0004042244685371958, 'beta_0': 0.8474757306866411, 'beta_1': 0.9903155481998439, 'epsilon': 1.4883507365744627e-08, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 09:57:36,075] Trial 308 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9924904802906872, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5696442916786257, 'global_pooling': 'max', 'learning_rate': 0.008692498814599117, 'weight_decay': 1.3328145861475135e-05, 'beta_0': 0.837229553986846, 'beta_1': 0.9911685691973409, 'epsilon': 1.0520437900811519e-08, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.46 GiB is free. Including non-PyTorch memory, this process has 41.09 GiB memory in use. Of the allocated memory 26.20 GiB is allocated by PyTorch, and 13.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 10:05:26,706] Trial 309 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9925186233976016, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5625303550005322, 'global_pooling': 'max', 'learning_rate': 0.005490787438968058, 'weight_decay': 1.5403963068674737e-05, 'beta_0': 0.8426893595142722, 'beta_1': 0.9889820767032727, 'epsilon': 1.2578379551842313e-08, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 10:14:55,208] Trial 310 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9945072631597787, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 75, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5498344540343129, 'global_pooling': 'max', 'learning_rate': 0.006495990314683074, 'weight_decay': 0.00017722754015615536, 'beta_0': 0.8522517025632722, 'beta_1': 0.990003244222547, 'epsilon': 1.8932042984523673e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 10:26:10,155] Trial 311 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9459294770779457, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5341257107389992, 'global_pooling': 'sum', 'learning_rate': 0.009302568353445576, 'weight_decay': 9.084326144088434e-06, 'beta_0': 0.8459861380680211, 'beta_1': 0.9895640624442621, 'epsilon': 1.676130323062185e-06, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.09 GiB is free. Including non-PyTorch memory, this process has 39.46 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 16.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 10:34:31,257] Trial 312 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9889915469625098, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 78, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5782747585672198, 'global_pooling': 'max', 'learning_rate': 0.012037921386682342, 'weight_decay': 0.00020386416840604155, 'beta_0': 0.849374470883332, 'beta_1': 0.9889470218600266, 'epsilon': 1.033757613478439e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 10:44:04,743] Trial 313 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9874219916884797, 'batch_size': 46, 'attention_heads': 12, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5589372400542756, 'global_pooling': 'max', 'learning_rate': 0.015656143130775, 'weight_decay': 0.000461302264122545, 'beta_0': 0.8539688197706865, 'beta_1': 0.9899792574199493, 'epsilon': 2.8119800899999233e-06, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 10:53:24,228] Trial 314 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9952178361505325, 'batch_size': 50, 'attention_heads': 6, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5566075989508459, 'global_pooling': 'max', 'learning_rate': 0.016471602445348506, 'weight_decay': 0.000596752737139706, 'beta_0': 0.8552922182550783, 'beta_1': 0.9894223267311616, 'epsilon': 1.6136104212683048e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.71 GiB is free. Including non-PyTorch memory, this process has 38.84 GiB memory in use. Of the allocated memory 22.36 GiB is allocated by PyTorch, and 15.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 11:01:45,860] Trial 315 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9854950391346718, 'batch_size': 47, 'attention_heads': 12, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5430197590609436, 'global_pooling': 'max', 'learning_rate': 0.012391068320852789, 'weight_decay': 0.00046958878401087153, 'beta_0': 0.853302380660162, 'beta_1': 0.9899152578978004, 'epsilon': 2.642391197227012e-06, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 11:11:13,824] Trial 316 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9879096819107444, 'batch_size': 45, 'attention_heads': 12, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.44612161086557955, 'global_pooling': 'max', 'learning_rate': 0.014619701956697482, 'weight_decay': 0.0003774302799331151, 'beta_0': 0.8502459576730319, 'beta_1': 0.9892729523033083, 'epsilon': 1.4531256749124348e-06, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 15, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 11:20:42,520] Trial 317 finished with value: 0.8909090909090909 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9916322568016506, 'batch_size': 40, 'attention_heads': 12, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43324948096250954, 'global_pooling': 'max', 'learning_rate': 0.018267161200684563, 'weight_decay': 0.0005212663486551599, 'beta_0': 0.8524886802286643, 'beta_1': 0.990014608393352, 'epsilon': 2.0418196174183953e-06, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 11:30:35,572] Trial 318 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9738505041744892, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5599085150053823, 'global_pooling': 'max', 'learning_rate': 0.014256818878204673, 'weight_decay': 0.0004584183244503909, 'beta_0': 0.8357317700480422, 'beta_1': 0.9896602782307814, 'epsilon': 2.3816830551000232e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 11:40:11,253] Trial 319 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9878898897430353, 'batch_size': 49, 'attention_heads': 13, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5651122900426855, 'global_pooling': 'max', 'learning_rate': 0.010398045467917477, 'weight_decay': 1.169266673675532e-05, 'beta_0': 0.8480274204662893, 'beta_1': 0.9887566216271272, 'epsilon': 3.973526551481666e-06, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.38 GiB is free. Including non-PyTorch memory, this process has 42.17 GiB memory in use. Of the allocated memory 21.28 GiB is allocated by PyTorch, and 19.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 11:48:32,527] Trial 320 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9930699806474632, 'batch_size': 46, 'attention_heads': 13, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5504555528014593, 'global_pooling': 'max', 'learning_rate': 0.022375431590418604, 'weight_decay': 2.024131076311858e-05, 'beta_0': 0.8447096856155051, 'beta_1': 0.9881532512744229, 'epsilon': 1.9951314968680367e-08, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 11:58:09,476] Trial 321 finished with value: 0.9454545454545454 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9899708954223768, 'batch_size': 44, 'attention_heads': 10, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5713984259091672, 'global_pooling': 'max', 'learning_rate': 0.007935062620994728, 'weight_decay': 1.800941379918676e-05, 'beta_0': 0.855410631922794, 'beta_1': 0.9904471838514571, 'epsilon': 7.541856746372097e-06, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:07:55,045] Trial 322 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9765302251564573, 'batch_size': 44, 'attention_heads': 10, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5727366557626878, 'global_pooling': 'max', 'learning_rate': 0.007920535764089891, 'weight_decay': 1.8555010059245152e-05, 'beta_0': 0.8577553728450373, 'beta_1': 0.9904742646510074, 'epsilon': 1.9731839285629115e-05, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:18:09,585] Trial 323 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9496890465485434, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5813476083712024, 'global_pooling': 'max', 'learning_rate': 0.006744874974624768, 'weight_decay': 1.4118981777240244e-05, 'beta_0': 0.85505910789536, 'beta_1': 0.989095202931911, 'epsilon': 9.438355681192183e-06, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:29:31,047] Trial 324 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9418521066051233, 'batch_size': 41, 'attention_heads': 10, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5688562794725793, 'global_pooling': 'sum', 'learning_rate': 0.005017056849685021, 'weight_decay': 2.3180118734539644e-05, 'beta_0': 0.8508631490220456, 'beta_1': 0.9886169295074982, 'epsilon': 1.5506318079761574e-08, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:39:53,505] Trial 325 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9383125057955354, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43382207828703045, 'global_pooling': 'max', 'learning_rate': 0.007955300258446799, 'weight_decay': 1.613247213076167e-05, 'beta_0': 0.8474698973984057, 'beta_1': 0.9909014886019811, 'epsilon': 1.4521850533705491e-05, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:50:14,691] Trial 326 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9974221129591573, 'batch_size': 44, 'attention_heads': 10, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5743358689012156, 'global_pooling': 'max', 'learning_rate': 0.0003305275134505053, 'weight_decay': 2.1904160234985988e-05, 'beta_0': 0.8346123215956277, 'beta_1': 0.9893704045777553, 'epsilon': 1.3112879023630336e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 12:59:43,447] Trial 327 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9916068673290129, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.565973308647, 'global_pooling': 'max', 'learning_rate': 0.009502598113409236, 'weight_decay': 1.7539617784291178e-05, 'beta_0': 0.8437416265122587, 'beta_1': 0.9883553477405157, 'epsilon': 2.456401370190592e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 13:09:17,324] Trial 328 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.990344226008675, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5404061818288949, 'global_pooling': 'max', 'learning_rate': 0.005902258717160886, 'weight_decay': 1.2223073981665464e-05, 'beta_0': 0.8558917232797693, 'beta_1': 0.9879363555551472, 'epsilon': 3.2730811559465467e-08, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 13:19:24,718] Trial 329 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9472654744721207, 'batch_size': 41, 'attention_heads': 9, 'hidden_dimension': 81, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5255110009019898, 'global_pooling': 'max', 'learning_rate': 0.007928329469588486, 'weight_decay': 2.0166311400737278e-05, 'beta_0': 0.8593004953703197, 'beta_1': 0.9889487637530686, 'epsilon': 7.514544245418022e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 13:29:16,330] Trial 330 finished with value: 0.8909090909090909 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9939402820856829, 'batch_size': 42, 'attention_heads': 10, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5528396445303738, 'global_pooling': 'max', 'learning_rate': 0.0001637998428028091, 'weight_decay': 1.4513129286343409e-05, 'beta_0': 0.851901272966981, 'beta_1': 0.991736750854167, 'epsilon': 2.915425242160998e-05, 'balanced_loss': True, 'epochs': 90, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 13:39:19,050] Trial 331 finished with value: 0.8787878787878788 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.944019272196258, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.594280859390397, 'global_pooling': 'sum', 'learning_rate': 0.01002753216632553, 'weight_decay': 2.6257491763485333e-05, 'beta_0': 0.849421123891579, 'beta_1': 0.9904557559668222, 'epsilon': 1.883125563710622e-08, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 12, 'plateau_patience': 24, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.14 GiB is free. Including non-PyTorch memory, this process has 39.42 GiB memory in use. Of the allocated memory 28.34 GiB is allocated by PyTorch, and 9.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 13:49:26,443] Trial 332 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9406174535102214, 'batch_size': 44, 'attention_heads': 8, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5451071460994655, 'global_pooling': 'max', 'learning_rate': 0.006668570882230746, 'weight_decay': 1.059438849284554e-05, 'beta_0': 0.8460039440390699, 'beta_1': 0.9869461907820397, 'epsilon': 1.0022738710110654e-08, 'balanced_loss': True, 'epochs': 98, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:00:36,830] Trial 333 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9454372618852123, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5602205781038977, 'global_pooling': 'max', 'learning_rate': 0.005188053561216501, 'weight_decay': 1.8095274434099247e-05, 'beta_0': 0.8406240109014501, 'beta_1': 0.9896141407090842, 'epsilon': 2.7772485735628862e-08, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:11:03,478] Trial 334 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9426007461796039, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5686998752215082, 'global_pooling': 'max', 'learning_rate': 0.007621923888082768, 'weight_decay': 0.0006650608001526566, 'beta_0': 0.8528856759009971, 'beta_1': 0.9886828310135543, 'epsilon': 2.141601642995956e-08, 'balanced_loss': True, 'epochs': 152, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:20:53,326] Trial 335 finished with value: 0.9030303030303031 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9529934226330995, 'batch_size': 42, 'attention_heads': 14, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5781835899902623, 'global_pooling': 'max', 'learning_rate': 0.010735422556853148, 'weight_decay': 0.0002642839671386894, 'beta_0': 0.8491378361648156, 'beta_1': 0.9902797952323976, 'epsilon': 5.5788025199234465e-06, 'balanced_loss': True, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:31:12,689] Trial 336 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9390473443535367, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4542226980583064, 'global_pooling': 'max', 'learning_rate': 0.009185120145941784, 'weight_decay': 1.5834405506706506e-05, 'beta_0': 0.8567662679268844, 'beta_1': 0.9882255500757451, 'epsilon': 1.2200553834898048e-05, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:41:37,724] Trial 337 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9478148826496153, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5484493172206211, 'global_pooling': 'max', 'learning_rate': 0.00430950606169968, 'weight_decay': 0.0003388176952495872, 'beta_0': 0.8506129119982733, 'beta_1': 0.9891113325506823, 'epsilon': 4.472662132799788e-06, 'balanced_loss': True, 'epochs': 155, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 14:51:35,873] Trial 338 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9436880993955303, 'batch_size': 39, 'attention_heads': 11, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5103802196444367, 'global_pooling': 'max', 'learning_rate': 0.0056806314098154665, 'weight_decay': 2.8878062339178897e-05, 'beta_0': 0.8467892508120373, 'beta_1': 0.9873866002196487, 'epsilon': 7.284528559470747e-08, 'balanced_loss': True, 'epochs': 165, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 3.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.51 GiB is free. Including non-PyTorch memory, this process has 42.04 GiB memory in use. Of the allocated memory 26.95 GiB is allocated by PyTorch, and 13.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 15:00:49,462] Trial 339 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9412790671270445, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5363058396774627, 'global_pooling': 'sum', 'learning_rate': 0.011778740763059637, 'weight_decay': 2.4013604205604384e-05, 'beta_0': 0.8543604595963528, 'beta_1': 0.9897361553931382, 'epsilon': 1.5726063102117324e-08, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 15:11:00,645] Trial 340 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9455216865841761, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5548713165381893, 'global_pooling': 'max', 'learning_rate': 0.006980201802527978, 'weight_decay': 1.2949298760355613e-05, 'beta_0': 0.8430448559531725, 'beta_1': 0.9887053425420829, 'epsilon': 1.2929519564793787e-08, 'balanced_loss': True, 'epochs': 102, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 502.69 MiB is free. Including non-PyTorch memory, this process has 44.06 GiB memory in use. Of the allocated memory 28.08 GiB is allocated by PyTorch, and 14.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 15:19:22,656] Trial 341 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.942776703878526, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 42, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5869002235430432, 'global_pooling': 'max', 'learning_rate': 0.008266401267595837, 'weight_decay': 1.9696926069024917e-05, 'beta_0': 0.8479832786787903, 'beta_1': 0.9893731829522209, 'epsilon': 1.7531917359058324e-05, 'balanced_loss': True, 'epochs': 160, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 15:29:48,945] Trial 342 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9372278473741782, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 75, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5655705982340262, 'global_pooling': 'max', 'learning_rate': 0.006177109949242202, 'weight_decay': 1.6817349343182856e-05, 'beta_0': 0.8521166982159268, 'beta_1': 0.9877536883625606, 'epsilon': 3.488966931186873e-08, 'balanced_loss': True, 'epochs': 93, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 15:39:55,477] Trial 343 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9399102584398515, 'batch_size': 44, 'attention_heads': 14, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5581496962924906, 'global_pooling': 'max', 'learning_rate': 0.008667501214047206, 'weight_decay': 2.286451312805857e-05, 'beta_0': 0.8446444982029545, 'beta_1': 0.9908121525076491, 'epsilon': 1.8007339478640087e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 15:50:23,260] Trial 344 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9504946837107278, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5732519463127339, 'global_pooling': 'max', 'learning_rate': 0.004969972137611068, 'weight_decay': 1.5144307872077734e-05, 'beta_0': 0.8507002792809144, 'beta_1': 0.9901526429221075, 'epsilon': 2.274167959327536e-08, 'balanced_loss': True, 'epochs': 105, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 16:00:31,863] Trial 345 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.947949377523066, 'batch_size': 43, 'attention_heads': 14, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5503573487950654, 'global_pooling': 'max', 'learning_rate': 0.004080957833394318, 'weight_decay': 1.845830500377384e-05, 'beta_0': 0.84635091729334, 'beta_1': 0.9883670457876114, 'epsilon': 2.7686622189059815e-08, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.63 GiB is free. Including non-PyTorch memory, this process has 41.93 GiB memory in use. Of the allocated memory 27.86 GiB is allocated by PyTorch, and 12.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 16:08:53,675] Trial 346 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9442460024392517, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 37, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5626742937182209, 'global_pooling': 'max', 'learning_rate': 0.0005606256227583374, 'weight_decay': 6.710330948399199e-06, 'beta_0': 0.8554212174358607, 'beta_1': 0.9927844050929426, 'epsilon': 4.572894660974311e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 3.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 86.69 MiB is free. Including non-PyTorch memory, this process has 44.47 GiB memory in use. Of the allocated memory 27.49 GiB is allocated by PyTorch, and 15.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 16:18:06,979] Trial 347 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9419530448250043, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 94, 'number_of_hidden_layers': 0, 'dropout_rate': 0.541832016196367, 'global_pooling': 'sum', 'learning_rate': 0.011453962104122797, 'weight_decay': 0.0003981523586526712, 'beta_0': 0.8486320241150328, 'beta_1': 0.9837796926994389, 'epsilon': 1.4539454933153642e-08, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 16:28:11,961] Trial 348 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9462139067098265, 'batch_size': 41, 'attention_heads': 14, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5312985576511416, 'global_pooling': 'max', 'learning_rate': 0.00691903102928616, 'weight_decay': 8.603828058366942e-06, 'beta_0': 0.8537578006564602, 'beta_1': 0.9890765713905799, 'epsilon': 1.2128937324991813e-06, 'balanced_loss': True, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 16:38:16,420] Trial 349 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9438050444991908, 'batch_size': 39, 'attention_heads': 12, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5528126771798763, 'global_pooling': 'max', 'learning_rate': 0.009344380732144685, 'weight_decay': 1.4009576420982715e-05, 'beta_0': 0.8582806724788065, 'beta_1': 0.9880162505951047, 'epsilon': 1.9836057350269228e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 16:48:59,161] Trial 350 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9393751273684784, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5467185261336971, 'global_pooling': 'max', 'learning_rate': 0.003396558450360819, 'weight_decay': 2.1253305379237825e-05, 'beta_0': 0.8381039519209403, 'beta_1': 0.9886868459649416, 'epsilon': 3.091855408546356e-08, 'balanced_loss': True, 'epochs': 164, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.95 GiB is free. Including non-PyTorch memory, this process has 40.61 GiB memory in use. Of the allocated memory 24.85 GiB is allocated by PyTorch, and 14.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 16:57:20,447] Trial 351 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9665925968465574, 'batch_size': 38, 'attention_heads': 14, 'hidden_dimension': 55, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5555821765039078, 'global_pooling': 'max', 'learning_rate': 0.005612503472743758, 'weight_decay': 0.0005614632740044596, 'beta_0': 0.852129712014364, 'beta_1': 0.9973106199079084, 'epsilon': 8.405521852210646e-07, 'balanced_loss': True, 'epochs': 109, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 17:06:52,841] Trial 352 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9904872360553386, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5633235690591565, 'global_pooling': 'max', 'learning_rate': 0.0072608543656068935, 'weight_decay': 1.0297626790497e-05, 'beta_0': 0.8410161816553956, 'beta_1': 0.98967015010499, 'epsilon': 2.3671944434086334e-08, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 17:16:42,703] Trial 353 finished with value: 0.9212121212121213 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.948745892983541, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.538947828168462, 'global_pooling': 'max', 'learning_rate': 0.012119062775232059, 'weight_decay': 3.492985170543586e-05, 'beta_0': 0.848424527037771, 'beta_1': 0.9893525727323363, 'epsilon': 1.2266152126714522e-08, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.58 GiB is free. Including non-PyTorch memory, this process has 39.97 GiB memory in use. Of the allocated memory 35.28 GiB is allocated by PyTorch, and 3.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 17:26:52,025] Trial 354 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9417630630806519, 'batch_size': 41, 'attention_heads': 10, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.569890267305664, 'global_pooling': 'max', 'learning_rate': 0.00864916600512959, 'weight_decay': 3.0373015053325484e-05, 'beta_0': 0.8505691883452541, 'beta_1': 0.9883221352909448, 'epsilon': 1.6701123580285578e-08, 'balanced_loss': True, 'epochs': 100, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 514.69 MiB is free. Including non-PyTorch memory, this process has 44.05 GiB memory in use. Of the allocated memory 27.66 GiB is allocated by PyTorch, and 15.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 17:35:14,934] Trial 355 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9457468200917222, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5810752984954732, 'global_pooling': 'sum', 'learning_rate': 0.00489400249167369, 'weight_decay': 1.1891656426281765e-05, 'beta_0': 0.8459545061609234, 'beta_1': 0.9900728367493489, 'epsilon': 6.827544440435903e-06, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 42.77 GiB memory in use. Of the allocated memory 20.96 GiB is allocated by PyTorch, and 20.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 17:43:36,481] Trial 356 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9949780017266707, 'batch_size': 43, 'attention_heads': 13, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5595254452258189, 'global_pooling': 'max', 'learning_rate': 0.0007635889070974272, 'weight_decay': 0.00029109368169595004, 'beta_0': 0.8557164534683313, 'beta_1': 0.9875264135605434, 'epsilon': 4.135483458326608e-08, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 17:53:36,153] Trial 357 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9405730731308702, 'batch_size': 60, 'attention_heads': 12, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5469529049207004, 'global_pooling': 'max', 'learning_rate': 0.01026192634385702, 'weight_decay': 2.619587489713659e-05, 'beta_0': 0.8436051394695903, 'beta_1': 0.9889352900982012, 'epsilon': 2.3326084488797445e-05, 'balanced_loss': True, 'epochs': 151, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 468.69 MiB is free. Including non-PyTorch memory, this process has 44.10 GiB memory in use. Of the allocated memory 29.07 GiB is allocated by PyTorch, and 13.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 18:01:57,781] Trial 358 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9354473967033251, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 35, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5745182784739897, 'global_pooling': 'max', 'learning_rate': 0.006258181043642578, 'weight_decay': 1.6804199198753423e-05, 'beta_0': 0.8533926150309478, 'beta_1': 0.9905692014053362, 'epsilon': 1.6867462131444984e-06, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.93 GiB is free. Including non-PyTorch memory, this process has 38.63 GiB memory in use. Of the allocated memory 27.84 GiB is allocated by PyTorch, and 9.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 18:10:20,323] Trial 359 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9444467466348925, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 100, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5217243155892088, 'global_pooling': 'max', 'learning_rate': 0.007818804776235219, 'weight_decay': 1.3448469973847026e-05, 'beta_0': 0.8497186712091639, 'beta_1': 0.9913048587549051, 'epsilon': 2.3909509905113026e-08, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 25, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 18:20:34,766] Trial 360 finished with value: 0.9090909090909091 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9379654919005399, 'batch_size': 41, 'attention_heads': 13, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5524002070751062, 'global_pooling': 'max', 'learning_rate': 0.013800505587664812, 'weight_decay': 1.9151758150501356e-05, 'beta_0': 0.851403660504341, 'beta_1': 0.9879091050707524, 'epsilon': 1.9171492085008466e-08, 'balanced_loss': True, 'epochs': 155, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 18:31:00,395] Trial 361 finished with value: 0.9272727272727272 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9468411819263628, 'batch_size': 37, 'attention_heads': 14, 'hidden_dimension': 69, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5655365410411635, 'global_pooling': 'max', 'learning_rate': 0.0065271082204072526, 'weight_decay': 0.000345354524153877, 'beta_0': 0.8472783428413777, 'beta_1': 0.9896470809436968, 'epsilon': 1.3779281432823662e-08, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 18:40:42,270] Trial 362 finished with value: 0.8242424242424242 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9429219757977142, 'batch_size': 54, 'attention_heads': 12, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5423756550980079, 'global_pooling': 'sum', 'learning_rate': 0.0041477369880487365, 'weight_decay': 2.2819300833570165e-05, 'beta_0': 0.8571661692868086, 'beta_1': 0.9892323396435986, 'epsilon': 5.560662181052839e-08, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 18:50:00,702] Trial 363 finished with value: 0.8909090909090909 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9987880248130364, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 73, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5537260500232194, 'global_pooling': 'max', 'learning_rate': 0.010138086180386117, 'weight_decay': 0.00021669365078654372, 'beta_0': 0.8443713104920961, 'beta_1': 0.9885025932539581, 'epsilon': 1.0281165777669456e-06, 'balanced_loss': True, 'epochs': 162, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 19:00:07,597] Trial 364 finished with value: 0.9393939393939394 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9412305478009607, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 46, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5300017087685179, 'global_pooling': 'max', 'learning_rate': 0.005545982476836364, 'weight_decay': 1.5874801351621617e-05, 'beta_0': 0.8049098541483481, 'beta_1': 0.9888574014360562, 'epsilon': 3.457438773889165e-08, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 19:10:12,101] Trial 365 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9512010830082983, 'batch_size': 38, 'attention_heads': 13, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42911913742152596, 'global_pooling': 'max', 'learning_rate': 0.008602153396628183, 'weight_decay': 1.9376657491640602e-05, 'beta_0': 0.8541004964107596, 'beta_1': 0.9899024385856525, 'epsilon': 1.5733403086310112e-08, 'balanced_loss': True, 'epochs': 158, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 19:21:55,537] Trial 366 finished with value: 0.8606060606060606 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9449839993307694, 'batch_size': 45, 'attention_heads': 14, 'hidden_dimension': 56, 'number_of_hidden_layers': 1, 'dropout_rate': 0.560458187326596, 'global_pooling': 'max', 'learning_rate': 0.007597602849792754, 'weight_decay': 0.0007249568542632521, 'beta_0': 0.8491923939415913, 'beta_1': 0.98811119129813, 'epsilon': 2.7184571473345285e-08, 'balanced_loss': True, 'epochs': 168, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.95 GiB is free. Including non-PyTorch memory, this process has 38.61 GiB memory in use. Of the allocated memory 27.27 GiB is allocated by PyTorch, and 10.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 19:30:17,914] Trial 367 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9487825687385418, 'batch_size': 42, 'attention_heads': 13, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5460574748548896, 'global_pooling': 'max', 'learning_rate': 0.011633307919897896, 'weight_decay': 0.0005071413370574851, 'beta_0': 0.8513354071805921, 'beta_1': 0.9871964396040172, 'epsilon': 3.2833743274164843e-06, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
CUDA out of memory. Tried to allocate 5.99 GiB. GPU 0 has a total capacity of 44.56 GiB of which 5.92 GiB is free. Including non-PyTorch memory, this process has 38.63 GiB memory in use. Of the allocated memory 28.05 GiB is allocated by PyTorch, and 9.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 19:38:39,614] Trial 368 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9429096147347815, 'batch_size': 43, 'attention_heads': 12, 'hidden_dimension': 61, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5682369197124463, 'global_pooling': 'max', 'learning_rate': 0.006156097540407257, 'weight_decay': 2.5010543522288675e-05, 'beta_0': 0.8461947075810591, 'beta_1': 0.9865414540102494, 'epsilon': 1.2126904681348819e-08, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 19:48:25,157] Trial 369 finished with value: 0.8848484848484849 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9703650839034638, 'batch_size': 39, 'attention_heads': 14, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5350943194423796, 'global_pooling': 'max', 'learning_rate': 0.0011208073749604036, 'weight_decay': 1.4014277395880607e-05, 'beta_0': 0.85974912522197, 'beta_1': 0.9893636578604218, 'epsilon': 1.1508433379880652e-05, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 19:58:16,100] Trial 370 finished with value: 0.5818181818181818 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9390306143705432, 'batch_size': 41, 'attention_heads': 11, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5589180358926743, 'global_pooling': 'sum', 'learning_rate': 0.003132252588004254, 'weight_decay': 0.0004203612042687474, 'beta_0': 0.8528489409587815, 'beta_1': 0.9845053631640396, 'epsilon': 1.969912876171048e-08, 'balanced_loss': True, 'epochs': 131, 'early_stopping_patience': 12, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 76 with value: 0.9454545454545454.
[I 2024-12-05 20:07:43,564] Trial 371 finished with value: 0.9515151515151515 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9931677248466678, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.44269883253946685, 'global_pooling': 'max', 'learning_rate': 0.004868036819974237, 'weight_decay': 2.0821887736335203e-05, 'beta_0': 0.8552736292746274, 'beta_1': 0.9903264147765077, 'epsilon': 3.110922004184621e-08, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 371 with value: 0.9515151515151515.
[I 2024-12-05 20:17:38,025] Trial 372 finished with value: 0.896969696969697 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9448362108209938, 'batch_size': 39, 'attention_heads': 13, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.443125681925109, 'global_pooling': 'max', 'learning_rate': 0.005357172688655559, 'weight_decay': 2.117164642238465e-05, 'beta_0': 0.8565826604368502, 'beta_1': 0.9886675993510953, 'epsilon': 3.727390026652937e-08, 'balanced_loss': True, 'epochs': 150, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 371 with value: 0.9515151515151515.
CUDA out of memory. Tried to allocate 5.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 42.86 GiB memory in use. Of the allocated memory 18.95 GiB is allocated by PyTorch, and 22.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-05 20:24:04,688] Trial 373 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9934066193077103, 'batch_size': 40, 'attention_heads': 13, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43638655804100224, 'global_pooling': 'max', 'learning_rate': 0.0043348006049672245, 'weight_decay': 3.0484487935690555e-05, 'beta_0': 0.854771748343983, 'beta_1': 0.9902532340577737, 'epsilon': 3.055511672074871e-08, 'balanced_loss': True, 'epochs': 155, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 371 with value: 0.9515151515151515.
[I 2024-12-05 20:33:48,266] Trial 374 finished with value: 0.9333333333333333 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9414476535030747, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.45592506152872014, 'global_pooling': 'max', 'learning_rate': 0.0039053637095877937, 'weight_decay': 1.7522276903660126e-05, 'beta_0': 0.8593119974562776, 'beta_1': 0.9890923754485224, 'epsilon': 2.69649842944714e-08, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 371 with value: 0.9515151515151515.
[I 2024-12-05 20:43:27,969] Trial 375 finished with value: 0.9151515151515152 and parameters: {'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9471006713752071, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 44, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4635689854172546, 'global_pooling': 'max', 'learning_rate': 0.00481473012189594, 'weight_decay': 2.496967702988946e-05, 'beta_0': 0.8566766765172479, 'beta_1': 0.9897531438139425, 'epsilon': 2.4571343541217646e-08, 'balanced_loss': True, 'epochs': 157, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 371 with value: 0.9515151515151515.
slurmstepd: error: *** JOB 14062064 ON gpu027 CANCELLED AT 2024-12-05T20:50:56 DUE TO TIME LIMIT ***
