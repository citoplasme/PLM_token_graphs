[I 2024-12-03 05:06:48,666] Using an existing study with name 'R8-GATv2-xlnet-xlnet-base-cased-Grouped-No_Aggregation' instead of creating a new one.
[I 2024-12-03 05:18:18,071] Trial 59 finished with value: 0.938929934055331 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7728270603232129, 'batch_size': 244, 'attention_heads': 11, 'hidden_dimension': 115, 'number_of_hidden_layers': 0, 'dropout_rate': 0.310814759724931, 'global_pooling': 'max', 'learning_rate': 0.0009006262677075493, 'weight_decay': 3.3989968400292857e-06, 'beta_0': 0.8438917897905724, 'beta_1': 0.9849758401991635, 'epsilon': 2.1029428780399843e-07, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 37 with value: 0.940717230209263.
[I 2024-12-03 05:29:08,974] Trial 60 finished with value: 0.9361084605270571 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7771869777394594, 'batch_size': 243, 'attention_heads': 11, 'hidden_dimension': 110, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30080119162251845, 'global_pooling': 'max', 'learning_rate': 0.0008626170093331092, 'weight_decay': 0.00019244596362636023, 'beta_0': 0.8536081219589317, 'beta_1': 0.9846899564351547, 'epsilon': 1.540738239901367e-07, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 37 with value: 0.940717230209263.
[I 2024-12-03 05:39:46,109] Trial 61 finished with value: 0.9207578573560518 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8121018410380039, 'batch_size': 245, 'attention_heads': 11, 'hidden_dimension': 129, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3191719663925406, 'global_pooling': 'max', 'learning_rate': 0.0015083085778122192, 'weight_decay': 2.0846980653589516e-06, 'beta_0': 0.8463115027229309, 'beta_1': 0.9828274843012258, 'epsilon': 2.2648393983844493e-07, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 37 with value: 0.940717230209263.
[I 2024-12-03 05:52:39,712] Trial 62 finished with value: 0.9408099158670469 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7651997192574059, 'batch_size': 255, 'attention_heads': 13, 'hidden_dimension': 136, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34652259240895816, 'global_pooling': 'max', 'learning_rate': 0.0007445268378808818, 'weight_decay': 3.804541749410179e-06, 'beta_0': 0.8418014754546866, 'beta_1': 0.9869418475540946, 'epsilon': 1.0568533338735834e-07, 'balanced_loss': False, 'epochs': 155, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 62 with value: 0.9408099158670469.
[I 2024-12-03 06:05:11,421] Trial 63 finished with value: 0.9379612914799952 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7967127302637398, 'batch_size': 252, 'attention_heads': 13, 'hidden_dimension': 142, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34567615468011637, 'global_pooling': 'max', 'learning_rate': 0.0004374029324858842, 'weight_decay': 3.8599106237869565e-06, 'beta_0': 0.8432965920462034, 'beta_1': 0.9868616505459561, 'epsilon': 5.55286155380564e-07, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 62 with value: 0.9408099158670469.
[I 2024-12-03 06:18:32,723] Trial 64 finished with value: 0.9358757339436488 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7695909260743933, 'batch_size': 221, 'attention_heads': 13, 'hidden_dimension': 134, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34956908092167144, 'global_pooling': 'max', 'learning_rate': 0.00197641263426459, 'weight_decay': 6.941543981339162e-06, 'beta_0': 0.8557779066688775, 'beta_1': 0.9851465197803702, 'epsilon': 9.304420999698111e-08, 'balanced_loss': False, 'epochs': 149, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 62 with value: 0.9408099158670469.
[I 2024-12-03 06:29:42,833] Trial 65 finished with value: 0.940983947424612 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7416321034643212, 'batch_size': 242, 'attention_heads': 11, 'hidden_dimension': 111, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31127082553607266, 'global_pooling': 'max', 'learning_rate': 0.0009290061102455362, 'weight_decay': 2.9547689813432915e-06, 'beta_0': 0.8499060830965879, 'beta_1': 0.9867838877739642, 'epsilon': 1.6888996663127866e-07, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 65 with value: 0.940983947424612.
[I 2024-12-03 06:42:26,494] Trial 66 finished with value: 0.9403279654215626 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7887616907737626, 'batch_size': 255, 'attention_heads': 12, 'hidden_dimension': 151, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3124848473782129, 'global_pooling': 'max', 'learning_rate': 0.0012578682657946982, 'weight_decay': 1.7648663341777466e-06, 'beta_0': 0.8488568671956987, 'beta_1': 0.9867884134715823, 'epsilon': 1.5304924217318268e-07, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 21, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 65 with value: 0.940983947424612.
[I 2024-12-03 06:54:25,623] Trial 67 finished with value: 0.9428346209364438 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8219689562419674, 'batch_size': 244, 'attention_heads': 14, 'hidden_dimension': 167, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31835187767039397, 'global_pooling': 'max', 'learning_rate': 0.001133711378534671, 'weight_decay': 1.6585535430885198e-06, 'beta_0': 0.8599397883380038, 'beta_1': 0.986610497389935, 'epsilon': 6.107639933072e-08, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 67 with value: 0.9428346209364438.
CUDA out of memory. Tried to allocate 5.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.68 GiB is free. Including non-PyTorch memory, this process has 39.87 GiB memory in use. Of the allocated memory 36.61 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 07:07:23,351] Trial 68 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7888963937260044, 'batch_size': 240, 'attention_heads': 14, 'hidden_dimension': 158, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31471634960756933, 'global_pooling': 'max', 'learning_rate': 0.00118973950415528, 'weight_decay': 1.3548970674382453e-06, 'beta_0': 0.8641930148149093, 'beta_1': 0.9879177647862629, 'epsilon': 6.879387611259273e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 67 with value: 0.9428346209364438.
[I 2024-12-03 07:19:38,103] Trial 69 finished with value: 0.9317749236895954 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8181249865612795, 'batch_size': 248, 'attention_heads': 15, 'hidden_dimension': 170, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3350478772738642, 'global_pooling': 'max', 'learning_rate': 0.002269594916354165, 'weight_decay': 1.8072209113386535e-06, 'beta_0': 0.8715055891170652, 'beta_1': 0.9865910588437953, 'epsilon': 2.396321240508593e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 67 with value: 0.9428346209364438.
[I 2024-12-03 07:33:51,613] Trial 70 finished with value: 0.902939765424779 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8238110539184929, 'batch_size': 255, 'attention_heads': 14, 'hidden_dimension': 150, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33660683339547265, 'global_pooling': 'max', 'learning_rate': 0.0001758687779084456, 'weight_decay': 9.062118597320699e-06, 'beta_0': 0.8491021336255153, 'beta_1': 0.9877711672829149, 'epsilon': 4.804275912069041e-07, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 67 with value: 0.9428346209364438.
[I 2024-12-03 07:45:28,413] Trial 71 finished with value: 0.9326723488489048 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8444545274281582, 'batch_size': 137, 'attention_heads': 16, 'hidden_dimension': 206, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32031168775387175, 'global_pooling': 'max', 'learning_rate': 0.00476440271081749, 'weight_decay': 4.320676106890704e-06, 'beta_0': 0.8617354079405689, 'beta_1': 0.9864298374871826, 'epsilon': 1.0479150212570545e-06, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 67 with value: 0.9428346209364438.
CUDA out of memory. Tried to allocate 5.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.99 GiB is free. Including non-PyTorch memory, this process has 39.56 GiB memory in use. Of the allocated memory 37.96 GiB is allocated by PyTorch, and 457.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 07:57:13,123] Trial 72 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7680444578470051, 'batch_size': 245, 'attention_heads': 12, 'hidden_dimension': 177, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3072978561612617, 'global_pooling': 'max', 'learning_rate': 0.0011037109284951392, 'weight_decay': 3.0315555424298067e-06, 'beta_0': 0.8405122614140021, 'beta_1': 0.984339758959476, 'epsilon': 2.1755168903584934e-07, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 21, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 67 with value: 0.9428346209364438.
[I 2024-12-03 08:08:27,875] Trial 73 finished with value: 0.930078757708155 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7872629194363188, 'batch_size': 224, 'attention_heads': 13, 'hidden_dimension': 120, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31171160143833526, 'global_pooling': 'max', 'learning_rate': 0.00146774427995652, 'weight_decay': 2.9489408843840425e-06, 'beta_0': 0.8494776732421606, 'beta_1': 0.9854993284813746, 'epsilon': 5.427081746629305e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 67 with value: 0.9428346209364438.
[I 2024-12-03 08:19:28,922] Trial 74 finished with value: 0.9450954505201472 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8357092362914367, 'batch_size': 241, 'attention_heads': 12, 'hidden_dimension': 155, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32837060465706647, 'global_pooling': 'max', 'learning_rate': 0.0004969124898104183, 'weight_decay': 6.404959435525511e-06, 'beta_0': 0.8564318510823238, 'beta_1': 0.9862937683395998, 'epsilon': 1.416060028600286e-07, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 08:30:16,070] Trial 75 finished with value: 0.9353205134734304 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8380070497438088, 'batch_size': 256, 'attention_heads': 12, 'hidden_dimension': 155, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32890680689161206, 'global_pooling': 'max', 'learning_rate': 0.00035991432213490135, 'weight_decay': 6.526317074964164e-06, 'beta_0': 0.8562596505459634, 'beta_1': 0.9883328670786421, 'epsilon': 1.5875807805929351e-07, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 08:40:19,564] Trial 76 finished with value: 0.933755157278874 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8540893427349212, 'batch_size': 162, 'attention_heads': 13, 'hidden_dimension': 164, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3637204502973257, 'global_pooling': 'max', 'learning_rate': 0.0006072454667720116, 'weight_decay': 1.4320218021054555e-06, 'beta_0': 0.8716959267346522, 'beta_1': 0.9862989491709485, 'epsilon': 9.027670034126721e-08, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 2.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1006.69 MiB is free. Including non-PyTorch memory, this process has 43.57 GiB memory in use. Of the allocated memory 41.19 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:48:25,712] Trial 77 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8084005153053068, 'batch_size': 177, 'attention_heads': 12, 'hidden_dimension': 184, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3383246190686274, 'global_pooling': 'max', 'learning_rate': 0.0005509358096013076, 'weight_decay': 2.048526774441303e-06, 'beta_0': 0.8835386189110929, 'beta_1': 0.9875946434453963, 'epsilon': 2.687778958283438e-07, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 15, 'plateau_patience': 22, 'plateau_divider': 9}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.25 GiB is free. Including non-PyTorch memory, this process has 40.31 GiB memory in use. Of the allocated memory 34.50 GiB is allocated by PyTorch, and 4.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 08:56:36,593] Trial 78 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7044000478133375, 'batch_size': 214, 'attention_heads': 14, 'hidden_dimension': 138, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3540694282637181, 'global_pooling': 'max', 'learning_rate': 0.0002943225801486567, 'weight_decay': 1.7395606145193393e-06, 'beta_0': 0.8603622768097295, 'beta_1': 0.9871418997935962, 'epsilon': 1.3459195735878155e-07, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 23, 'plateau_patience': 21, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.19 GiB is free. Including non-PyTorch memory, this process has 43.37 GiB memory in use. Of the allocated memory 40.73 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 09:04:24,380] Trial 79 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.6762005362300406, 'batch_size': 241, 'attention_heads': 12, 'hidden_dimension': 150, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3210324285744099, 'global_pooling': 'max', 'learning_rate': 0.0007852953447895233, 'weight_decay': 4.296811383648005e-06, 'beta_0': 0.857887922364591, 'beta_1': 0.9903466638788047, 'epsilon': 2.7702885975452797e-08, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 09:14:40,851] Trial 80 finished with value: 0.9246704332175794 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8283285961603674, 'batch_size': 141, 'attention_heads': 10, 'hidden_dimension': 162, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3007486534258218, 'global_pooling': 'max', 'learning_rate': 0.0004738107204065892, 'weight_decay': 5.7639396427630695e-06, 'beta_0': 0.8512451018665309, 'beta_1': 0.984363553542258, 'epsilon': 6.261999771505933e-08, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 09:29:49,155] Trial 81 finished with value: 0.9318971329099033 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7943070922461997, 'batch_size': 248, 'attention_heads': 13, 'hidden_dimension': 171, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33540015116651406, 'global_pooling': 'max', 'learning_rate': 0.0015204054551675357, 'weight_decay': 2.1595886374332872e-06, 'beta_0': 0.8536034021605121, 'beta_1': 0.9829852973721307, 'epsilon': 4.3095571146851176e-07, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 09:41:35,688] Trial 82 finished with value: 0.9380324049005695 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7671484352163834, 'batch_size': 231, 'attention_heads': 12, 'hidden_dimension': 127, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31261924304884303, 'global_pooling': 'max', 'learning_rate': 0.0009655128774036122, 'weight_decay': 3.5691467731921875e-06, 'beta_0': 0.8361138880390205, 'beta_1': 0.9852357677773377, 'epsilon': 1.0532069548650249e-07, 'balanced_loss': False, 'epochs': 165, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 09:52:12,932] Trial 83 finished with value: 0.9270108397673384 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7357837494474996, 'batch_size': 240, 'attention_heads': 10, 'hidden_dimension': 119, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30939885890361407, 'global_pooling': 'max', 'learning_rate': 0.001108776695655499, 'weight_decay': 3.150045084720829e-06, 'beta_0': 0.8429859551998997, 'beta_1': 0.9861751503704098, 'epsilon': 2.022712517356327e-07, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 4.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 40.86 GiB memory in use. Of the allocated memory 38.00 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 10:00:20,436] Trial 84 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.6635768189698776, 'batch_size': 247, 'attention_heads': 11, 'hidden_dimension': 145, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32833679547011463, 'global_pooling': 'max', 'learning_rate': 0.002184005891533279, 'weight_decay': 4.5695350028623425e-05, 'beta_0': 0.847451298650551, 'beta_1': 0.9873248479738351, 'epsilon': 6.584053989481587e-07, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 10:12:23,274] Trial 85 finished with value: 0.9224698473455124 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7280981785931766, 'batch_size': 153, 'attention_heads': 9, 'hidden_dimension': 133, 'number_of_hidden_layers': 0, 'dropout_rate': 0.320890240932742, 'global_pooling': 'max', 'learning_rate': 0.0013322503413306615, 'weight_decay': 0.00011735606131447061, 'beta_0': 0.8700364719930609, 'beta_1': 0.9847276266286283, 'epsilon': 7.834998720448039e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 10:22:55,105] Trial 86 finished with value: 0.9351344058353612 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.872808750299213, 'batch_size': 216, 'attention_heads': 13, 'hidden_dimension': 140, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3543811865743376, 'global_pooling': 'max', 'learning_rate': 0.0007202573033313159, 'weight_decay': 4.817619964222526e-06, 'beta_0': 0.8640998769735461, 'beta_1': 0.9819027203926396, 'epsilon': 2.6257726192462135e-07, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 3.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.99 GiB is free. Including non-PyTorch memory, this process has 41.56 GiB memory in use. Of the allocated memory 39.18 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 10:31:01,848] Trial 87 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7828817161091883, 'batch_size': 227, 'attention_heads': 12, 'hidden_dimension': 180, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3416836343026852, 'global_pooling': 'max', 'learning_rate': 0.000363286398177881, 'weight_decay': 2.2631947818289096e-06, 'beta_0': 0.8558132169631171, 'beta_1': 0.988373645450721, 'epsilon': 4.355723438471329e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 10:42:22,614] Trial 88 finished with value: 0.9305993028678585 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8056506240145321, 'batch_size': 236, 'attention_heads': 11, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3331600694027352, 'global_pooling': 'max', 'learning_rate': 0.0017839423687625406, 'weight_decay': 2.794055279016659e-06, 'beta_0': 0.8381657499798116, 'beta_1': 0.9867206303623528, 'epsilon': 1.6093122621561925e-07, 'balanced_loss': False, 'epochs': 51, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 10:52:27,651] Trial 89 finished with value: 0.9297342541974918 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8194785731401423, 'batch_size': 197, 'attention_heads': 12, 'hidden_dimension': 108, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3668796872651522, 'global_pooling': 'max', 'learning_rate': 0.002731206638919758, 'weight_decay': 1.21861550887112e-06, 'beta_0': 0.8503915627823277, 'beta_1': 0.9892807670916801, 'epsilon': 1.383733089763689e-06, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 5.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.27 GiB is free. Including non-PyTorch memory, this process has 40.28 GiB memory in use. Of the allocated memory 37.88 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 11:00:16,789] Trial 90 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7166455938610188, 'batch_size': 250, 'attention_heads': 14, 'hidden_dimension': 167, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31764848283033487, 'global_pooling': 'sum', 'learning_rate': 0.00023853146260612395, 'weight_decay': 1.673089693961511e-06, 'beta_0': 0.8030555835219362, 'beta_1': 0.9839548613731239, 'epsilon': 1.2926599530581944e-07, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 5.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.49 GiB is free. Including non-PyTorch memory, this process has 43.07 GiB memory in use. Of the allocated memory 39.56 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 11:08:22,259] Trial 91 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.6972073594764291, 'batch_size': 243, 'attention_heads': 10, 'hidden_dimension': 229, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3060372682727507, 'global_pooling': 'max', 'learning_rate': 0.0005153429347550818, 'weight_decay': 1.7734541934872117e-05, 'beta_0': 0.841975642698329, 'beta_1': 0.9938516505031086, 'epsilon': 5.656985235195806e-08, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 17, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 11:19:22,818] Trial 92 finished with value: 0.9448229128207762 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7594941062388842, 'batch_size': 254, 'attention_heads': 11, 'hidden_dimension': 111, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34842996839632473, 'global_pooling': 'max', 'learning_rate': 0.0008427194361552417, 'weight_decay': 2.3436309175396766e-06, 'beta_0': 0.8448718325059048, 'beta_1': 0.9873685499914433, 'epsilon': 1.778360103621072e-07, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 11:30:17,779] Trial 93 finished with value: 0.9237712659558526 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7623856507299841, 'batch_size': 255, 'attention_heads': 11, 'hidden_dimension': 94, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3520855322660198, 'global_pooling': 'max', 'learning_rate': 0.0009260707694974419, 'weight_decay': 3.623447617756421e-06, 'beta_0': 0.8527097422865721, 'beta_1': 0.9857909392790273, 'epsilon': 3.1821460500360214e-08, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 11:41:25,845] Trial 94 finished with value: 0.9374095647112239 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.77172796272824, 'batch_size': 249, 'attention_heads': 11, 'hidden_dimension': 105, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3260159891735806, 'global_pooling': 'max', 'learning_rate': 0.0007759510397108516, 'weight_decay': 2.4105352615615184e-06, 'beta_0': 0.8478254154828684, 'beta_1': 0.987359816775962, 'epsilon': 3.7132730246704257e-07, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 21, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 11:52:18,972] Trial 95 finished with value: 0.9402691118341201 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7922722697958141, 'batch_size': 235, 'attention_heads': 12, 'hidden_dimension': 117, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3413334415816249, 'global_pooling': 'max', 'learning_rate': 0.000991903262582756, 'weight_decay': 1.850787395512955e-06, 'beta_0': 0.8455404103012291, 'beta_1': 0.9880905338107414, 'epsilon': 7.310571716795885e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 12:03:39,296] Trial 96 finished with value: 0.9324862587400065 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7469103882400133, 'batch_size': 238, 'attention_heads': 12, 'hidden_dimension': 125, 'number_of_hidden_layers': 0, 'dropout_rate': 0.359956722235189, 'global_pooling': 'max', 'learning_rate': 0.0013178019468020473, 'weight_decay': 1.8633863480958403e-06, 'beta_0': 0.8338908662607737, 'beta_1': 0.9880271802852407, 'epsilon': 7.054663860360018e-08, 'balanced_loss': False, 'epochs': 151, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
CUDA out of memory. Tried to allocate 4.94 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.99 GiB is free. Including non-PyTorch memory, this process has 41.56 GiB memory in use. Of the allocated memory 39.03 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-03 12:11:44,634] Trial 97 finished with value: -1.0 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.670232758470384, 'batch_size': 232, 'attention_heads': 13, 'hidden_dimension': 157, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34304438490587535, 'global_pooling': 'max', 'learning_rate': 0.000415585454363902, 'weight_decay': 1.0342320808107038e-06, 'beta_0': 0.8380125352177605, 'beta_1': 0.9926892489713763, 'epsilon': 1.0551401494098157e-07, 'balanced_loss': False, 'epochs': 136, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 9}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 12:24:05,496] Trial 98 finished with value: 0.9312195156007623 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.793703570239278, 'batch_size': 235, 'attention_heads': 12, 'hidden_dimension': 134, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3324670392174405, 'global_pooling': 'max', 'learning_rate': 0.0007222611507093089, 'weight_decay': 1.4972717501555907e-06, 'beta_0': 0.8616276429848787, 'beta_1': 0.991646805651446, 'epsilon': 2.046837949397013e-08, 'balanced_loss': False, 'epochs': 141, 'early_stopping_patience': 21, 'plateau_patience': 10, 'plateau_divider': 6}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 12:34:44,981] Trial 99 finished with value: 0.9289822668929707 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8007204639941926, 'batch_size': 224, 'attention_heads': 13, 'hidden_dimension': 98, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34214926261161616, 'global_pooling': 'max', 'learning_rate': 0.0017828473251499956, 'weight_decay': 1.2931507225017187e-06, 'beta_0': 0.8581077869882872, 'beta_1': 0.9867854458772027, 'epsilon': 8.68481358919745e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 12:50:12,699] Trial 100 finished with value: 0.9332200458217605 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7537052506099051, 'batch_size': 256, 'attention_heads': 12, 'hidden_dimension': 85, 'number_of_hidden_layers': 1, 'dropout_rate': 0.32377127354139146, 'global_pooling': 'max', 'learning_rate': 0.0010768506120013644, 'weight_decay': 2.7611395967696647e-06, 'beta_0': 0.8472519690599531, 'beta_1': 0.9875731075971453, 'epsilon': 4.415793742335233e-08, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:03:29,985] Trial 101 finished with value: 0.9001722118376904 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8493830488728201, 'batch_size': 250, 'attention_heads': 13, 'hidden_dimension': 116, 'number_of_hidden_layers': 0, 'dropout_rate': 0.36813736014293075, 'global_pooling': 'sum', 'learning_rate': 0.0005661373824543688, 'weight_decay': 1.8087949969386724e-06, 'beta_0': 0.8512724994950374, 'beta_1': 0.9887411810230604, 'epsilon': 1.44281945295043e-07, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:15:22,504] Trial 102 finished with value: 0.9450078832597395 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.774941535247975, 'batch_size': 244, 'attention_heads': 11, 'hidden_dimension': 114, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30954380248500407, 'global_pooling': 'max', 'learning_rate': 0.0015848107142103893, 'weight_decay': 3.1802485278040117e-06, 'beta_0': 0.8452762329958574, 'beta_1': 0.9855524436388731, 'epsilon': 1.8803828021213857e-07, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:26:38,018] Trial 103 finished with value: 0.9166811674164616 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.760785270346615, 'batch_size': 243, 'attention_heads': 11, 'hidden_dimension': 123, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3151915096072416, 'global_pooling': 'max', 'learning_rate': 0.0014900520207160456, 'weight_decay': 4.862144611752968e-06, 'beta_0': 0.8457243152141971, 'beta_1': 0.986244768654329, 'epsilon': 1.9195929777540498e-07, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 19, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:36:07,615] Trial 104 finished with value: 0.9267176934835579 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8343881730752443, 'batch_size': 251, 'attention_heads': 4, 'hidden_dimension': 199, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3025874497092678, 'global_pooling': 'max', 'learning_rate': 0.0022270600964036593, 'weight_decay': 3.964616834435785e-06, 'beta_0': 0.854726446896963, 'beta_1': 0.9855829107657497, 'epsilon': 2.9827965527996076e-07, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:47:59,219] Trial 105 finished with value: 0.9344414428494944 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7799403135755485, 'batch_size': 229, 'attention_heads': 11, 'hidden_dimension': 146, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33060163879767007, 'global_pooling': 'max', 'learning_rate': 0.0009741465826536765, 'weight_decay': 2.1119848892858486e-06, 'beta_0': 0.8409265471779386, 'beta_1': 0.9882283992788928, 'epsilon': 1.2347898551456163e-07, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 13:59:33,293] Trial 106 finished with value: 0.9376826940278119 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7855665132637583, 'batch_size': 240, 'attention_heads': 12, 'hidden_dimension': 107, 'number_of_hidden_layers': 0, 'dropout_rate': 0.357194075453736, 'global_pooling': 'max', 'learning_rate': 0.0016183064966206239, 'weight_decay': 3.0092156367674338e-06, 'beta_0': 0.8508134504755707, 'beta_1': 0.9869554009588187, 'epsilon': 7.548970586679977e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 14:11:17,143] Trial 107 finished with value: 0.9341091431451481 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8147289504015618, 'batch_size': 218, 'attention_heads': 11, 'hidden_dimension': 131, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4049453198790045, 'global_pooling': 'max', 'learning_rate': 0.0012578232295606623, 'weight_decay': 2.4263340930186316e-06, 'beta_0': 0.8239106376348375, 'beta_1': 0.9865241467530921, 'epsilon': 1.0571339824126998e-07, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 20, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 14:22:32,799] Trial 108 finished with value: 0.9312754657819844 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7503853623296113, 'batch_size': 246, 'attention_heads': 10, 'hidden_dimension': 114, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3486099701503043, 'global_pooling': 'max', 'learning_rate': 0.0007633326234851788, 'weight_decay': 1.4344391817448131e-06, 'beta_0': 0.8484136049974953, 'beta_1': 0.9953546096712463, 'epsilon': 6.112304917998144e-08, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 14:37:31,207] Trial 109 finished with value: 0.9309410101288054 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.6508587403197774, 'batch_size': 211, 'attention_heads': 12, 'hidden_dimension': 58, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4704117794831773, 'global_pooling': 'max', 'learning_rate': 0.002742167048078558, 'weight_decay': 5.798621886084051e-06, 'beta_0': 0.8454617452644154, 'beta_1': 0.985394597364888, 'epsilon': 1.7758296029546195e-07, 'balanced_loss': False, 'epochs': 138, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 14:48:46,243] Trial 110 finished with value: 0.9221391419148879 and parameters: {'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.730221908996322, 'batch_size': 157, 'attention_heads': 10, 'hidden_dimension': 140, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32445764387402737, 'global_pooling': 'max', 'learning_rate': 0.0005924320190724635, 'weight_decay': 7.80520874447435e-06, 'beta_0': 0.859416097854421, 'beta_1': 0.9859796474623884, 'epsilon': 2.1062234504871236e-06, 'balanced_loss': False, 'epochs': 155, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 15:00:15,355] Trial 111 finished with value: 0.9229221135112944 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7409081487083108, 'batch_size': 172, 'attention_heads': 12, 'hidden_dimension': 137, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3170533827822306, 'global_pooling': 'max', 'learning_rate': 0.0018727063063497537, 'weight_decay': 1.6419572562788124e-06, 'beta_0': 0.856755901000303, 'beta_1': 0.9892855716072454, 'epsilon': 2.5778469398716957e-07, 'balanced_loss': False, 'epochs': 149, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 15:11:07,768] Trial 112 finished with value: 0.9258953355426462 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7634191654193138, 'batch_size': 243, 'attention_heads': 11, 'hidden_dimension': 117, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3110202452631385, 'global_pooling': 'max', 'learning_rate': 0.0009281894018900274, 'weight_decay': 3.371694404246027e-06, 'beta_0': 0.844351564569962, 'beta_1': 0.9874097961283915, 'epsilon': 2.250408503891454e-07, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 15:21:32,328] Trial 113 finished with value: 0.9302477660031963 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7748762569032707, 'batch_size': 234, 'attention_heads': 11, 'hidden_dimension': 98, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3059205908944689, 'global_pooling': 'max', 'learning_rate': 0.0011980945160275185, 'weight_decay': 4.4885728457260014e-06, 'beta_0': 0.8390342999597606, 'beta_1': 0.9937555800505808, 'epsilon': 1.3531749180920692e-07, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 15:33:28,372] Trial 114 finished with value: 0.941099566587114 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7894862438933133, 'batch_size': 256, 'attention_heads': 11, 'hidden_dimension': 123, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31167896147653845, 'global_pooling': 'max', 'learning_rate': 0.0008036499142973126, 'weight_decay': 2.7255541661712486e-06, 'beta_0': 0.8417839132270742, 'beta_1': 0.9850115255531885, 'epsilon': 9.694219900837062e-08, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 15:49:44,787] Trial 115 finished with value: 0.929554129879564 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7908584056154458, 'batch_size': 256, 'attention_heads': 10, 'hidden_dimension': 45, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3375370816179328, 'global_pooling': 'max', 'learning_rate': 0.000411266785502293, 'weight_decay': 2.7878659611611198e-06, 'beta_0': 0.8349117818492581, 'beta_1': 0.9865293966254763, 'epsilon': 4.734164688604809e-08, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 16:01:15,344] Trial 116 finished with value: 0.9303552232556113 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8258501347391426, 'batch_size': 252, 'attention_heads': 11, 'hidden_dimension': 159, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3006948793724583, 'global_pooling': 'max', 'learning_rate': 0.0006775316848785832, 'weight_decay': 1.9291836919810644e-06, 'beta_0': 0.8413513787085847, 'beta_1': 0.984979167538011, 'epsilon': 9.786379376779542e-08, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 21, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 16:10:41,797] Trial 117 finished with value: 0.943106444723591 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8013857209364461, 'batch_size': 145, 'attention_heads': 5, 'hidden_dimension': 124, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32966980483032804, 'global_pooling': 'max', 'learning_rate': 0.000499118507832391, 'weight_decay': 2.303919321901024e-06, 'beta_0': 0.8520738371767189, 'beta_1': 0.9878501218276647, 'epsilon': 3.718333465812448e-08, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 74 with value: 0.9450954505201472.
[I 2024-12-03 16:20:49,807] Trial 118 finished with value: 0.955876017992461 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8003384296685887, 'batch_size': 116, 'attention_heads': 6, 'hidden_dimension': 122, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31717488791681325, 'global_pooling': 'max', 'learning_rate': 0.00032994559679945203, 'weight_decay': 2.3149036088580965e-06, 'beta_0': 0.8522539104325153, 'beta_1': 0.9877940036124038, 'epsilon': 8.137166142313036e-08, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 118 with value: 0.955876017992461.
[I 2024-12-03 16:30:54,529] Trial 119 finished with value: 0.9308546024751474 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7999241435001909, 'batch_size': 92, 'attention_heads': 5, 'hidden_dimension': 123, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3157849232477806, 'global_pooling': 'max', 'learning_rate': 0.00026870665039596914, 'weight_decay': 2.3362270860852064e-06, 'beta_0': 0.854088220513104, 'beta_1': 0.9869335630812227, 'epsilon': 1.6173768763989974e-07, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 118 with value: 0.955876017992461.
[I 2024-12-03 16:40:13,835] Trial 120 finished with value: 0.9310688694862517 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8173708125243385, 'batch_size': 113, 'attention_heads': 5, 'hidden_dimension': 128, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3245726579599927, 'global_pooling': 'max', 'learning_rate': 0.00032635283570051825, 'weight_decay': 3.0892704659704885e-06, 'beta_0': 0.8506890932298852, 'beta_1': 0.9858625744083459, 'epsilon': 5.846708358093652e-08, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 14, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 118 with value: 0.955876017992461.
[I 2024-12-03 16:49:50,955] Trial 121 finished with value: 0.9103180442413741 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8089113679247042, 'batch_size': 136, 'attention_heads': 4, 'hidden_dimension': 150, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31135443751957176, 'global_pooling': 'mean', 'learning_rate': 0.0001481582578234301, 'weight_decay': 2.1996130396631227e-06, 'beta_0': 0.8678994237284086, 'beta_1': 0.9843690489921715, 'epsilon': 3.864178590950041e-06, 'balanced_loss': False, 'epochs': 56, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 118 with value: 0.955876017992461.
[I 2024-12-03 17:00:33,056] Trial 122 finished with value: 0.9494286190519815 and parameters: {'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7836371067047998, 'batch_size': 118, 'attention_heads': 6, 'hidden_dimension': 122, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3294030463618581, 'global_pooling': 'max', 'learning_rate': 0.00050056198782538, 'weight_decay': 1.1921447522538133e-06, 'beta_0': 0.8525228400783798, 'beta_1': 0.9879799705942477, 'epsilon': 3.332530193211053e-08, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 118 with value: 0.955876017992461.
slurmstepd: error: *** JOB 14051919 ON gpu040 CANCELLED AT 2024-12-03T17:06:36 DUE TO TIME LIMIT ***
