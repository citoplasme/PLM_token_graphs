[I 2024-12-18 06:15:47,362] A new study created in RDB with name: R8-GATv2-facebook-bart-large-Surrogate-No_Aggregation
Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 700.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 112.69 MiB is free. Including non-PyTorch memory, this process has 44.44 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 1.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:23:31,706] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7564817426134086, 'batch_size': 150, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
[I 2024-12-18 06:37:21,897] Trial 1 finished with value: 0.9407957181342508 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9788152345580505, 'batch_size': 233, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 1 with value: 0.9407957181342508.
[I 2024-12-18 06:51:36,702] Trial 2 finished with value: 0.942923479532741 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9605155877742886, 'batch_size': 138, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 702.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 580.69 MiB is free. Including non-PyTorch memory, this process has 43.99 GiB memory in use. Of the allocated memory 42.12 GiB is allocated by PyTorch, and 735.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:57:45,900] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7297750275380542, 'batch_size': 128, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 492.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 324.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 41.68 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:04:59,958] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7787204186204115, 'batch_size': 174, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 9.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.53 GiB is free. Including non-PyTorch memory, this process has 40.02 GiB memory in use. Of the allocated memory 34.82 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:13:58,979] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9040772280477951, 'batch_size': 233, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 3.33 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.53 GiB is free. Including non-PyTorch memory, this process has 42.02 GiB memory in use. Of the allocated memory 37.62 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:23:00,252] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8519105905803794, 'batch_size': 142, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 562.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 40.69 MiB is free. Including non-PyTorch memory, this process has 44.51 GiB memory in use. Of the allocated memory 42.52 GiB is allocated by PyTorch, and 859.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:30:27,517] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7758183080154022, 'batch_size': 98, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.93 GiB is free. Including non-PyTorch memory, this process has 42.62 GiB memory in use. Of the allocated memory 37.70 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:39:28,038] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9041247058895944, 'batch_size': 251, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 602.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 22.69 MiB is free. Including non-PyTorch memory, this process has 44.53 GiB memory in use. Of the allocated memory 41.92 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:48:15,829] Trial 9 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8220606401321402, 'batch_size': 138, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 2 with value: 0.942923479532741.
[I 2024-12-18 08:02:14,145] Trial 10 finished with value: 0.7533914851517357 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9942499450596111, 'batch_size': 40, 'attention_heads': 4, 'hidden_dimension': 48, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5215216634175063, 'global_pooling': 'sum', 'learning_rate': 0.0002656836893415814, 'weight_decay': 8.178412772916804e-06, 'beta_0': 0.8049297522472849, 'beta_1': 0.9840036011509948, 'epsilon': 1.3261882354835817e-05, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 10, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 2 with value: 0.942923479532741.
[I 2024-12-18 08:20:44,759] Trial 11 finished with value: 0.7654617076671146 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9965745266953072, 'batch_size': 205, 'attention_heads': 9, 'hidden_dimension': 254, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3965390541849626, 'global_pooling': 'max', 'learning_rate': 4.0083099308315954e-05, 'weight_decay': 1.9948878206620123e-05, 'beta_0': 0.8229997949552795, 'beta_1': 0.9846023396580135, 'epsilon': 1.2304350108398785e-06, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 2.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 42.66 GiB memory in use. Of the allocated memory 40.01 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:30:02,307] Trial 12 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9428667987346189, 'batch_size': 94, 'attention_heads': 11, 'hidden_dimension': 174, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4232817731798428, 'global_pooling': 'mean', 'learning_rate': 0.0003794553385403524, 'weight_decay': 1.0332855569626058e-06, 'beta_0': 0.8160804530670469, 'beta_1': 0.9847976982734662, 'epsilon': 1.051978058963241e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 2 with value: 0.942923479532741.
CUDA out of memory. Tried to allocate 528.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 302.69 MiB is free. Including non-PyTorch memory, this process has 44.26 GiB memory in use. Of the allocated memory 42.42 GiB is allocated by PyTorch, and 705.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:35:19,182] Trial 13 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.6873409124367824, 'batch_size': 200, 'attention_heads': 7, 'hidden_dimension': 256, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48082126313764223, 'global_pooling': 'max', 'learning_rate': 6.567919987784165e-05, 'weight_decay': 3.7947055714200486e-05, 'beta_0': 0.8379605356560551, 'beta_1': 0.9827099355668892, 'epsilon': 4.088261595658418e-06, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 2 with value: 0.942923479532741.
[I 2024-12-18 08:49:32,350] Trial 14 finished with value: 0.943771694514494 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9332786847765561, 'batch_size': 95, 'attention_heads': 4, 'hidden_dimension': 181, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3806622045960868, 'global_pooling': 'sum', 'learning_rate': 0.0007163547440785774, 'weight_decay': 0.0001242481468574075, 'beta_0': 0.8159973543818061, 'beta_1': 0.986691679686623, 'epsilon': 3.3584310308021153e-07, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 14 with value: 0.943771694514494.
[I 2024-12-18 09:05:09,351] Trial 15 finished with value: 0.9558519399918003 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9069340174364714, 'batch_size': 65, 'attention_heads': 4, 'hidden_dimension': 176, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4184972264557809, 'global_pooling': 'sum', 'learning_rate': 0.0012907927554706986, 'weight_decay': 9.49048471129446e-05, 'beta_0': 0.8144156676117761, 'beta_1': 0.9946506297224307, 'epsilon': 2.641771339771067e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 09:20:51,544] Trial 16 finished with value: 0.9514336064206868 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8832370185709574, 'batch_size': 36, 'attention_heads': 4, 'hidden_dimension': 161, 'number_of_hidden_layers': 1, 'dropout_rate': 0.42462872333668983, 'global_pooling': 'sum', 'learning_rate': 0.00122860622618721, 'weight_decay': 0.00010905195766146757, 'beta_0': 0.8515675401044176, 'beta_1': 0.9957286903556802, 'epsilon': 2.663350180692337e-07, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 408.69 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 40.96 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:32:04,081] Trial 17 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8504471575251773, 'batch_size': 38, 'attention_heads': 7, 'hidden_dimension': 135, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43156852640865934, 'global_pooling': 'sum', 'learning_rate': 0.012518572091498547, 'weight_decay': 7.953225035554188e-05, 'beta_0': 0.8983389870168317, 'beta_1': 0.9975615419419763, 'epsilon': 2.7494376052367026e-07, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 21, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 44.56 GiB of which 126.69 MiB is free. Including non-PyTorch memory, this process has 44.43 GiB memory in use. Of the allocated memory 38.41 GiB is allocated by PyTorch, and 4.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:43:17,885] Trial 18 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8932419987571619, 'batch_size': 65, 'attention_heads': 8, 'hidden_dimension': 160, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41483859277909046, 'global_pooling': 'sum', 'learning_rate': 0.0012115838294488926, 'weight_decay': 1.332619900002374e-05, 'beta_0': 0.8508960916792231, 'beta_1': 0.9945019000418447, 'epsilon': 1.4161448845446074e-07, 'balanced_loss': True, 'epochs': 121, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 536.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 262.69 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.61 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:53:59,437] Trial 19 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8224398675525912, 'batch_size': 70, 'attention_heads': 4, 'hidden_dimension': 108, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49527590330040305, 'global_pooling': 'sum', 'learning_rate': 0.013866125899083374, 'weight_decay': 5.639481675835927e-05, 'beta_0': 0.8595768306567276, 'beta_1': 0.9945781855455501, 'epsilon': 1.1608640886062668e-08, 'balanced_loss': False, 'epochs': 83, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 9}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 10:06:39,592] Trial 20 finished with value: 0.9502719091383403 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8631785466345524, 'batch_size': 61, 'attention_heads': 6, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5191266879553199, 'global_pooling': 'sum', 'learning_rate': 0.0017504262373620922, 'weight_decay': 0.00022187259110746315, 'beta_0': 0.8438910399996036, 'beta_1': 0.9956608925803254, 'epsilon': 4.879076263033234e-07, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 23, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 10:19:03,749] Trial 21 finished with value: 0.9504759124958524 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8671714802450794, 'batch_size': 64, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5160426629834005, 'global_pooling': 'sum', 'learning_rate': 0.0017603506793051522, 'weight_decay': 0.00017218923451216837, 'beta_0': 0.8456995900572255, 'beta_1': 0.9989426509645009, 'epsilon': 4.3900312894840703e-07, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 10:31:28,826] Trial 22 finished with value: 0.9505741941103932 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8841170973359408, 'batch_size': 35, 'attention_heads': 5, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4448513344069146, 'global_pooling': 'sum', 'learning_rate': 0.005607934282020947, 'weight_decay': 0.0001882128407454295, 'beta_0': 0.8568174169176169, 'beta_1': 0.9987684528976996, 'epsilon': 5.10803805684534e-07, 'balanced_loss': False, 'epochs': 118, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 10:46:13,867] Trial 23 finished with value: 0.9539983138824273 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9216707136958865, 'batch_size': 33, 'attention_heads': 5, 'hidden_dimension': 69, 'number_of_hidden_layers': 1, 'dropout_rate': 0.43729236582964387, 'global_pooling': 'sum', 'learning_rate': 0.007876313349317245, 'weight_decay': 6.445775781691864e-05, 'beta_0': 0.8551661734594724, 'beta_1': 0.9929616495295206, 'epsilon': 1.2141082011143717e-07, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 10:58:42,678] Trial 24 finished with value: 0.9384034286754526 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9220866341619044, 'batch_size': 50, 'attention_heads': 4, 'hidden_dimension': 80, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4021451833836418, 'global_pooling': 'sum', 'learning_rate': 0.007207120741431889, 'weight_decay': 5.8260310761555996e-05, 'beta_0': 0.8727807276868152, 'beta_1': 0.9925609947971767, 'epsilon': 1.0747472203893285e-07, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 6}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 11:11:26,361] Trial 25 finished with value: 0.8963875382551485 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.963322413567556, 'batch_size': 77, 'attention_heads': 7, 'hidden_dimension': 124, 'number_of_hidden_layers': 1, 'dropout_rate': 0.45249511639276696, 'global_pooling': 'sum', 'learning_rate': 0.0389976795901964, 'weight_decay': 1.9630815772803804e-05, 'beta_0': 0.8247012855944147, 'beta_1': 0.9927751283313769, 'epsilon': 2.711495434773286e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 11:24:55,963] Trial 26 finished with value: 0.8786230027173582 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9408769923214684, 'batch_size': 111, 'attention_heads': 5, 'hidden_dimension': 153, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3564376905446138, 'global_pooling': 'sum', 'learning_rate': 0.032698315381756014, 'weight_decay': 5.3115522991395093e-05, 'beta_0': 0.8570653954727818, 'beta_1': 0.9943285677652125, 'epsilon': 1.5126214883240803e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 662.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 262.69 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.48 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 11:34:43,343] Trial 27 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8101054846093967, 'batch_size': 81, 'attention_heads': 8, 'hidden_dimension': 172, 'number_of_hidden_layers': 2, 'dropout_rate': 0.434187321605497, 'global_pooling': 'sum', 'learning_rate': 0.0008587923267411983, 'weight_decay': 7.98774234956142e-05, 'beta_0': 0.8428988896177093, 'beta_1': 0.9961331881350994, 'epsilon': 7.196184631576938e-07, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 11:48:54,724] Trial 28 finished with value: 0.9161523507500458 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9197063674646984, 'batch_size': 50, 'attention_heads': 6, 'hidden_dimension': 63, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3901049063821286, 'global_pooling': 'sum', 'learning_rate': 0.007543947245747004, 'weight_decay': 0.0004799609904139147, 'beta_0': 0.8885964186207054, 'beta_1': 0.9909338006389286, 'epsilon': 2.328037090502271e-06, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 44.56 GiB of which 670.69 MiB is free. Including non-PyTorch memory, this process has 43.90 GiB memory in use. Of the allocated memory 40.35 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:00:56,282] Trial 29 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.878546744244989, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 114, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3663756640200222, 'global_pooling': 'sum', 'learning_rate': 0.001182300477320937, 'weight_decay': 1.7860801319313403e-05, 'beta_0': 0.851947317570507, 'beta_1': 0.9911943270335606, 'epsilon': 2.266371733042925e-08, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 44.56 GiB of which 410.69 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 40.24 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 12:12:15,508] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8427224228953123, 'batch_size': 115, 'attention_heads': 4, 'hidden_dimension': 136, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4110181214357908, 'global_pooling': 'sum', 'learning_rate': 0.02535487234192166, 'weight_decay': 0.0008585001898257058, 'beta_0': 0.829689701935632, 'beta_1': 0.9928245806932903, 'epsilon': 7.738907033234592e-08, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 7}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 12:24:55,660] Trial 31 finished with value: 0.953927412200013 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8860350335674559, 'batch_size': 50, 'attention_heads': 5, 'hidden_dimension': 81, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4445507607131097, 'global_pooling': 'sum', 'learning_rate': 0.004773189173051661, 'weight_decay': 0.00021581721538364236, 'beta_0': 0.857704317752747, 'beta_1': 0.9988026277299106, 'epsilon': 6.544607047449322e-07, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 12:37:32,935] Trial 32 finished with value: 0.9350828219754987 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9065538102577688, 'batch_size': 51, 'attention_heads': 5, 'hidden_dimension': 89, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4715911927424736, 'global_pooling': 'sum', 'learning_rate': 0.010548402940511736, 'weight_decay': 0.00032163775444513164, 'beta_0': 0.8643630387442194, 'beta_1': 0.997689391243231, 'epsilon': 2.0855457724619034e-07, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 12:49:34,235] Trial 33 finished with value: 0.9515818758102297 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9549857983424763, 'batch_size': 55, 'attention_heads': 5, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4591089871578954, 'global_pooling': 'sum', 'learning_rate': 0.0032089340915160067, 'weight_decay': 8.95229579984591e-05, 'beta_0': 0.8498974159914292, 'beta_1': 0.9955468876522827, 'epsilon': 7.934421609407894e-07, 'balanced_loss': False, 'epochs': 68, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 13:01:23,076] Trial 34 finished with value: 0.9485996449462405 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9684704369841232, 'batch_size': 82, 'attention_heads': 6, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.45259574161332816, 'global_pooling': 'sum', 'learning_rate': 0.0035193176992940594, 'weight_decay': 0.00024504167019183555, 'beta_0': 0.8723325988801616, 'beta_1': 0.9938145498949765, 'epsilon': 1.5248181395328212e-06, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 13:13:24,222] Trial 35 finished with value: 0.9381314717891005 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9400657889248386, 'batch_size': 166, 'attention_heads': 5, 'hidden_dimension': 91, 'number_of_hidden_layers': 0, 'dropout_rate': 0.503654880537339, 'global_pooling': 'sum', 'learning_rate': 0.020449787539403946, 'weight_decay': 4.326136289997957e-05, 'beta_0': 0.840531878809646, 'beta_1': 0.9967270970221576, 'epsilon': 8.422064301472767e-07, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 13:25:23,772] Trial 36 finished with value: 0.9469357142552788 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9492497237788468, 'batch_size': 55, 'attention_heads': 7, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4675446697293665, 'global_pooling': 'sum', 'learning_rate': 0.0045719532066028894, 'weight_decay': 8.048068326984431e-05, 'beta_0': 0.8606919626898349, 'beta_1': 0.9978114603174845, 'epsilon': 2.372174102306799e-06, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 13:37:10,306] Trial 37 finished with value: 0.9493559454720135 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9790878972913846, 'batch_size': 80, 'attention_heads': 9, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.46447394481516946, 'global_pooling': 'max', 'learning_rate': 0.0020807141930053617, 'weight_decay': 0.0003855504840051275, 'beta_0': 0.8476238706867815, 'beta_1': 0.9956274390779625, 'epsilon': 4.90886031880694e-06, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 15, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 15 with value: 0.9558519399918003.
CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 222.69 MiB is free. Including non-PyTorch memory, this process has 44.34 GiB memory in use. Of the allocated memory 42.77 GiB is allocated by PyTorch, and 424.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:42:13,880] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.6614904218473883, 'batch_size': 110, 'attention_heads': 5, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5477473510094332, 'global_pooling': 'mean', 'learning_rate': 0.0005654083673883879, 'weight_decay': 0.00014107750680977726, 'beta_0': 0.8313258780206864, 'beta_1': 0.988549560289174, 'epsilon': 8.974775070817822e-06, 'balanced_loss': True, 'epochs': 50, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 14:01:19,928] Trial 39 finished with value: 0.9547465307264451 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9246933000862712, 'batch_size': 46, 'attention_heads': 16, 'hidden_dimension': 87, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43875967659806875, 'global_pooling': 'sum', 'learning_rate': 0.00014196446215295814, 'weight_decay': 4.744861904960802e-06, 'beta_0': 0.8540366251222846, 'beta_1': 0.993558980018149, 'epsilon': 2.1188740349833597e-06, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 15 with value: 0.9558519399918003.
[I 2024-12-18 14:17:07,303] Trial 40 finished with value: 0.9568162959944471 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9153939638077027, 'batch_size': 72, 'attention_heads': 16, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4408620431887175, 'global_pooling': 'max', 'learning_rate': 0.00011926356981208386, 'weight_decay': 3.308964411903598e-06, 'beta_0': 0.8783887281828445, 'beta_1': 0.9907684832922293, 'epsilon': 1.971344568771825e-05, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 14:31:42,432] Trial 41 finished with value: 0.9556058371500377 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9184396643733024, 'batch_size': 46, 'attention_heads': 16, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.439578518084554, 'global_pooling': 'max', 'learning_rate': 0.0001361693490459697, 'weight_decay': 3.4073427425321834e-06, 'beta_0': 0.87900181809089, 'beta_1': 0.9903557518622905, 'epsilon': 7.766966902691868e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 14:47:43,251] Trial 42 finished with value: 0.9532215795568608 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9212226669164638, 'batch_size': 70, 'attention_heads': 16, 'hidden_dimension': 98, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4104407478112266, 'global_pooling': 'max', 'learning_rate': 0.00013524336174745125, 'weight_decay': 2.7995857362463673e-06, 'beta_0': 0.8828038633888136, 'beta_1': 0.9902918871493361, 'epsilon': 6.969206515844011e-05, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 43.20 GiB memory in use. Of the allocated memory 38.76 GiB is allocated by PyTorch, and 3.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 14:56:49,251] Trial 43 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9106950784677605, 'batch_size': 44, 'attention_heads': 16, 'hidden_dimension': 120, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4386060805337867, 'global_pooling': 'max', 'learning_rate': 2.5313826107055985e-05, 'weight_decay': 5.327799536500063e-06, 'beta_0': 0.8935578624932199, 'beta_1': 0.988958551873151, 'epsilon': 2.9101614021336117e-05, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 15:09:58,687] Trial 44 finished with value: 0.9402966280617999 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9774660037414785, 'batch_size': 90, 'attention_heads': 14, 'hidden_dimension': 197, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42169993706230785, 'global_pooling': 'max', 'learning_rate': 0.00013300130318555104, 'weight_decay': 1.6998255522793633e-06, 'beta_0': 0.8698987808856579, 'beta_1': 0.9920166863663997, 'epsilon': 2.0099492467725066e-05, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.64 GiB is free. Including non-PyTorch memory, this process has 42.91 GiB memory in use. Of the allocated memory 37.59 GiB is allocated by PyTorch, and 4.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:19:05,299] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9022193309746344, 'batch_size': 62, 'attention_heads': 15, 'hidden_dimension': 99, 'number_of_hidden_layers': 1, 'dropout_rate': 0.33026776304029026, 'global_pooling': 'max', 'learning_rate': 0.00020641727099292764, 'weight_decay': 4.768969728318131e-06, 'beta_0': 0.8754508510530002, 'beta_1': 0.9936311989685075, 'epsilon': 9.871617325475754e-05, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 662.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 546.69 MiB is free. Including non-PyTorch memory, this process has 44.02 GiB memory in use. Of the allocated memory 42.19 GiB is allocated by PyTorch, and 693.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:26:56,969] Trial 46 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7974840104399413, 'batch_size': 32, 'attention_heads': 13, 'hidden_dimension': 228, 'number_of_hidden_layers': 0, 'dropout_rate': 0.48394728978808305, 'global_pooling': 'max', 'learning_rate': 7.753965975437792e-05, 'weight_decay': 8.060019611100969e-06, 'beta_0': 0.8787793835126431, 'beta_1': 0.9916704040472804, 'epsilon': 3.6441481893298424e-05, 'balanced_loss': False, 'epochs': 77, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 43.49 GiB memory in use. Of the allocated memory 40.19 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:36:22,068] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9279621059768717, 'batch_size': 126, 'attention_heads': 15, 'hidden_dimension': 87, 'number_of_hidden_layers': 1, 'dropout_rate': 0.38027468283142096, 'global_pooling': 'max', 'learning_rate': 2.2695526765516405e-05, 'weight_decay': 2.210023198492429e-06, 'beta_0': 0.8661367070370827, 'beta_1': 0.9901003664993316, 'epsilon': 1.1975258981710053e-05, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 21, 'plateau_patience': 21, 'plateau_divider': 5}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 1.61 GiB. GPU 0 has a total capacity of 44.56 GiB of which 882.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 39.49 GiB is allocated by PyTorch, and 3.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:45:26,952] Trial 48 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8391206618341164, 'batch_size': 44, 'attention_heads': 16, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4038935104813002, 'global_pooling': 'max', 'learning_rate': 0.00035132787048054904, 'weight_decay': 3.315632554497797e-06, 'beta_0': 0.8007159752103736, 'beta_1': 0.9873123585554044, 'epsilon': 4.372482462941653e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 25, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 702.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 402.69 MiB is free. Including non-PyTorch memory, this process has 44.16 GiB memory in use. Of the allocated memory 42.34 GiB is allocated by PyTorch, and 683.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:51:35,510] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7279952303073403, 'batch_size': 101, 'attention_heads': 13, 'hidden_dimension': 46, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4323574732468764, 'global_pooling': 'mean', 'learning_rate': 7.621275142757395e-05, 'weight_decay': 1.4901610319939457e-06, 'beta_0': 0.8082985711350413, 'beta_1': 0.9932740390816465, 'epsilon': 2.20024402409114e-05, 'balanced_loss': False, 'epochs': 84, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 5.46 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.44 GiB is free. Including non-PyTorch memory, this process has 40.11 GiB memory in use. Of the allocated memory 37.19 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:00:39,373] Trial 50 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.865601759763067, 'batch_size': 74, 'attention_heads': 15, 'hidden_dimension': 205, 'number_of_hidden_layers': 1, 'dropout_rate': 0.47624137299407304, 'global_pooling': 'max', 'learning_rate': 4.77433498081061e-05, 'weight_decay': 7.187311098266255e-06, 'beta_0': 0.8888746887386411, 'beta_1': 0.9893902431702299, 'epsilon': 9.170917097808649e-05, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 21, 'plateau_patience': 23, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 2.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 43.38 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:11:20,771] Trial 51 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8924106476808221, 'batch_size': 44, 'attention_heads': 16, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.44390706769280924, 'global_pooling': 'max', 'learning_rate': 0.00019849081792330524, 'weight_decay': 4.163483483648386e-06, 'beta_0': 0.879064255770901, 'beta_1': 0.9903025085204025, 'epsilon': 1.8989271045021023e-06, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 2.91 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 43.24 GiB memory in use. Of the allocated memory 37.49 GiB is allocated by PyTorch, and 4.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:21:59,759] Trial 52 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9103957681909902, 'batch_size': 58, 'attention_heads': 14, 'hidden_dimension': 110, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4201066849566908, 'global_pooling': 'mean', 'learning_rate': 0.00037974809501346136, 'weight_decay': 2.5639070678021327e-05, 'beta_0': 0.8559141258610212, 'beta_1': 0.9919419298748423, 'epsilon': 6.98159566760094e-08, 'balanced_loss': False, 'epochs': 122, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 16:37:16,303] Trial 53 finished with value: 0.936995242243005 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8930162700761926, 'batch_size': 45, 'attention_heads': 11, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5976729747728587, 'global_pooling': 'sum', 'learning_rate': 0.00011138991482925692, 'weight_decay': 1.2032756462061342e-05, 'beta_0': 0.862342213285342, 'beta_1': 0.9880334407938819, 'epsilon': 5.819436698413972e-05, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 40 with value: 0.9568162959944471.
CUDA out of memory. Tried to allocate 2.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 42.52 GiB memory in use. Of the allocated memory 37.99 GiB is allocated by PyTorch, and 3.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:48:40,358] Trial 54 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8738805109100506, 'batch_size': 87, 'attention_heads': 14, 'hidden_dimension': 95, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4494217811105718, 'global_pooling': 'max', 'learning_rate': 0.0006026270090957047, 'weight_decay': 2.480542264340894e-06, 'beta_0': 0.8678075997641136, 'beta_1': 0.9908630075816452, 'epsilon': 3.3328727646210295e-06, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 17:11:35,298] Trial 55 finished with value: 0.9297728809298424 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9312819196192433, 'batch_size': 67, 'attention_heads': 16, 'hidden_dimension': 80, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4947677287195525, 'global_pooling': 'sum', 'learning_rate': 1.1977488823465632e-05, 'weight_decay': 1.2384821719223173e-06, 'beta_0': 0.8541434270274341, 'beta_1': 0.994091366388261, 'epsilon': 6.918771426508608e-06, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 21, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 40 with value: 0.9568162959944471.
[I 2024-12-18 17:28:10,447] Trial 56 finished with value: 0.9682403888722344 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9513398866560276, 'batch_size': 152, 'attention_heads': 15, 'hidden_dimension': 144, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42836394745605977, 'global_pooling': 'sum', 'learning_rate': 0.0002976881660738593, 'weight_decay': 6.3223820213965085e-06, 'beta_0': 0.8357432460308338, 'beta_1': 0.9811900582045325, 'epsilon': 1.7765707792660108e-07, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 17:49:01,294] Trial 57 finished with value: 0.9010297214146964 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9846343143341288, 'batch_size': 172, 'attention_heads': 15, 'hidden_dimension': 146, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3978193023151379, 'global_pooling': 'max', 'learning_rate': 0.000318701961778627, 'weight_decay': 3.4227151907413136e-06, 'beta_0': 0.8157517509545821, 'beta_1': 0.9863244766764323, 'epsilon': 1.6165299714897625e-07, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 56 with value: 0.9682403888722344.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 908.69 MiB is free. Including non-PyTorch memory, this process has 43.67 GiB memory in use. Of the allocated memory 40.89 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 17:57:54,074] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9512820736279414, 'batch_size': 152, 'attention_heads': 13, 'hidden_dimension': 171, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4281384554944816, 'global_pooling': 'sum', 'learning_rate': 0.00010048717678802071, 'weight_decay': 5.513047275004474e-06, 'beta_0': 0.8231616467452809, 'beta_1': 0.98245877267087, 'epsilon': 3.814796350429645e-07, 'balanced_loss': True, 'epochs': 146, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 56 with value: 0.9682403888722344.
CUDA out of memory. Tried to allocate 3.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.53 GiB is free. Including non-PyTorch memory, this process has 42.03 GiB memory in use. Of the allocated memory 38.56 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 18:09:05,036] Trial 59 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.962085448269658, 'batch_size': 190, 'attention_heads': 10, 'hidden_dimension': 185, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41025848269387927, 'global_pooling': 'mean', 'learning_rate': 0.00017737987790162417, 'weight_decay': 1.3073323716757222e-05, 'beta_0': 0.811226458179905, 'beta_1': 0.9809192573242826, 'epsilon': 9.817617595841197e-08, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 18:21:28,091] Trial 60 finished with value: 0.914653277906994 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9955849451206114, 'batch_size': 155, 'attention_heads': 16, 'hidden_dimension': 136, 'number_of_hidden_layers': 0, 'dropout_rate': 0.384844249591743, 'global_pooling': 'sum', 'learning_rate': 0.00046061693080757263, 'weight_decay': 9.862354477479913e-06, 'beta_0': 0.8343610801258494, 'beta_1': 0.9945381761906825, 'epsilon': 5.947606982747762e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 18:36:41,335] Trial 61 finished with value: 0.954493805084587 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9340415567271726, 'batch_size': 38, 'attention_heads': 15, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4373675089294868, 'global_pooling': 'sum', 'learning_rate': 0.00016159770534001527, 'weight_decay': 6.193928047290109e-06, 'beta_0': 0.8825639930341282, 'beta_1': 0.9949225957776715, 'epsilon': 5.827588851122287e-07, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 18:53:17,996] Trial 62 finished with value: 0.923382826101139 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9378029557803876, 'batch_size': 135, 'attention_heads': 15, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.45912176411631433, 'global_pooling': 'sum', 'learning_rate': 4.997327356575639e-05, 'weight_decay': 5.852441979744064e-06, 'beta_0': 0.8893725814371677, 'beta_1': 0.9950713395834502, 'epsilon': 2.401927169841198e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 19:09:11,707] Trial 63 finished with value: 0.9482935485063222 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.914880859252985, 'batch_size': 40, 'attention_heads': 14, 'hidden_dimension': 103, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4369869017247296, 'global_pooling': 'sum', 'learning_rate': 0.00026777893798158995, 'weight_decay': 6.751227781062196e-06, 'beta_0': 0.8833055584369568, 'beta_1': 0.9928694091216517, 'epsilon': 3.2770837707572364e-07, 'balanced_loss': False, 'epochs': 134, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 56 with value: 0.9682403888722344.
CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.11 GiB is free. Including non-PyTorch memory, this process has 42.44 GiB memory in use. Of the allocated memory 36.39 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:20:24,890] Trial 64 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8989230843733507, 'batch_size': 244, 'attention_heads': 16, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4206737410667691, 'global_pooling': 'sum', 'learning_rate': 0.00016116020060977395, 'weight_decay': 3.8689073786105005e-06, 'beta_0': 0.8756550277287977, 'beta_1': 0.9949475077570206, 'epsilon': 5.637924562360637e-07, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 19:37:03,561] Trial 65 finished with value: 0.9191568639340186 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9274588507731321, 'batch_size': 217, 'attention_heads': 15, 'hidden_dimension': 52, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4390458343742752, 'global_pooling': 'sum', 'learning_rate': 0.0002487646990154992, 'weight_decay': 1.8199046739833528e-06, 'beta_0': 0.8990668569329813, 'beta_1': 0.9963838097210417, 'epsilon': 1.2013524615590827e-07, 'balanced_loss': False, 'epochs': 126, 'early_stopping_patience': 21, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 19:55:19,515] Trial 66 finished with value: 0.910539957037051 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.944194258341522, 'batch_size': 59, 'attention_heads': 12, 'hidden_dimension': 65, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4591699153862068, 'global_pooling': 'sum', 'learning_rate': 3.41951458621076e-05, 'weight_decay': 9.709111410822496e-06, 'beta_0': 0.894088871231821, 'beta_1': 0.9933922430180674, 'epsilon': 3.976441343570728e-08, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 20:09:21,365] Trial 67 finished with value: 0.9472902034416866 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9674560781494719, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 218, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42902099727917575, 'global_pooling': 'sum', 'learning_rate': 0.0009137609227750562, 'weight_decay': 2.760036874650898e-06, 'beta_0': 0.8379056948144722, 'beta_1': 0.9921963682876193, 'epsilon': 1.872603870455712e-07, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 56 with value: 0.9682403888722344.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 43.07 GiB memory in use. Of the allocated memory 40.45 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:20:24,029] Trial 68 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9533382735626349, 'batch_size': 159, 'attention_heads': 14, 'hidden_dimension': 164, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36979031262675305, 'global_pooling': 'sum', 'learning_rate': 8.560795460082442e-05, 'weight_decay': 2.3872470603401108e-05, 'beta_0': 0.8276377271646306, 'beta_1': 0.9912304703635588, 'epsilon': 2.863210796482364e-07, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 56 with value: 0.9682403888722344.
CUDA out of memory. Tried to allocate 4.03 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.71 GiB is free. Including non-PyTorch memory, this process has 40.85 GiB memory in use. Of the allocated memory 37.85 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:31:20,308] Trial 69 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9187630059509803, 'batch_size': 189, 'attention_heads': 15, 'hidden_dimension': 127, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4150294687609504, 'global_pooling': 'sum', 'learning_rate': 6.439726192201179e-05, 'weight_decay': 6.719094560231448e-05, 'beta_0': 0.818956956532514, 'beta_1': 0.9892896839690766, 'epsilon': 1.1333527042702922e-06, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 20:46:56,589] Trial 70 finished with value: 0.9549965699534855 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9379873986512397, 'batch_size': 40, 'attention_heads': 16, 'hidden_dimension': 114, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3943842128347946, 'global_pooling': 'max', 'learning_rate': 0.00014703955126558942, 'weight_decay': 9.964793624526043e-05, 'beta_0': 0.8454392198137385, 'beta_1': 0.9859810822311714, 'epsilon': 4.315207610650046e-07, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 21:02:55,892] Trial 71 finished with value: 0.9564126558759636 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9338620233727417, 'batch_size': 39, 'attention_heads': 16, 'hidden_dimension': 117, 'number_of_hidden_layers': 0, 'dropout_rate': 0.39272566186639535, 'global_pooling': 'max', 'learning_rate': 0.00014584444989364806, 'weight_decay': 4.476938428459692e-05, 'beta_0': 0.8465041120185625, 'beta_1': 0.9837299141427769, 'epsilon': 4.809839692825406e-07, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 21:19:55,271] Trial 72 finished with value: 0.9492207715152999 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9349906958164026, 'batch_size': 40, 'attention_heads': 16, 'hidden_dimension': 145, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3946681460462954, 'global_pooling': 'max', 'learning_rate': 0.00014990601537498741, 'weight_decay': 4.242461794008189e-05, 'beta_0': 0.84191992941327, 'beta_1': 0.983848992537824, 'epsilon': 4.4180084447700003e-07, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 21:35:47,563] Trial 73 finished with value: 0.9507690777378744 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9442366293009056, 'batch_size': 53, 'attention_heads': 16, 'hidden_dimension': 117, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3539227260658086, 'global_pooling': 'max', 'learning_rate': 0.00010109290110858811, 'weight_decay': 3.2969899190644106e-05, 'beta_0': 0.8444474120875617, 'beta_1': 0.9819027329130391, 'epsilon': 1.011656789190005e-06, 'balanced_loss': False, 'epochs': 55, 'early_stopping_patience': 24, 'plateau_patience': 12, 'plateau_divider': 4}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 21:51:12,879] Trial 74 finished with value: 0.9549092399343027 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9671932778375946, 'batch_size': 74, 'attention_heads': 15, 'hidden_dimension': 105, 'number_of_hidden_layers': 0, 'dropout_rate': 0.37180269030072616, 'global_pooling': 'max', 'learning_rate': 5.760094192190666e-05, 'weight_decay': 0.00010494212692628838, 'beta_0': 0.8472582538989946, 'beta_1': 0.9838650168219127, 'epsilon': 1.4248974357164995e-06, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 3}. Best is trial 56 with value: 0.9682403888722344.
[I 2024-12-18 22:04:58,974] Trial 75 finished with value: 0.933842762038827 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9834564747149602, 'batch_size': 72, 'attention_heads': 15, 'hidden_dimension': 104, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3662840737151787, 'global_pooling': 'max', 'learning_rate': 5.876373323262232e-05, 'weight_decay': 0.0001031054678981767, 'beta_0': 0.8387155860100061, 'beta_1': 0.9832037828534536, 'epsilon': 3.1061657766672053e-06, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 56 with value: 0.9682403888722344.
slurmstepd: error: *** JOB 14110885 ON gpu014 CANCELLED AT 2024-12-18T22:15:44 DUE TO TIME LIMIT ***
