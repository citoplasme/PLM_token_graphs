[I 2024-12-15 04:00:46,197] Using an existing study with name 'R8-GATv2-facebook-bart-large-Grouped-No_Aggregation' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 1.63 GiB. GPU 0 has a total capacity of 44.56 GiB of which 950.69 MiB is free. Including non-PyTorch memory, this process has 43.62 GiB memory in use. Of the allocated memory 41.81 GiB is allocated by PyTorch, and 679.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-15 04:08:47,393] Trial 237 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7066249599767848, 'batch_size': 42, 'attention_heads': 16, 'hidden_dimension': 94, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5848398348422337, 'global_pooling': 'max', 'learning_rate': 2.2570767694183132e-05, 'weight_decay': 6.815851727997351e-06, 'beta_0': 0.8850680414949327, 'beta_1': 0.9895347053754959, 'epsilon': 5.0352922046728765e-05, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 04:59:53,013] Trial 238 finished with value: 0.9603561918658511 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7232500880035639, 'batch_size': 45, 'attention_heads': 16, 'hidden_dimension': 81, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5489515974533946, 'global_pooling': 'max', 'learning_rate': 1.8187900890460363e-05, 'weight_decay': 0.00046066806632596573, 'beta_0': 0.8837055344821462, 'beta_1': 0.9881466100992763, 'epsilon': 4.5831865469121454e-05, 'balanced_loss': True, 'epochs': 132, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 05:39:43,355] Trial 239 finished with value: 0.9671319784498281 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7123432197446291, 'batch_size': 36, 'attention_heads': 16, 'hidden_dimension': 91, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5588781965559758, 'global_pooling': 'max', 'learning_rate': 2.6240095084446382e-05, 'weight_decay': 0.0001908971370283969, 'beta_0': 0.8813583692076622, 'beta_1': 0.9912524574821889, 'epsilon': 2.716734876424798e-05, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 06:32:35,660] Trial 240 finished with value: 0.9744387638986622 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7319612792289817, 'batch_size': 41, 'attention_heads': 15, 'hidden_dimension': 76, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5663735545454568, 'global_pooling': 'max', 'learning_rate': 1.4410696480671797e-05, 'weight_decay': 0.00032925488989963826, 'beta_0': 0.8878634836250473, 'beta_1': 0.9888292334292962, 'epsilon': 5.8276986710373966e-05, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 07:17:27,046] Trial 241 finished with value: 0.9446550912779237 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7321098233069249, 'batch_size': 40, 'attention_heads': 16, 'hidden_dimension': 71, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5685450900269834, 'global_pooling': 'max', 'learning_rate': 1.239877456480672e-05, 'weight_decay': 0.00033164803660461586, 'beta_0': 0.8935935212982321, 'beta_1': 0.989793355912885, 'epsilon': 3.639587179310136e-05, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 07:33:19,901] Trial 242 finished with value: 0.954382660116323 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7389028804803149, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 76, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5802145766165365, 'global_pooling': 'max', 'learning_rate': 0.00028912644384523044, 'weight_decay': 7.613335110014574e-06, 'beta_0': 0.887550573838609, 'beta_1': 0.9883731612176867, 'epsilon': 5.737796957666719e-05, 'balanced_loss': True, 'epochs': 147, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 08:19:33,074] Trial 243 finished with value: 0.9616828922991772 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7293991843152884, 'batch_size': 51, 'attention_heads': 15, 'hidden_dimension': 80, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5654562373805254, 'global_pooling': 'max', 'learning_rate': 1.4400496509479326e-05, 'weight_decay': 3.7670184732977847e-06, 'beta_0': 0.8898013542959275, 'beta_1': 0.9886558727066309, 'epsilon': 4.115600875143683e-05, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 09:03:33,749] Trial 244 finished with value: 0.9637650223419446 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7199650849175325, 'batch_size': 36, 'attention_heads': 16, 'hidden_dimension': 72, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5325805944551565, 'global_pooling': 'max', 'learning_rate': 1.5038315673151097e-05, 'weight_decay': 0.00028436815400352007, 'beta_0': 0.8875461672447205, 'beta_1': 0.9890537827614901, 'epsilon': 3.179371198985155e-05, 'balanced_loss': True, 'epochs': 111, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 09:55:34,482] Trial 245 finished with value: 0.9072730569541998 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7277496840750456, 'batch_size': 41, 'attention_heads': 16, 'hidden_dimension': 86, 'number_of_hidden_layers': 3, 'dropout_rate': 0.573803323301522, 'global_pooling': 'max', 'learning_rate': 1.1543297664020864e-05, 'weight_decay': 0.00023166739584525768, 'beta_0': 0.8800853353416203, 'beta_1': 0.9907621296708559, 'epsilon': 5.196401490840413e-05, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 10:45:27,529] Trial 246 finished with value: 0.9777375558728941 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.718158177038049, 'batch_size': 45, 'attention_heads': 15, 'hidden_dimension': 76, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5547289358307905, 'global_pooling': 'max', 'learning_rate': 2.041796277124419e-05, 'weight_decay': 0.00037989565588482886, 'beta_0': 0.8846792013954715, 'beta_1': 0.992306505237816, 'epsilon': 6.887576756892103e-05, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 11:31:02,333] Trial 247 finished with value: 0.9635488272971936 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7195525403130558, 'batch_size': 47, 'attention_heads': 16, 'hidden_dimension': 77, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5445544896279111, 'global_pooling': 'max', 'learning_rate': 1.891834990843124e-05, 'weight_decay': 0.00038266071924839375, 'beta_0': 0.8852691361952414, 'beta_1': 0.9919943243671193, 'epsilon': 6.927922302271367e-05, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 12:27:19,451] Trial 248 finished with value: 0.9660935890399336 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7318878085718846, 'batch_size': 38, 'attention_heads': 16, 'hidden_dimension': 81, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5521666773621553, 'global_pooling': 'max', 'learning_rate': 1.6654200635923616e-05, 'weight_decay': 0.0003361442231224952, 'beta_0': 0.8910233344288673, 'beta_1': 0.9916781397588984, 'epsilon': 6.326297326870622e-05, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 13:03:13,774] Trial 249 finished with value: 0.9752289526434099 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7239310033826075, 'batch_size': 42, 'attention_heads': 15, 'hidden_dimension': 75, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5613631436587618, 'global_pooling': 'max', 'learning_rate': 3.13380619210802e-05, 'weight_decay': 5.3885944193067264e-06, 'beta_0': 0.8821496326172173, 'beta_1': 0.9828822742512917, 'epsilon': 8.70631366332366e-05, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 13:29:58,721] Trial 250 finished with value: 0.9623336371502919 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7236279765218641, 'batch_size': 44, 'attention_heads': 15, 'hidden_dimension': 69, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5638555554367363, 'global_pooling': 'max', 'learning_rate': 3.5966973583385956e-05, 'weight_decay': 5.400803691842473e-06, 'beta_0': 0.8817830820296589, 'beta_1': 0.9831024441978964, 'epsilon': 1.8415135511308838e-07, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 14:01:02,720] Trial 251 finished with value: 0.9628277538139437 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7138496275524526, 'batch_size': 50, 'attention_heads': 15, 'hidden_dimension': 74, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5601868019876433, 'global_pooling': 'max', 'learning_rate': 2.9295290631360425e-05, 'weight_decay': 6.356155328118251e-06, 'beta_0': 0.8843678010354782, 'beta_1': 0.9828957543154209, 'epsilon': 8.102646947583139e-08, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 14:34:23,935] Trial 252 finished with value: 0.9637634862213686 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6973570878599766, 'batch_size': 41, 'attention_heads': 15, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5679075619777219, 'global_pooling': 'max', 'learning_rate': 3.362794893045723e-05, 'weight_decay': 4.783612577885065e-06, 'beta_0': 0.886393817325838, 'beta_1': 0.9923328363890018, 'epsilon': 9.467182838882415e-05, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 15:00:36,524] Trial 253 finished with value: 0.9685883541083069 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7188827938882114, 'batch_size': 36, 'attention_heads': 15, 'hidden_dimension': 77, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5249964496626369, 'global_pooling': 'max', 'learning_rate': 4.758282139047272e-05, 'weight_decay': 5.520393901245244e-06, 'beta_0': 0.8826647567230135, 'beta_1': 0.9829036667414024, 'epsilon': 2.2496860500815642e-05, 'balanced_loss': True, 'epochs': 147, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 15:33:20,415] Trial 254 finished with value: 0.954504659030847 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.704462334670421, 'batch_size': 46, 'attention_heads': 15, 'hidden_dimension': 71, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5560918865775953, 'global_pooling': 'max', 'learning_rate': 2.5519960530407644e-05, 'weight_decay': 4.023748530692082e-06, 'beta_0': 0.8883648880421142, 'beta_1': 0.9913588567665408, 'epsilon': 8.319628283998484e-05, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 213 with value: 0.9803273479163068.
[I 2024-12-15 16:20:53,366] Trial 255 finished with value: 0.9805781782055967 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7253785224029475, 'batch_size': 54, 'attention_heads': 16, 'hidden_dimension': 77, 'number_of_hidden_layers': 3, 'dropout_rate': 0.575229274537238, 'global_pooling': 'max', 'learning_rate': 1.8835708491479414e-05, 'weight_decay': 0.0002107854048164062, 'beta_0': 0.8854740838624714, 'beta_1': 0.9833307171437825, 'epsilon': 9.238312760857475e-07, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 16:55:35,667] Trial 256 finished with value: 0.9608883072908889 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7241782546430445, 'batch_size': 55, 'attention_heads': 16, 'hidden_dimension': 74, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5690592312093344, 'global_pooling': 'max', 'learning_rate': 3.894497098371472e-05, 'weight_decay': 7.895908445574887e-06, 'beta_0': 0.88476933162611, 'beta_1': 0.9836071067214628, 'epsilon': 2.7223415405714896e-05, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 17:38:24,819] Trial 257 finished with value: 0.9670793565677359 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7405838924446517, 'batch_size': 50, 'attention_heads': 16, 'hidden_dimension': 77, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5775824487020418, 'global_pooling': 'max', 'learning_rate': 3.0006610378413195e-05, 'weight_decay': 0.00014722575556038207, 'beta_0': 0.8790644541755793, 'beta_1': 0.9833499628595614, 'epsilon': 8.462167446343858e-05, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 18:19:35,725] Trial 258 finished with value: 0.972691147464144 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6859327833050505, 'batch_size': 44, 'attention_heads': 15, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5731966788174239, 'global_pooling': 'max', 'learning_rate': 2.151885165038343e-05, 'weight_decay': 6.501339704536757e-06, 'beta_0': 0.8820764256185769, 'beta_1': 0.9890830396201429, 'epsilon': 6.957544930850657e-07, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 18:48:00,329] Trial 259 finished with value: 0.9500693059457854 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6887129872466877, 'batch_size': 54, 'attention_heads': 15, 'hidden_dimension': 70, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5736954167560582, 'global_pooling': 'max', 'learning_rate': 2.3661936695799273e-05, 'weight_decay': 0.00017905257385777967, 'beta_0': 0.882830976217023, 'beta_1': 0.9888713783274318, 'epsilon': 9.670141224259697e-07, 'balanced_loss': True, 'epochs': 62, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 19:31:44,870] Trial 260 finished with value: 0.9703956958172308 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.686153208864419, 'batch_size': 46, 'attention_heads': 15, 'hidden_dimension': 64, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5819961629446372, 'global_pooling': 'max', 'learning_rate': 2.0334173945064182e-05, 'weight_decay': 5.0795424883786375e-06, 'beta_0': 0.8813042098406835, 'beta_1': 0.9893510887403861, 'epsilon': 1.2072514429643139e-06, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 19:54:32,833] Trial 261 finished with value: 0.9123444157299819 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7025619563861127, 'batch_size': 43, 'attention_heads': 15, 'hidden_dimension': 61, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5852903970194213, 'global_pooling': 'sum', 'learning_rate': 2.738761163658051e-05, 'weight_decay': 6.034675857900123e-06, 'beta_0': 0.8800627328617845, 'beta_1': 0.9899944519184809, 'epsilon': 7.525634567756128e-05, 'balanced_loss': True, 'epochs': 140, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 20:20:52,507] Trial 262 finished with value: 0.9267515568828204 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.712504987044819, 'batch_size': 32, 'attention_heads': 15, 'hidden_dimension': 68, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5763069510018997, 'global_pooling': 'max', 'learning_rate': 3.311368263282021e-05, 'weight_decay': 0.0002937503644295774, 'beta_0': 0.8833491720791405, 'beta_1': 0.9890666264946475, 'epsilon': 1.4006860338948294e-06, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 43.46 GiB memory in use. Of the allocated memory 40.67 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-15 20:47:21,970] Trial 263 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6935248265098142, 'batch_size': 50, 'attention_heads': 15, 'hidden_dimension': 85, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5597043262947932, 'global_pooling': 'max', 'learning_rate': 1.7327890392353237e-05, 'weight_decay': 0.00011933714434204572, 'beta_0': 0.8861181690770347, 'beta_1': 0.9903566136622172, 'epsilon': 4.356773516319171e-05, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 390.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 41.47 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-15 20:55:22,715] Trial 264 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.679742651014013, 'batch_size': 42, 'attention_heads': 16, 'hidden_dimension': 80, 'number_of_hidden_layers': 3, 'dropout_rate': 0.59414095931483, 'global_pooling': 'max', 'learning_rate': 2.2020929935692845e-05, 'weight_decay': 0.00041427574131867325, 'beta_0': 0.8780019082447111, 'beta_1': 0.9925359442929026, 'epsilon': 1.0373800068295728e-06, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 21:29:21,182] Trial 265 finished with value: 0.9680971591537292 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7207528947113335, 'batch_size': 35, 'attention_heads': 16, 'hidden_dimension': 72, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5660043840205348, 'global_pooling': 'max', 'learning_rate': 2.6334666519688568e-05, 'weight_decay': 0.00020694853023355935, 'beta_0': 0.8820386918365506, 'beta_1': 0.989521323534658, 'epsilon': 8.564816210009081e-07, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 43.44 GiB memory in use. Of the allocated memory 39.41 GiB is allocated by PyTorch, and 2.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-15 21:37:21,460] Trial 266 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.714452404319162, 'batch_size': 47, 'attention_heads': 16, 'hidden_dimension': 87, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5743781435706395, 'global_pooling': 'max', 'learning_rate': 5.258191872428569e-05, 'weight_decay': 4.41292188449417e-06, 'beta_0': 0.884702257187654, 'beta_1': 0.9865408435035449, 'epsilon': 7.200463792749451e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 22:08:37,092] Trial 267 finished with value: 0.9695712955487494 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7064770491102209, 'batch_size': 42, 'attention_heads': 15, 'hidden_dimension': 79, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5622324060295159, 'global_pooling': 'max', 'learning_rate': 4.2136275092818985e-05, 'weight_decay': 0.0002497364021603756, 'beta_0': 0.8862960932212673, 'beta_1': 0.9878408891014974, 'epsilon': 2.631142215186998e-06, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 22:42:50,802] Trial 268 finished with value: 0.9648537424465162 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7179256435298901, 'batch_size': 36, 'attention_heads': 16, 'hidden_dimension': 74, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5444491487575759, 'global_pooling': 'max', 'learning_rate': 2.0134184413847014e-05, 'weight_decay': 6.818168843710978e-06, 'beta_0': 0.8809081187517365, 'beta_1': 0.9833493201832965, 'epsilon': 1.7490161021195476e-06, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-15 23:17:47,954] Trial 269 finished with value: 0.9746856380518891 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6963460232566566, 'batch_size': 52, 'attention_heads': 16, 'hidden_dimension': 67, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5700138454893052, 'global_pooling': 'max', 'learning_rate': 3.224326232334197e-05, 'weight_decay': 5.701689638562929e-06, 'beta_0': 0.8834841015094549, 'beta_1': 0.9915832867829898, 'epsilon': 3.5568472619208264e-05, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 40.85 GiB is allocated by PyTorch, and 1.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-15 23:44:28,803] Trial 270 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.685081067570356, 'batch_size': 58, 'attention_heads': 16, 'hidden_dimension': 61, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5702594931996241, 'global_pooling': 'max', 'learning_rate': 1.7455171480246057e-05, 'weight_decay': 5.662699442786745e-06, 'beta_0': 0.879390911914956, 'beta_1': 0.9916786070261846, 'epsilon': 3.6367578546546634e-05, 'balanced_loss': True, 'epochs': 133, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 00:26:53,598] Trial 271 finished with value: 0.9666371028029614 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7361343255233236, 'batch_size': 62, 'attention_heads': 16, 'hidden_dimension': 67, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5795073690484599, 'global_pooling': 'max', 'learning_rate': 2.3158471057577426e-05, 'weight_decay': 5.0708078459415264e-06, 'beta_0': 0.8828047920856807, 'beta_1': 0.9912061857322517, 'epsilon': 6.616599252296938e-07, 'balanced_loss': True, 'epochs': 152, 'early_stopping_patience': 13, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 01:19:34,718] Trial 272 finished with value: 0.9268076390414433 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.6925699051589156, 'batch_size': 53, 'attention_heads': 16, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5872023874123445, 'global_pooling': 'max', 'learning_rate': 1.3634683348318524e-05, 'weight_decay': 4.2420465268624335e-06, 'beta_0': 0.8839055023721082, 'beta_1': 0.9907927727524938, 'epsilon': 3.31462896726636e-05, 'balanced_loss': True, 'epochs': 140, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 43.35 GiB memory in use. Of the allocated memory 40.74 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 01:27:39,720] Trial 273 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.727370135416055, 'batch_size': 49, 'attention_heads': 15, 'hidden_dimension': 123, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5678726312157084, 'global_pooling': 'max', 'learning_rate': 2.924605339730635e-05, 'weight_decay': 7.306564879329072e-06, 'beta_0': 0.8812904744655305, 'beta_1': 0.992048772395781, 'epsilon': 4.015622924274635e-05, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 10, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 43.35 GiB memory in use. Of the allocated memory 39.59 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 01:34:46,805] Trial 274 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.697830228005001, 'batch_size': 55, 'attention_heads': 9, 'hidden_dimension': 243, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5530720722642084, 'global_pooling': 'max', 'learning_rate': 2.3609531502950623e-05, 'weight_decay': 6.3091539348931264e-06, 'beta_0': 0.8774710931111069, 'beta_1': 0.9929264795943333, 'epsilon': 2.8094107257770332e-05, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 12, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 02:06:28,139] Trial 275 finished with value: 0.9622198066362448 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7424943976470987, 'batch_size': 46, 'attention_heads': 16, 'hidden_dimension': 73, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5730174454559913, 'global_pooling': 'max', 'learning_rate': 3.724452732842525e-05, 'weight_decay': 0.00016831833210871353, 'beta_0': 0.8872350741267329, 'beta_1': 0.9838366472452431, 'epsilon': 5.380545937859861e-05, 'balanced_loss': True, 'epochs': 149, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 02:31:24,667] Trial 276 finished with value: 0.9563346922017011 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7106220768167457, 'batch_size': 43, 'attention_heads': 15, 'hidden_dimension': 77, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5795629979269041, 'global_pooling': 'max', 'learning_rate': 6.265391509417529e-05, 'weight_decay': 3.469336465975738e-06, 'beta_0': 0.8794574238322557, 'beta_1': 0.9828056077357487, 'epsilon': 3.250082731363303e-05, 'balanced_loss': True, 'epochs': 136, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 348.69 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 39.29 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 02:39:10,223] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7240228233034204, 'batch_size': 51, 'attention_heads': 16, 'hidden_dimension': 83, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5486870586180899, 'global_pooling': 'max', 'learning_rate': 1.7199895207668698e-05, 'weight_decay': 5.7654689359875834e-06, 'beta_0': 0.8833165617566074, 'beta_1': 0.9830010536248602, 'epsilon': 3.2478643940148524e-07, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 03:04:57,252] Trial 278 finished with value: 0.9453051751212118 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7332481776843857, 'batch_size': 41, 'attention_heads': 16, 'hidden_dimension': 63, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5397275237849746, 'global_pooling': 'max', 'learning_rate': 2.8257818067704565e-05, 'weight_decay': 9.443626464419657e-06, 'beta_0': 0.8920309962738239, 'beta_1': 0.9916594909458079, 'epsilon': 4.3580279410198815e-05, 'balanced_loss': True, 'epochs': 86, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 146.69 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 42.10 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 03:12:07,588] Trial 279 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7178132390824045, 'batch_size': 128, 'attention_heads': 16, 'hidden_dimension': 88, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5639507498764849, 'global_pooling': 'max', 'learning_rate': 2.1035354559801147e-05, 'weight_decay': 4.955449902794889e-06, 'beta_0': 0.8852577470506969, 'beta_1': 0.9910787151561634, 'epsilon': 8.556916941485708e-08, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 03:54:34,962] Trial 280 finished with value: 0.9316302028977186 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7269753844413808, 'batch_size': 46, 'attention_heads': 15, 'hidden_dimension': 70, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5550478492740532, 'global_pooling': 'max', 'learning_rate': 1.2150427415878434e-05, 'weight_decay': 0.00022241211487886186, 'beta_0': 0.8811584845390447, 'beta_1': 0.9831601546838183, 'epsilon': 5.2759073475548204e-05, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 04:40:03,204] Trial 281 finished with value: 0.9331646868364855 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7088519513141591, 'batch_size': 39, 'attention_heads': 16, 'hidden_dimension': 75, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5273029503481564, 'global_pooling': 'max', 'learning_rate': 1.5602410868591232e-05, 'weight_decay': 7.613822110883667e-06, 'beta_0': 0.8885228415732901, 'beta_1': 0.9922513777247717, 'epsilon': 3.670111022158224e-05, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 14, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 05:34:49,935] Trial 282 finished with value: 0.970274804297081 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7361271174195796, 'batch_size': 47, 'attention_heads': 16, 'hidden_dimension': 83, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5453865513048491, 'global_pooling': 'max', 'learning_rate': 2.5672582212394223e-05, 'weight_decay': 0.00018517868757974396, 'beta_0': 0.8784512322472694, 'beta_1': 0.9826723127293776, 'epsilon': 9.9110761870382e-05, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.04 GiB is free. Including non-PyTorch memory, this process has 43.51 GiB memory in use. Of the allocated memory 40.98 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 05:57:29,186] Trial 283 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7005058912228087, 'batch_size': 38, 'attention_heads': 16, 'hidden_dimension': 95, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5590122600061247, 'global_pooling': 'sum', 'learning_rate': 3.233237571840175e-05, 'weight_decay': 6.088850920375884e-06, 'beta_0': 0.8739874958022552, 'beta_1': 0.9902556032484205, 'epsilon': 5.356676734128918e-08, 'balanced_loss': True, 'epochs': 113, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 06:45:11,826] Trial 284 finished with value: 0.9601249347890736 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7183906132768294, 'batch_size': 57, 'attention_heads': 15, 'hidden_dimension': 78, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5306315688937897, 'global_pooling': 'max', 'learning_rate': 1.924476108879807e-05, 'weight_decay': 4.5089962853816175e-06, 'beta_0': 0.8833431453627814, 'beta_1': 0.9834801254920293, 'epsilon': 2.5158290124490494e-05, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 07:14:02,324] Trial 285 finished with value: 0.9721029271352379 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7440249568072149, 'batch_size': 43, 'attention_heads': 16, 'hidden_dimension': 88, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5702959935082212, 'global_pooling': 'max', 'learning_rate': 4.3118115776884264e-05, 'weight_decay': 8.1983989986717e-06, 'beta_0': 0.8936548320219909, 'beta_1': 0.9840074464298598, 'epsilon': 1.9266416945715808e-08, 'balanced_loss': True, 'epochs': 146, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 08:01:58,401] Trial 286 finished with value: 0.9666491581853359 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6691057184671524, 'batch_size': 36, 'attention_heads': 16, 'hidden_dimension': 71, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5530941079728837, 'global_pooling': 'max', 'learning_rate': 2.2862549265201697e-05, 'weight_decay': 0.00014242718373167967, 'beta_0': 0.8867123942876513, 'beta_1': 0.9825579301207402, 'epsilon': 6.37366831249966e-05, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.52 GiB. GPU 0 has a total capacity of 44.56 GiB of which 648.69 MiB is free. Including non-PyTorch memory, this process has 43.92 GiB memory in use. Of the allocated memory 39.10 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 08:12:49,516] Trial 287 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.727047744645735, 'batch_size': 52, 'attention_heads': 16, 'hidden_dimension': 80, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5361545612002794, 'global_pooling': 'max', 'learning_rate': 1.6194534578432435e-05, 'weight_decay': 0.00041571839960848685, 'beta_0': 0.881638061144813, 'beta_1': 0.9915040680549575, 'epsilon': 4.5838540167638006e-05, 'balanced_loss': True, 'epochs': 131, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 08:29:31,963] Trial 288 finished with value: 0.9633669029263608 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7144161883276976, 'batch_size': 41, 'attention_heads': 10, 'hidden_dimension': 85, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5421223901774312, 'global_pooling': 'max', 'learning_rate': 0.00019019391685686648, 'weight_decay': 0.00026649965728845897, 'beta_0': 0.8900971084765841, 'beta_1': 0.9872872071911788, 'epsilon': 3.009822089223818e-05, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 08:45:05,660] Trial 289 finished with value: 0.9604465087347911 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7234302284846813, 'batch_size': 48, 'attention_heads': 16, 'hidden_dimension': 67, 'number_of_hidden_layers': 3, 'dropout_rate': 0.592778306332498, 'global_pooling': 'max', 'learning_rate': 0.0004341157342138076, 'weight_decay': 6.883326793851833e-06, 'beta_0': 0.876759654046996, 'beta_1': 0.9908149196726922, 'epsilon': 6.0462128000693024e-05, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 21, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.60 GiB is free. Including non-PyTorch memory, this process has 42.96 GiB memory in use. Of the allocated memory 39.87 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 08:53:05,974] Trial 290 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6827584276784716, 'batch_size': 36, 'attention_heads': 15, 'hidden_dimension': 101, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5647325161100997, 'global_pooling': 'max', 'learning_rate': 3.537749512430341e-05, 'weight_decay': 5.372681953450297e-06, 'beta_0': 0.8846893071222311, 'beta_1': 0.9898475545909868, 'epsilon': 1.0688607145933028e-08, 'balanced_loss': True, 'epochs': 123, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 09:05:57,071] Trial 291 finished with value: 0.08528528528528528 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7322902708992848, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 75, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5487783705312372, 'global_pooling': 'max', 'learning_rate': 0.0154033431805622, 'weight_decay': 0.00019609485315805729, 'beta_0': 0.8805282553014239, 'beta_1': 0.984381497880596, 'epsilon': 2.2098565574364523e-05, 'balanced_loss': True, 'epochs': 109, 'early_stopping_patience': 15, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 44.56 GiB of which 742.69 MiB is free. Including non-PyTorch memory, this process has 43.83 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 09:13:08,059] Trial 292 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.6939443005682758, 'batch_size': 88, 'attention_heads': 15, 'hidden_dimension': 91, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5784341674275474, 'global_pooling': 'max', 'learning_rate': 2.808340413640891e-05, 'weight_decay': 3.923015957335132e-06, 'beta_0': 0.8867980927724446, 'beta_1': 0.9832078309140833, 'epsilon': 3.880749228114353e-05, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 10:07:28,578] Trial 293 finished with value: 0.9407398300358121 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7096268103595689, 'batch_size': 44, 'attention_heads': 16, 'hidden_dimension': 83, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5589493660289748, 'global_pooling': 'max', 'learning_rate': 1.9050115720180412e-05, 'weight_decay': 0.00030872200798026645, 'beta_0': 0.8829999667111187, 'beta_1': 0.9970269614160256, 'epsilon': 8.299569216352571e-05, 'balanced_loss': True, 'epochs': 150, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 10:56:34,895] Trial 294 finished with value: 0.9245603323981906 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7212808242498766, 'batch_size': 40, 'attention_heads': 16, 'hidden_dimension': 78, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5729184664761021, 'global_pooling': 'max', 'learning_rate': 1.001992181451619e-05, 'weight_decay': 0.0003768405713994978, 'beta_0': 0.8849259484201469, 'beta_1': 0.9912115970575541, 'epsilon': 5.0913959323487895e-05, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 41.16 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 11:04:33,613] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7369025641062138, 'batch_size': 51, 'attention_heads': 16, 'hidden_dimension': 113, 'number_of_hidden_layers': 3, 'dropout_rate': 0.41159849104593105, 'global_pooling': 'max', 'learning_rate': 1.4258775840629419e-05, 'weight_decay': 6.227556393981634e-06, 'beta_0': 0.8883299198377661, 'beta_1': 0.9827475595258001, 'epsilon': 3.1740847186071246e-05, 'balanced_loss': True, 'epochs': 112, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 11:30:38,399] Trial 296 finished with value: 0.9680173912681774 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7016736306383407, 'batch_size': 37, 'attention_heads': 16, 'hidden_dimension': 72, 'number_of_hidden_layers': 3, 'dropout_rate': 0.552372823556362, 'global_pooling': 'max', 'learning_rate': 5.020454437538924e-05, 'weight_decay': 0.0002322309366916231, 'beta_0': 0.8789822541445317, 'beta_1': 0.9887273850466265, 'epsilon': 1.7180540212369895e-05, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 22, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 12:14:10,462] Trial 297 finished with value: 0.9420528228549906 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.753951324989443, 'batch_size': 45, 'attention_heads': 15, 'hidden_dimension': 60, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5343725084116516, 'global_pooling': 'max', 'learning_rate': 2.4427758189373553e-05, 'weight_decay': 0.0005123355738198658, 'beta_0': 0.8917129792494918, 'beta_1': 0.9905376198775133, 'epsilon': 8.523504462668058e-07, 'balanced_loss': True, 'epochs': 128, 'early_stopping_patience': 21, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 826.69 MiB is free. Including non-PyTorch memory, this process has 43.75 GiB memory in use. Of the allocated memory 40.62 GiB is allocated by PyTorch, and 1.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 12:46:41,068] Trial 298 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7146875370308867, 'batch_size': 65, 'attention_heads': 16, 'hidden_dimension': 68, 'number_of_hidden_layers': 3, 'dropout_rate': 0.43320923960202595, 'global_pooling': 'max', 'learning_rate': 3.060735238677183e-05, 'weight_decay': 0.00015739325909027097, 'beta_0': 0.8821438763085897, 'beta_1': 0.9823897100660262, 'epsilon': 6.957683234305699e-05, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 13:21:37,731] Trial 299 finished with value: 0.9696645788435698 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7288547857802037, 'batch_size': 40, 'attention_heads': 16, 'hidden_dimension': 80, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5625068525043241, 'global_pooling': 'max', 'learning_rate': 2.0204135921489757e-05, 'weight_decay': 4.781023377954566e-06, 'beta_0': 0.8842325528064439, 'beta_1': 0.9835444405756854, 'epsilon': 4.848811724073483e-07, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 12, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 13:53:42,249] Trial 300 finished with value: 0.9567204655450567 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.722955327360298, 'batch_size': 35, 'attention_heads': 16, 'hidden_dimension': 75, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5408271233476982, 'global_pooling': 'max', 'learning_rate': 3.8480897635963055e-05, 'weight_decay': 5.418417417001492e-06, 'beta_0': 0.8947772966949517, 'beta_1': 0.9925939134022636, 'epsilon': 4.506967089095919e-05, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 14:43:38,337] Trial 301 finished with value: 0.9601552065716792 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7423292645638757, 'batch_size': 49, 'attention_heads': 15, 'hidden_dimension': 87, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4428188000023584, 'global_pooling': 'max', 'learning_rate': 1.3008428473389036e-05, 'weight_decay': 0.00027056617322278055, 'beta_0': 0.8895088596757734, 'beta_1': 0.9830978807342609, 'epsilon': 2.697547935011947e-05, 'balanced_loss': True, 'epochs': 133, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 44.56 GiB of which 280.69 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 40.59 GiB is allocated by PyTorch, and 2.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-16 15:23:20,715] Trial 302 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7078409628234876, 'batch_size': 43, 'attention_heads': 16, 'hidden_dimension': 84, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5463627952602791, 'global_pooling': 'max', 'learning_rate': 2.5572202874089234e-05, 'weight_decay': 3.102352080260228e-06, 'beta_0': 0.8869512247238297, 'beta_1': 0.9916553811556164, 'epsilon': 3.554900056621533e-05, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 22, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.
[I 2024-12-16 16:09:38,399] Trial 303 finished with value: 0.8963155478010553 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.730733909652948, 'batch_size': 39, 'attention_heads': 16, 'hidden_dimension': 92, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5856487475914129, 'global_pooling': 'sum', 'learning_rate': 1.810705418198151e-05, 'weight_decay': 0.00020855896596216062, 'beta_0': 0.8772086495239027, 'beta_1': 0.9819917268510188, 'epsilon': 6.230993514764807e-05, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 255 with value: 0.9805781782055967.

[TRIAL] 255 [VALIDATION PERFORMANCE] 0.9805781782055967 [TRAINING LOSS] 0.04889725018917333 [VALIDATION LOSS] 0.09335861523591336 

number                                     255
value                                 0.980578
params_threshold                      0.725379
params_attention_heads                      16
params_balanced_loss                      True
params_embedding_pooling_operation         max
params_attention_pooling_operation        mean
params_batch_size                           54
params_dropout_rate                   0.575229
params_early_stopping_patience              20
params_epochs                              144
params_global_pooling                      max
params_hidden_dimension                     77
params_learning_rate                  0.000019
params_number_of_hidden_layers               3
params_plateau_divider                       7
params_plateau_patience                     19
params_weight_decay                   0.000211
params_beta_0                         0.885474
params_beta_1                         0.983331
params_epsilon                        0.000001
user_attrs_epoch                          94.0
user_attrs_training_loss              0.048897
user_attrs_validation_loss            0.093359
params_left_stride                          64
params_right_stride                        128
Name: 255, dtype: object
37 Val: 0.9719040419020664 Test: 0.9274919761295998
CUDA out of memory. Tried to allocate 1.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 304.69 MiB is free. Including non-PyTorch memory, this process has 44.26 GiB memory in use. Of the allocated memory 35.82 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
38 Exception...
39 Val: 0.96902957044558 Test: 0.9414366364770479
CUDA out of memory. Tried to allocate 1.66 GiB. GPU 0 has a total capacity of 44.56 GiB of which 792.69 MiB is free. Including non-PyTorch memory, this process has 43.78 GiB memory in use. Of the allocated memory 39.64 GiB is allocated by PyTorch, and 2.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
40 Exception...
CUDA out of memory. Tried to allocate 1.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 632.69 MiB is free. Including non-PyTorch memory, this process has 43.94 GiB memory in use. Of the allocated memory 41.36 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
41 Exception...
42 Val: 0.9802697475825912 Test: 0.9310870800811246
CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 620.69 MiB is free. Including non-PyTorch memory, this process has 43.95 GiB memory in use. Of the allocated memory 40.96 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
43 Exception...
CUDA out of memory. Tried to allocate 1.66 GiB. GPU 0 has a total capacity of 44.56 GiB of which 688.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 41.30 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
44 Exception...
45 Val: 0.9719751769517904 Test: 0.9340724486890046
46 Val: 0.9669949666964257 Test: 0.9368103390476765
Validation performance: 96.7 & 97.2  0.51 & 98.03
Testing performance: 92.75 & 93.42  0.53 & 94.14

[TRIAL] 213 [VALIDATION PERFORMANCE] 0.9803273479163068 [TRAINING LOSS] 0.05818528197084538 [VALIDATION LOSS] 0.10532039879860046 

number                                     213
value                                 0.980327
params_threshold                      0.719007
params_attention_heads                      16
params_balanced_loss                      True
params_embedding_pooling_operation         max
params_attention_pooling_operation        mean
params_batch_size                           37
params_dropout_rate                   0.580391
params_early_stopping_patience              20
params_epochs                              111
params_global_pooling                      max
params_hidden_dimension                     91
params_learning_rate                  0.000046
params_number_of_hidden_layers               3
params_plateau_divider                       7
params_plateau_patience                     19
params_weight_decay                   0.000194
params_beta_0                         0.885745
params_beta_1                         0.989606
params_epsilon                        0.000043
user_attrs_epoch                          39.0
user_attrs_training_loss              0.058185
user_attrs_validation_loss             0.10532
params_left_stride                          64
params_right_stride                        128
Name: 213, dtype: object
37 Val: 0.9733475801787355 Test: 0.9298869807578927
CUDA out of memory. Tried to allocate 1.72 GiB. GPU 0 has a total capacity of 44.56 GiB of which 734.69 MiB is free. Including non-PyTorch memory, this process has 43.84 GiB memory in use. Of the allocated memory 40.74 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
38 Exception...
39 Val: 0.9686589336454801 Test: 0.9408713382026807
CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.42 GiB is free. Including non-PyTorch memory, this process has 43.13 GiB memory in use. Of the allocated memory 39.39 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
40 Exception...
CUDA out of memory. Tried to allocate 1.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 43.07 GiB memory in use. Of the allocated memory 39.37 GiB is allocated by PyTorch, and 2.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
41 Exception...
42 Val: 0.9803273479163068 Test: 0.9381725285772494
43 Val: 0.9685718713865659 Test: 0.9340997303501153
44 Val: 0.9719835024178526 Test: 0.9397475293236481
CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 43.44 GiB memory in use. Of the allocated memory 39.66 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
45 Exception...
46 Val: 0.9654023740491007 Test: 0.9289759342087669
Validation performance: 96.54 & 97.14  0.52 & 98.03
Testing performance: 92.9 & 93.53  0.51 & 94.09

[TRIAL] 246 [VALIDATION PERFORMANCE] 0.9777375558728941 [TRAINING LOSS] 0.0378247824603245 [VALIDATION LOSS] 0.10013040285557509 

number                                     246
value                                 0.977738
params_threshold                      0.718158
params_attention_heads                      15
params_balanced_loss                      True
params_embedding_pooling_operation         max
params_attention_pooling_operation        mean
params_batch_size                           45
params_dropout_rate                   0.554729
params_early_stopping_patience              21
params_epochs                              142
params_global_pooling                      max
params_hidden_dimension                     76
params_learning_rate                   0.00002
params_number_of_hidden_layers               3
params_plateau_divider                       7
params_plateau_patience                     19
params_weight_decay                    0.00038
params_beta_0                         0.884679
params_beta_1                         0.992307
params_epsilon                        0.000069
user_attrs_epoch                         112.0
user_attrs_training_loss              0.037825
user_attrs_validation_loss             0.10013
params_left_stride                          64
params_right_stride                        128
Name: 246, dtype: object
slurmstepd: error: *** JOB 14097824 ON gpu027 CANCELLED AT 2024-12-17T04:00:46 DUE TO TIME LIMIT ***
