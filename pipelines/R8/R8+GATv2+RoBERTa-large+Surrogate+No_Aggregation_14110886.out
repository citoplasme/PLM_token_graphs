Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2024-12-18 06:15:46,853] A new study created in RDB with name: R8-GATv2-FacebookAI-roberta-large-Surrogate-No_Aggregation
Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 94.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 41.29 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:22:12,610] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7564817426134086, 'batch_size': 150, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
[I 2024-12-18 06:37:03,222] Trial 1 finished with value: 0.939648404919641 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9788152345580505, 'batch_size': 233, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 1 with value: 0.939648404919641.
[I 2024-12-18 06:53:20,674] Trial 2 finished with value: 0.9103670364569112 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9605155877742886, 'batch_size': 138, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 908.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 676.69 MiB is free. Including non-PyTorch memory, this process has 43.89 GiB memory in use. Of the allocated memory 41.65 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 06:57:15,803] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7297750275380542, 'batch_size': 128, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 622.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 324.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 41.82 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:02:19,918] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7787204186204115, 'batch_size': 174, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 8.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.57 GiB is free. Including non-PyTorch memory, this process has 41.98 GiB memory in use. Of the allocated memory 39.07 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:12:33,832] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9040772280477951, 'batch_size': 233, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 382.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 354.69 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 42.35 GiB is allocated by PyTorch, and 722.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:20:14,639] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8519105905803794, 'batch_size': 142, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 1 with value: 0.939648404919641.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-18 07:20:16,327] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7758183080154022, 'batch_size': 98, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 898.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 192.69 MiB is free. Including non-PyTorch memory, this process has 44.37 GiB memory in use. Of the allocated memory 42.06 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:30:46,827] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9041247058895944, 'batch_size': 251, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 1 with value: 0.939648404919641.
CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 342.69 MiB is free. Including non-PyTorch memory, this process has 44.22 GiB memory in use. Of the allocated memory 41.16 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 07:37:03,912] Trial 9 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8220606401321402, 'batch_size': 138, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 1 with value: 0.939648404919641.
[I 2024-12-18 07:52:32,804] Trial 10 finished with value: 0.2659857183074364 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9943461369090066, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 48, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3977762266143904, 'global_pooling': 'sum', 'learning_rate': 4.519442811197253e-05, 'weight_decay': 1.391414427795109e-05, 'beta_0': 0.8215105131549126, 'beta_1': 0.9849479143747095, 'epsilon': 5.948652419846486e-07, 'balanced_loss': True, 'epochs': 91, 'early_stopping_patience': 25, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 1 with value: 0.939648404919641.
[I 2024-12-18 08:04:56,129] Trial 11 finished with value: 0.9479004462240216 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9965745266953072, 'batch_size': 205, 'attention_heads': 4, 'hidden_dimension': 254, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5017228311693176, 'global_pooling': 'max', 'learning_rate': 0.00030957021614699733, 'weight_decay': 1.040021777795823e-05, 'beta_0': 0.8085606586988479, 'beta_1': 0.9842238504829, 'epsilon': 2.7144548409427893e-06, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 10, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 11 with value: 0.9479004462240216.
CUDA out of memory. Tried to allocate 736.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 78.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:08:24,020] Trial 12 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.6794678376460593, 'batch_size': 206, 'attention_heads': 4, 'hidden_dimension': 250, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5081699154554011, 'global_pooling': 'max', 'learning_rate': 0.00011292643020568445, 'weight_decay': 9.112084128938665e-06, 'beta_0': 0.8161541958464219, 'beta_1': 0.9847976982734662, 'epsilon': 3.1914768556465596e-06, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 11 with value: 0.9479004462240216.
CUDA out of memory. Tried to allocate 4.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.21 GiB is free. Including non-PyTorch memory, this process has 40.34 GiB memory in use. Of the allocated memory 34.31 GiB is allocated by PyTorch, and 4.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:18:59,686] Trial 13 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.935423225429787, 'batch_size': 205, 'attention_heads': 11, 'hidden_dimension': 256, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40560445824670244, 'global_pooling': 'max', 'learning_rate': 3.918280364882131e-05, 'weight_decay': 3.7947055714200486e-05, 'beta_0': 0.8385264631559951, 'beta_1': 0.9840857020999852, 'epsilon': 1.9747342091229038e-05, 'balanced_loss': False, 'epochs': 93, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 11 with value: 0.9479004462240216.
[I 2024-12-18 08:29:55,011] Trial 14 finished with value: 0.950431835011788 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9997353474576673, 'batch_size': 206, 'attention_heads': 8, 'hidden_dimension': 168, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5316869446562313, 'global_pooling': 'max', 'learning_rate': 0.0007163547440785774, 'weight_decay': 0.0001242481468574075, 'beta_0': 0.8505013175968061, 'beta_1': 0.986691679686623, 'epsilon': 8.480768432408905e-07, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 21, 'plateau_patience': 21, 'plateau_divider': 10}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 2.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.30 GiB is free. Including non-PyTorch memory, this process has 43.25 GiB memory in use. Of the allocated memory 40.62 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:40:08,395] Trial 15 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9169141205212279, 'batch_size': 187, 'attention_heads': 7, 'hidden_dimension': 162, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5303921550345196, 'global_pooling': 'max', 'learning_rate': 0.0005663985788036183, 'weight_decay': 7.126759017353428e-05, 'beta_0': 0.8981752259338325, 'beta_1': 0.9942617699068594, 'epsilon': 4.39906335140932e-07, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 782.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 612.69 MiB is free. Including non-PyTorch memory, this process has 43.96 GiB memory in use. Of the allocated memory 40.86 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 08:49:55,819] Trial 16 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8515341881523538, 'batch_size': 176, 'attention_heads': 8, 'hidden_dimension': 55, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5994273832319155, 'global_pooling': 'sum', 'learning_rate': 0.00122860622618721, 'weight_decay': 1.131972459279219e-05, 'beta_0': 0.8545237148011087, 'beta_1': 0.98792850587226, 'epsilon': 1.1598630173023627e-05, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 14 with value: 0.950431835011788.
[I 2024-12-18 09:02:13,747] Trial 17 finished with value: 0.3234837418269365 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9538258615737971, 'batch_size': 100, 'attention_heads': 4, 'hidden_dimension': 185, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5075992731608033, 'global_pooling': 'max', 'learning_rate': 0.010592758388538851, 'weight_decay': 5.090524367378429e-06, 'beta_0': 0.8534896038405841, 'beta_1': 0.982877524946249, 'epsilon': 1.4448632210191146e-06, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 16, 'plateau_patience': 23, 'plateau_divider': 9}. Best is trial 14 with value: 0.950431835011788.
[I 2024-12-18 09:13:33,515] Trial 18 finished with value: 0.9359941015270068 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9966304459973653, 'batch_size': 215, 'attention_heads': 7, 'hidden_dimension': 123, 'number_of_hidden_layers': 0, 'dropout_rate': 0.42232439023176993, 'global_pooling': 'max', 'learning_rate': 0.0002942182398579171, 'weight_decay': 1.7723771230045992e-05, 'beta_0': 0.8245532437208418, 'beta_1': 0.9871461492475037, 'epsilon': 1.9470275750235379e-07, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 606.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 96.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 41.94 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:23:25,673] Trial 19 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8900396478255157, 'batch_size': 256, 'attention_heads': 5, 'hidden_dimension': 171, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5559053981752282, 'global_pooling': 'max', 'learning_rate': 0.0010093023973536523, 'weight_decay': 6.253613016759709e-05, 'beta_0': 0.8005312481474816, 'beta_1': 0.982122708167169, 'epsilon': 8.241323053983679e-05, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 43.40 GiB memory in use. Of the allocated memory 40.80 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:33:06,751] Trial 20 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8666666912053851, 'batch_size': 188, 'attention_heads': 7, 'hidden_dimension': 230, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5042139187722858, 'global_pooling': 'sum', 'learning_rate': 0.010799627874376775, 'weight_decay': 0.00024084116035986782, 'beta_0': 0.8438461240383416, 'beta_1': 0.9865453889524554, 'epsilon': 2.4484524848809268e-05, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 12, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 14 with value: 0.950431835011788.
[I 2024-12-18 09:47:46,128] Trial 21 finished with value: 0.9319698715541702 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9797405250413366, 'batch_size': 224, 'attention_heads': 11, 'hidden_dimension': 238, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43014025643390574, 'global_pooling': 'max', 'learning_rate': 9.890130541103471e-05, 'weight_decay': 0.00019605085416202377, 'beta_0': 0.8279340214701592, 'beta_1': 0.9856112225442458, 'epsilon': 1.341196069606771e-06, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 858.69 MiB is free. Including non-PyTorch memory, this process has 43.71 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 09:58:01,821] Trial 22 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9608929147790123, 'batch_size': 238, 'attention_heads': 12, 'hidden_dimension': 255, 'number_of_hidden_layers': 0, 'dropout_rate': 0.361889087598886, 'global_pooling': 'max', 'learning_rate': 0.00014638370511078414, 'weight_decay': 0.0004052703818917837, 'beta_0': 0.8132432596988407, 'beta_1': 0.9831689431968903, 'epsilon': 2.224076906110792e-06, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 24, 'plateau_patience': 22, 'plateau_divider': 6}. Best is trial 14 with value: 0.950431835011788.
CUDA out of memory. Tried to allocate 2.87 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 42.86 GiB memory in use. Of the allocated memory 41.13 GiB is allocated by PyTorch, and 589.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:08:03,426] Trial 23 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.940056240008319, 'batch_size': 198, 'attention_heads': 9, 'hidden_dimension': 220, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4791513938939588, 'global_pooling': 'max', 'learning_rate': 2.593074328487442e-05, 'weight_decay': 0.0009123802238285845, 'beta_0': 0.8649715719815638, 'beta_1': 0.986628488882759, 'epsilon': 5.603921469427055e-07, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 14 with value: 0.950431835011788.
[I 2024-12-18 10:20:03,797] Trial 24 finished with value: 0.9454094575298885 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9988584854215985, 'batch_size': 163, 'attention_heads': 13, 'hidden_dimension': 78, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5263455710207968, 'global_pooling': 'max', 'learning_rate': 0.0004013224920610712, 'weight_decay': 0.00014369210285812778, 'beta_0': 0.8429928673983533, 'beta_1': 0.9826180690151296, 'epsilon': 6.205480710176434e-06, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 20, 'plateau_patience': 21, 'plateau_divider': 5}. Best is trial 14 with value: 0.950431835011788.
[I 2024-12-18 10:32:10,420] Trial 25 finished with value: 0.9560815667887285 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9934989497423026, 'batch_size': 162, 'attention_heads': 16, 'hidden_dimension': 74, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5347720845519288, 'global_pooling': 'max', 'learning_rate': 0.00046198911344507505, 'weight_decay': 2.4782477564103955e-05, 'beta_0': 0.8419798151627595, 'beta_1': 0.9819335204381432, 'epsilon': 7.024129390264873e-06, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 43.19 GiB memory in use. Of the allocated memory 40.17 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 10:42:19,619] Trial 26 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9359640147939456, 'batch_size': 159, 'attention_heads': 16, 'hidden_dimension': 72, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5920161710509197, 'global_pooling': 'max', 'learning_rate': 0.0015689725702588285, 'weight_decay': 5.3115522991395093e-05, 'beta_0': 0.8601536837610989, 'beta_1': 0.982129946011773, 'epsilon': 6.017481446102913e-06, 'balanced_loss': True, 'epochs': 101, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 11:01:44,966] Trial 27 finished with value: 0.9518008168111434 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9703786831649261, 'batch_size': 120, 'attention_heads': 16, 'hidden_dimension': 113, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5506538991520507, 'global_pooling': 'max', 'learning_rate': 0.0007618772729827461, 'weight_decay': 2.1202359752079697e-05, 'beta_0': 0.872783509036895, 'beta_1': 0.998800192918023, 'epsilon': 1.2348171167491442e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 582.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 442.69 MiB is free. Including non-PyTorch memory, this process has 44.12 GiB memory in use. Of the allocated memory 42.12 GiB is allocated by PyTorch, and 866.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 11:05:01,064] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.6546169704823758, 'batch_size': 110, 'attention_heads': 16, 'hidden_dimension': 137, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5477022739287299, 'global_pooling': 'max', 'learning_rate': 0.006029308911410843, 'weight_decay': 2.0002364485345235e-05, 'beta_0': 0.8791535624926523, 'beta_1': 0.9987966729754157, 'epsilon': 3.526406322309019e-05, 'balanced_loss': False, 'epochs': 83, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 11:31:16,430] Trial 29 finished with value: 0.9330608455226649 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9610935797905009, 'batch_size': 75, 'attention_heads': 15, 'hidden_dimension': 110, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5726714332192433, 'global_pooling': 'sum', 'learning_rate': 0.000729087030526837, 'weight_decay': 4.519161643095247e-06, 'beta_0': 0.8486413341625744, 'beta_1': 0.9933311791692339, 'epsilon': 1.0124006117781686e-05, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 21, 'plateau_patience': 24, 'plateau_divider': 5}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 44.56 GiB of which 864.69 MiB is free. Including non-PyTorch memory, this process has 43.71 GiB memory in use. Of the allocated memory 40.46 GiB is allocated by PyTorch, and 2.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 11:45:06,376] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.923011737767738, 'batch_size': 118, 'attention_heads': 15, 'hidden_dimension': 86, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5241240088826381, 'global_pooling': 'max', 'learning_rate': 0.0016527656084560432, 'weight_decay': 2.2587838673287162e-05, 'beta_0': 0.8739058969428093, 'beta_1': 0.9964087200840209, 'epsilon': 1.4580580709122412e-05, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 11:58:49,764] Trial 31 finished with value: 0.9560812752068968 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9785884240783094, 'batch_size': 72, 'attention_heads': 16, 'hidden_dimension': 60, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49295277834604234, 'global_pooling': 'max', 'learning_rate': 0.00024692768042529876, 'weight_decay': 5.641265208849997e-06, 'beta_0': 0.8902221070452774, 'beta_1': 0.9840319685959846, 'epsilon': 1.0748592804718602e-08, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 12:12:31,872] Trial 32 finished with value: 0.9539272388735974 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9734639760318092, 'batch_size': 63, 'attention_heads': 16, 'hidden_dimension': 34, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5333166517025224, 'global_pooling': 'max', 'learning_rate': 0.0001736758390822057, 'weight_decay': 6.427866451896479e-06, 'beta_0': 0.8933920868390848, 'beta_1': 0.9886409103152887, 'epsilon': 1.8940571553937762e-08, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 22, 'plateau_patience': 21, 'plateau_divider': 10}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 12:27:28,020] Trial 33 finished with value: 0.9255693853552007 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9712179425713848, 'batch_size': 63, 'attention_heads': 16, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.568578814365262, 'global_pooling': 'max', 'learning_rate': 7.488188611403008e-05, 'weight_decay': 6.529319472743726e-06, 'beta_0': 0.89986087646536, 'beta_1': 0.9910833577855309, 'epsilon': 1.0954871206627958e-08, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 12:44:54,191] Trial 34 finished with value: 0.9554085294346883 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9752473624397269, 'batch_size': 74, 'attention_heads': 14, 'hidden_dimension': 65, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4593794979564439, 'global_pooling': 'max', 'learning_rate': 0.0001749018486396942, 'weight_decay': 1.4473521143821675e-06, 'beta_0': 0.8877350783155756, 'beta_1': 0.9886058784688629, 'epsilon': 1.9231004253083396e-08, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 20, 'plateau_patience': 23, 'plateau_divider': 8}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 13:05:21,604] Trial 35 finished with value: 0.8958279703771017 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9435958973719744, 'batch_size': 74, 'attention_heads': 14, 'hidden_dimension': 61, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4562753141771098, 'global_pooling': 'mean', 'learning_rate': 0.00015562005176279767, 'weight_decay': 2.9187972883646018e-06, 'beta_0': 0.892194094534656, 'beta_1': 0.9886501249003008, 'epsilon': 1.9124691515546688e-08, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 22, 'plateau_patience': 23, 'plateau_divider': 8}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 382.69 MiB is free. Including non-PyTorch memory, this process has 44.18 GiB memory in use. Of the allocated memory 41.28 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:14:17,798] Trial 36 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8850166441671046, 'batch_size': 41, 'attention_heads': 15, 'hidden_dimension': 39, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46781147315806504, 'global_pooling': 'max', 'learning_rate': 0.00020344889633761202, 'weight_decay': 1.058580085067608e-06, 'beta_0': 0.889010980277625, 'beta_1': 0.9891043130224098, 'epsilon': 1.5381650625992906e-08, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 20, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 586.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 106.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 41.91 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:18:15,949] Trial 37 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.7255965915809423, 'batch_size': 59, 'attention_heads': 13, 'hidden_dimension': 87, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43654479854589223, 'global_pooling': 'max', 'learning_rate': 6.354542146067412e-05, 'weight_decay': 1.6673738951439376e-06, 'beta_0': 0.8912910523722775, 'beta_1': 0.9914947828503057, 'epsilon': 2.7251403818757756e-08, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 22, 'plateau_patience': 24, 'plateau_divider': 8}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 622.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 500.69 MiB is free. Including non-PyTorch memory, this process has 44.06 GiB memory in use. Of the allocated memory 41.45 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:23:59,806] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8105006088406925, 'batch_size': 97, 'attention_heads': 14, 'hidden_dimension': 65, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4806379608733442, 'global_pooling': 'mean', 'learning_rate': 2.3526847757881522e-05, 'weight_decay': 2.797082808894325e-06, 'beta_0': 0.8840010191880951, 'beta_1': 0.9813242137644196, 'epsilon': 1.0392973320835547e-08, 'balanced_loss': True, 'epochs': 56, 'early_stopping_patience': 24, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 13:37:31,716] Trial 39 finished with value: 0.9555466053284694 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9740206260647626, 'batch_size': 80, 'attention_heads': 15, 'hidden_dimension': 35, 'number_of_hidden_layers': 3, 'dropout_rate': 0.49119807892656303, 'global_pooling': 'max', 'learning_rate': 0.0004231564836181946, 'weight_decay': 1.6478405689185243e-06, 'beta_0': 0.8826125565109026, 'beta_1': 0.991120832100933, 'epsilon': 9.865095918241453e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 25 with value: 0.9560815667887285.
CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1020.69 MiB is free. Including non-PyTorch memory, this process has 43.56 GiB memory in use. Of the allocated memory 40.73 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 13:47:20,091] Trial 40 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.912043174875548, 'batch_size': 84, 'attention_heads': 13, 'hidden_dimension': 96, 'number_of_hidden_layers': 4, 'dropout_rate': 0.445781654538004, 'global_pooling': 'max', 'learning_rate': 0.00037962353979565256, 'weight_decay': 1.6377281621148823e-06, 'beta_0': 0.88073922149119, 'beta_1': 0.9915467793668203, 'epsilon': 1.1380217128361015e-07, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 15, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 25 with value: 0.9560815667887285.
[I 2024-12-18 14:03:22,904] Trial 41 finished with value: 0.9616639212868312 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9745785791972349, 'batch_size': 55, 'attention_heads': 15, 'hidden_dimension': 32, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4922811915284818, 'global_pooling': 'max', 'learning_rate': 0.000204421107354276, 'weight_decay': 2.8081219833492326e-06, 'beta_0': 0.8951017744615807, 'beta_1': 0.9900933444126176, 'epsilon': 2.491758455055989e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 14:17:59,985] Trial 42 finished with value: 0.9553815521639619 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9795870969392275, 'batch_size': 53, 'attention_heads': 15, 'hidden_dimension': 50, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4902230317197663, 'global_pooling': 'max', 'learning_rate': 0.00025313472349707195, 'weight_decay': 2.755684972905708e-06, 'beta_0': 0.8956742702838026, 'beta_1': 0.990276338653993, 'epsilon': 3.103339453171653e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 17, 'plateau_patience': 22, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 14:33:15,132] Trial 43 finished with value: 0.9563236766853687 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9480508857530868, 'batch_size': 85, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46597090287209053, 'global_pooling': 'max', 'learning_rate': 0.0004501482327759262, 'weight_decay': 1.3816120486751584e-06, 'beta_0': 0.8861232923355153, 'beta_1': 0.9939334423337155, 'epsilon': 9.03730978352903e-08, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 14:46:20,372] Trial 44 finished with value: 0.14530834510689417 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9499940705672298, 'batch_size': 34, 'attention_heads': 15, 'hidden_dimension': 45, 'number_of_hidden_layers': 3, 'dropout_rate': 0.49195801073333933, 'global_pooling': 'mean', 'learning_rate': 0.03471701807797727, 'weight_decay': 3.479649470542475e-06, 'beta_0': 0.8859469718294603, 'beta_1': 0.9942667542275971, 'epsilon': 9.663720852678976e-08, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 41 with value: 0.9616639212868312.
The selected strides are greater or equal to the total chunk size.
[I 2024-12-18 14:46:21,910] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.924661992780673, 'batch_size': 86, 'attention_heads': 14, 'hidden_dimension': 32, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4702095734376294, 'global_pooling': 'max', 'learning_rate': 0.00042648631904435025, 'weight_decay': 2.025035755121822e-06, 'beta_0': 0.8733656294868961, 'beta_1': 0.9903827814865431, 'epsilon': 6.432839450280042e-08, 'balanced_loss': False, 'epochs': 199, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 14:59:29,693] Trial 46 finished with value: 0.9351149490414555 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9863521297951189, 'batch_size': 49, 'attention_heads': 12, 'hidden_dimension': 55, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5160176911951097, 'global_pooling': 'sum', 'learning_rate': 0.0024512710728531706, 'weight_decay': 1.03784575789278e-06, 'beta_0': 0.881797313234273, 'beta_1': 0.9953710445285355, 'epsilon': 2.1270705976344274e-07, 'balanced_loss': False, 'epochs': 189, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 15:18:38,315] Trial 47 finished with value: 0.9411348415453302 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9575656844158699, 'batch_size': 138, 'attention_heads': 15, 'hidden_dimension': 75, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4942405195805454, 'global_pooling': 'max', 'learning_rate': 0.0005388134018200429, 'weight_decay': 2.180891692355833e-06, 'beta_0': 0.8593569768563284, 'beta_1': 0.9925107684251949, 'epsilon': 1.4561425385196725e-07, 'balanced_loss': True, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 41 with value: 0.9616639212868312.
CUDA out of memory. Tried to allocate 854.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 290.69 MiB is free. Including non-PyTorch memory, this process has 44.27 GiB memory in use. Of the allocated memory 41.26 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:28:09,717] Trial 48 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8975316080349238, 'batch_size': 82, 'attention_heads': 16, 'hidden_dimension': 47, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4706775354907575, 'global_pooling': 'max', 'learning_rate': 9.363579371136137e-05, 'weight_decay': 7.099804373456422e-06, 'beta_0': 0.8356236545554129, 'beta_1': 0.9812804945659864, 'epsilon': 3.1736912484173437e-07, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
CUDA out of memory. Tried to allocate 658.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 564.69 MiB is free. Including non-PyTorch memory, this process has 44.00 GiB memory in use. Of the allocated memory 39.99 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:38:38,647] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9299761521787785, 'batch_size': 152, 'attention_heads': 13, 'hidden_dimension': 34, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4452674388377583, 'global_pooling': 'max', 'learning_rate': 0.0002385411022640246, 'weight_decay': 4.1341405148069456e-06, 'beta_0': 0.8772663892986757, 'beta_1': 0.992438394045048, 'epsilon': 3.2192685755902815e-08, 'balanced_loss': False, 'epochs': 151, 'early_stopping_patience': 13, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 41 with value: 0.9616639212868312.
CUDA out of memory. Tried to allocate 664.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 446.69 MiB is free. Including non-PyTorch memory, this process has 44.12 GiB memory in use. Of the allocated memory 41.24 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 15:49:37,540] Trial 50 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8753162657498554, 'batch_size': 96, 'attention_heads': 14, 'hidden_dimension': 56, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4173066799405619, 'global_pooling': 'mean', 'learning_rate': 0.0011235481896165265, 'weight_decay': 2.2574813864348544e-06, 'beta_0': 0.8668971399138644, 'beta_1': 0.9937637497393514, 'epsilon': 4.9524491289694766e-08, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 16:03:58,320] Trial 51 finished with value: 0.9511476022127756 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9777679244515806, 'batch_size': 76, 'attention_heads': 14, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4626040570072855, 'global_pooling': 'max', 'learning_rate': 0.00036628443792818724, 'weight_decay': 1.5676910788461962e-06, 'beta_0': 0.8876383152181252, 'beta_1': 0.9836846019846553, 'epsilon': 7.155986974015157e-08, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 20, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 16:20:26,271] Trial 52 finished with value: 0.9535923119936153 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9866622978702395, 'batch_size': 70, 'attention_heads': 15, 'hidden_dimension': 84, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4532778560419843, 'global_pooling': 'max', 'learning_rate': 0.0001226556588646111, 'weight_decay': 1.4221966557396753e-06, 'beta_0': 0.8954200439483945, 'beta_1': 0.9894851056960111, 'epsilon': 1.856044844406744e-08, 'balanced_loss': False, 'epochs': 192, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 5}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 16:42:31,137] Trial 53 finished with value: 0.9355169338588796 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9587157794439675, 'batch_size': 52, 'attention_heads': 16, 'hidden_dimension': 42, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4778705756536989, 'global_pooling': 'max', 'learning_rate': 5.6753019665670456e-05, 'weight_decay': 3.381038208234953e-06, 'beta_0': 0.8878064856725032, 'beta_1': 0.9856684540942163, 'epsilon': 2.3380592161416913e-08, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 17, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 498.69 MiB is free. Including non-PyTorch memory, this process has 44.07 GiB memory in use. Of the allocated memory 40.90 GiB is allocated by PyTorch, and 2.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 16:53:08,000] Trial 54 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9487838590146556, 'batch_size': 90, 'attention_heads': 12, 'hidden_dimension': 96, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5159665139130506, 'global_pooling': 'max', 'learning_rate': 0.0005450585670450872, 'weight_decay': 1.4040248295477262e-06, 'beta_0': 0.8996150433014177, 'beta_1': 0.9907298235365574, 'epsilon': 4.159193021955771e-08, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 14, 'plateau_patience': 24, 'plateau_divider': 7}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 17:16:10,879] Trial 55 finished with value: 0.9502529614283419 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9655402623426708, 'batch_size': 107, 'attention_heads': 14, 'hidden_dimension': 66, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5427684366718633, 'global_pooling': 'max', 'learning_rate': 0.00022105504055558065, 'weight_decay': 1.228630059227156e-06, 'beta_0': 0.8906427494581673, 'beta_1': 0.9878161485224706, 'epsilon': 1.2679786333614113e-08, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 19, 'plateau_patience': 22, 'plateau_divider': 4}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 17:29:20,285] Trial 56 finished with value: 0.9529473756329538 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9825592986149841, 'batch_size': 67, 'attention_heads': 15, 'hidden_dimension': 54, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49461734607517316, 'global_pooling': 'sum', 'learning_rate': 0.0008092276989024439, 'weight_decay': 2.2814815638199027e-06, 'beta_0': 0.8407901624632614, 'beta_1': 0.9801330668540261, 'epsilon': 5.71397261395356e-08, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 16, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 41 with value: 0.9616639212868312.
[I 2024-12-18 17:43:10,420] Trial 57 finished with value: 0.9634349052250321 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9918076298832168, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 60, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5113975014149055, 'global_pooling': 'max', 'learning_rate': 0.00027531652571053706, 'weight_decay': 9.000116961490015e-06, 'beta_0': 0.8756957261351574, 'beta_1': 0.995190504496183, 'epsilon': 3.783066586454393e-08, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 57 with value: 0.9634349052250321.
[I 2024-12-18 17:56:31,284] Trial 58 finished with value: 0.9482212213000181 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9920831628366058, 'batch_size': 41, 'attention_heads': 10, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5178552732219951, 'global_pooling': 'max', 'learning_rate': 0.00030319138733709455, 'weight_decay': 1.4902173506700627e-05, 'beta_0': 0.8771171140597664, 'beta_1': 0.9953708259454117, 'epsilon': 1.3389589662061717e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 57 with value: 0.9634349052250321.
[I 2024-12-18 18:08:43,074] Trial 59 finished with value: 0.9439148601632884 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9457678060914592, 'batch_size': 46, 'attention_heads': 13, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5024759750058057, 'global_pooling': 'max', 'learning_rate': 0.001657646149741811, 'weight_decay': 9.451835508495231e-06, 'beta_0': 0.8837060210375401, 'beta_1': 0.9976806741779154, 'epsilon': 4.123137274363412e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 57 with value: 0.9634349052250321.
[I 2024-12-18 18:22:25,081] Trial 60 finished with value: 0.942105148363144 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9990123040481074, 'batch_size': 58, 'attention_heads': 16, 'hidden_dimension': 60, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5112082700300887, 'global_pooling': 'max', 'learning_rate': 0.0005240998902665531, 'weight_decay': 4.942752128427738e-06, 'beta_0': 0.8699233474012649, 'beta_1': 0.9920283755675221, 'epsilon': 7.818100333184633e-08, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 57 with value: 0.9634349052250321.
[I 2024-12-18 18:45:53,902] Trial 61 finished with value: 0.953958813433395 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9702458908729072, 'batch_size': 169, 'attention_heads': 15, 'hidden_dimension': 76, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4830064993552368, 'global_pooling': 'max', 'learning_rate': 0.000120214603913766, 'weight_decay': 3.895410618388198e-05, 'beta_0': 0.8305366547905669, 'beta_1': 0.9951939933439741, 'epsilon': 1.4639125845358669e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 20, 'plateau_patience': 21, 'plateau_divider': 3}. Best is trial 57 with value: 0.9634349052250321.
[I 2024-12-18 19:02:30,370] Trial 62 finished with value: 0.9672434357894326 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9872330029439409, 'batch_size': 35, 'attention_heads': 14, 'hidden_dimension': 41, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5378833131641806, 'global_pooling': 'max', 'learning_rate': 0.00017771924643805294, 'weight_decay': 2.6680229190830897e-05, 'beta_0': 0.894595157894698, 'beta_1': 0.9930269640915723, 'epsilon': 3.40929669959721e-08, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 19:17:27,080] Trial 63 finished with value: 0.9429004054737248 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9871338006885745, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 41, 'number_of_hidden_layers': 3, 'dropout_rate': 0.49820612542866877, 'global_pooling': 'max', 'learning_rate': 0.0002910233466016164, 'weight_decay': 2.783628587974064e-05, 'beta_0': 0.8948458802248084, 'beta_1': 0.9932254894782871, 'epsilon': 3.2770837707572364e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 62 with value: 0.9672434357894326.
CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 44.56 GiB of which 48.69 MiB is free. Including non-PyTorch memory, this process has 44.51 GiB memory in use. Of the allocated memory 40.60 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:22:42,621] Trial 64 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8025083648940198, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 49, 'number_of_hidden_layers': 4, 'dropout_rate': 0.559661350814183, 'global_pooling': 'max', 'learning_rate': 0.0008544888452812523, 'weight_decay': 1.3274598744735063e-05, 'beta_0': 0.8973028869427023, 'beta_1': 0.9945030359644254, 'epsilon': 3.290855054176974e-08, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 11, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 19:48:40,564] Trial 65 finished with value: 0.9176607831689438 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9629141119082287, 'batch_size': 55, 'attention_heads': 15, 'hidden_dimension': 39, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5351376405388644, 'global_pooling': 'max', 'learning_rate': 7.920540185849848e-05, 'weight_decay': 7.391159829943151e-06, 'beta_0': 0.8920375533612552, 'beta_1': 0.99328556103087, 'epsilon': 9.578809831739849e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 62 with value: 0.9672434357894326.
CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 218.69 MiB is free. Including non-PyTorch memory, this process has 44.34 GiB memory in use. Of the allocated memory 41.96 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:52:37,051] Trial 66 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.7257141274339548, 'batch_size': 34, 'attention_heads': 11, 'hidden_dimension': 71, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5406101705250905, 'global_pooling': 'max', 'learning_rate': 3.793304264908315e-05, 'weight_decay': 4.825689586272342e-05, 'beta_0': 0.8817936153405638, 'beta_1': 0.9970091475463968, 'epsilon': 2.494445266728175e-08, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 18, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 62 with value: 0.9672434357894326.
CUDA out of memory. Tried to allocate 882.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 698.69 MiB is free. Including non-PyTorch memory, this process has 43.87 GiB memory in use. Of the allocated memory 41.04 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 19:56:49,764] Trial 67 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.7555545975154193, 'batch_size': 130, 'attention_heads': 14, 'hidden_dimension': 53, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5252905343515627, 'global_pooling': 'sum', 'learning_rate': 0.0004411831587851457, 'weight_decay': 2.992568486057548e-05, 'beta_0': 0.8530510663846415, 'beta_1': 0.995956403139142, 'epsilon': 2.2625474187512185e-06, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 62 with value: 0.9672434357894326.
CUDA out of memory. Tried to allocate 822.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 722.69 MiB is free. Including non-PyTorch memory, this process has 43.85 GiB memory in use. Of the allocated memory 40.39 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:07:15,916] Trial 68 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9361262193413853, 'batch_size': 150, 'attention_heads': 15, 'hidden_dimension': 40, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5076377344818501, 'global_pooling': 'max', 'learning_rate': 0.0001399480318846748, 'weight_decay': 7.753677344235839e-05, 'beta_0': 0.8464360009076545, 'beta_1': 0.994778652139119, 'epsilon': 3.7607294634392296e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 62 with value: 0.9672434357894326.
CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 43.44 GiB memory in use. Of the allocated memory 39.17 GiB is allocated by PyTorch, and 3.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2024-12-18 20:13:50,436] Trial 69 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8501998376587949, 'batch_size': 179, 'attention_heads': 16, 'hidden_dimension': 151, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3134422309218492, 'global_pooling': 'max', 'learning_rate': 0.00018900378416311338, 'weight_decay': 1.127148625270311e-05, 'beta_0': 0.8859098641255984, 'beta_1': 0.9920596992772942, 'epsilon': 4.7051766001872545e-06, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 20:25:53,595] Trial 70 finished with value: 0.8093168461587217 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9988186787463623, 'batch_size': 65, 'attention_heads': 13, 'hidden_dimension': 60, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5596207658548608, 'global_pooling': 'mean', 'learning_rate': 0.0003382481389304881, 'weight_decay': 1.5824032220945485e-05, 'beta_0': 0.8768623962247376, 'beta_1': 0.9810649707226915, 'epsilon': 1.7091933688499559e-07, 'balanced_loss': False, 'epochs': 199, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 20:41:40,139] Trial 71 finished with value: 0.9613309298862562 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9781655207574649, 'batch_size': 80, 'attention_heads': 14, 'hidden_dimension': 68, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4742080098439588, 'global_pooling': 'max', 'learning_rate': 0.00017609687380997941, 'weight_decay': 1.8220504525941508e-06, 'beta_0': 0.8890332497922734, 'beta_1': 0.9897853204576306, 'epsilon': 1.6595441093303504e-08, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 20, 'plateau_patience': 21, 'plateau_divider': 8}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 21:02:44,169] Trial 72 finished with value: 0.9515045281560746 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9826537781147294, 'batch_size': 107, 'attention_heads': 14, 'hidden_dimension': 69, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5844797544987324, 'global_pooling': 'max', 'learning_rate': 0.0002602416335484905, 'weight_decay': 5.3403103045550815e-06, 'beta_0': 0.8895323534452435, 'beta_1': 0.9901574497811105, 'epsilon': 1.3496145707665531e-08, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 21:18:23,997] Trial 73 finished with value: 0.9491230289321821 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.954592116076946, 'batch_size': 91, 'attention_heads': 12, 'hidden_dimension': 59, 'number_of_hidden_layers': 3, 'dropout_rate': 0.48671775340199647, 'global_pooling': 'max', 'learning_rate': 0.0005717776260116926, 'weight_decay': 1.9026905589409637e-06, 'beta_0': 0.8969155957334557, 'beta_1': 0.9909453018453074, 'epsilon': 2.4802688640088214e-08, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 19, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 21:39:54,161] Trial 74 finished with value: 0.959461399530687 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9690445836515924, 'batch_size': 82, 'attention_heads': 14, 'hidden_dimension': 84, 'number_of_hidden_layers': 2, 'dropout_rate': 0.478531713080344, 'global_pooling': 'max', 'learning_rate': 9.770601710883996e-05, 'weight_decay': 3.59618692477028e-06, 'beta_0': 0.8926369376321511, 'beta_1': 0.9897470428094158, 'epsilon': 5.633416575253854e-08, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 20, 'plateau_patience': 21, 'plateau_divider': 8}. Best is trial 62 with value: 0.9672434357894326.
[I 2024-12-18 21:55:29,997] Trial 75 finished with value: 0.9492649240994022 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9896216016536915, 'batch_size': 59, 'attention_heads': 13, 'hidden_dimension': 81, 'number_of_hidden_layers': 2, 'dropout_rate': 0.47503495638543003, 'global_pooling': 'max', 'learning_rate': 8.844149547044327e-05, 'weight_decay': 3.410896535344372e-06, 'beta_0': 0.8936633866771083, 'beta_1': 0.9891731221919946, 'epsilon': 1.9816342095490246e-08, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 62 with value: 0.9672434357894326.
slurmstepd: error: *** JOB 14110886 ON gpu029 CANCELLED AT 2024-12-18T22:15:44 DUE TO TIME LIMIT ***
