[I 2025-01-14 22:38:58,112] Using an existing study with name 'R8-google-bert-bert-base-uncased' instead of creating a new one.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:57:39,472] Trial 52 finished with value: 0.943072496395745 and parameters: {'batch_size': 18, 'learning_rate': 3.621186546958565e-05, 'weight_decay': 2.3513080001380897e-06, 'beta_0': 0.8502222083507646, 'beta_1': 0.9920572433672404, 'epsilon': 4.5548354742250045e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:13:41,344] Trial 53 finished with value: 0.9456128149390512 and parameters: {'batch_size': 23, 'learning_rate': 3.613212011829509e-05, 'weight_decay': 4.969734815709925e-06, 'beta_0': 0.8467207440646778, 'beta_1': 0.9986096589118326, 'epsilon': 6.354044719160123e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:23:47,136] Trial 54 finished with value: 0.9131420820544549 and parameters: {'batch_size': 15, 'learning_rate': 4.115957751943267e-05, 'weight_decay': 1.1038251665362906e-05, 'beta_0': 0.8587854935928056, 'beta_1': 0.9980444377300658, 'epsilon': 3.814829249315713e-08, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:35:25,916] Trial 55 finished with value: 0.9449126042520479 and parameters: {'batch_size': 21, 'learning_rate': 4.6241984370791646e-05, 'weight_decay': 2.707011266889789e-06, 'beta_0': 0.8514025051786387, 'beta_1': 0.9964152925205231, 'epsilon': 1.4039142839870988e-05, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:53:26,843] Trial 56 finished with value: 0.9276981401673807 and parameters: {'batch_size': 17, 'learning_rate': 8.990464689668322e-05, 'weight_decay': 1.5090828147447702e-06, 'beta_0': 0.8413083785511063, 'beta_1': 0.9822098535067043, 'epsilon': 2.4983405549721166e-05, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:15:55,045] Trial 57 finished with value: 0.9377798436767459 and parameters: {'batch_size': 13, 'learning_rate': 2.3864583740197287e-05, 'weight_decay': 1.0372090653977997e-06, 'beta_0': 0.8753013388400006, 'beta_1': 0.9957449508805496, 'epsilon': 1.0102220319020404e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:28:00,304] Trial 58 finished with value: 0.9449730332346917 and parameters: {'batch_size': 19, 'learning_rate': 3.769579916178654e-05, 'weight_decay': 4.893222699215972e-06, 'beta_0': 0.8329633196303847, 'beta_1': 0.9940687159268582, 'epsilon': 9.723842462612583e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:39:46,190] Trial 59 finished with value: 0.9359603620228305 and parameters: {'batch_size': 24, 'learning_rate': 2.871159124536276e-05, 'weight_decay': 1.864327298688998e-06, 'beta_0': 0.864168007377452, 'beta_1': 0.9927533084194078, 'epsilon': 6.202275963028645e-08, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 2, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:53:49,137] Trial 60 finished with value: 0.9293340772832208 and parameters: {'batch_size': 16, 'learning_rate': 3.377669048211724e-05, 'weight_decay': 3.1601186178163006e-06, 'beta_0': 0.8257592972072562, 'beta_1': 0.9950981407341999, 'epsilon': 2.612518726619848e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:05:52,610] Trial 61 finished with value: 0.9311068989289071 and parameters: {'batch_size': 14, 'learning_rate': 7.918213659631251e-05, 'weight_decay': 2.57536102708851e-05, 'beta_0': 0.8573276518133606, 'beta_1': 0.9981399963472711, 'epsilon': 1.187911279664383e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:19:48,825] Trial 62 finished with value: 0.9435394881815347 and parameters: {'batch_size': 23, 'learning_rate': 7.013062228731918e-05, 'weight_decay': 0.0008794760175446727, 'beta_0': 0.8951180778023108, 'beta_1': 0.9909010845307563, 'epsilon': 7.018084080894189e-05, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:37:23,016] Trial 63 finished with value: 0.9350652874482481 and parameters: {'batch_size': 21, 'learning_rate': 4.18000166016025e-05, 'weight_decay': 1.752973172833225e-05, 'beta_0': 0.8832680288747266, 'beta_1': 0.9901287921199765, 'epsilon': 3.080352616164813e-05, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:55:12,945] Trial 64 finished with value: 0.9455287158319976 and parameters: {'batch_size': 25, 'learning_rate': 4.923964050483221e-05, 'weight_decay': 0.0007422784997434934, 'beta_0': 0.8694648906767735, 'beta_1': 0.9916514154974634, 'epsilon': 4.251721137992305e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:10:54,763] Trial 65 finished with value: 0.9293867876262398 and parameters: {'batch_size': 22, 'learning_rate': 2.6617924306461423e-05, 'weight_decay': 0.000468917380487922, 'beta_0': 0.8600694122525414, 'beta_1': 0.9882281847208095, 'epsilon': 7.323909701849414e-06, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:24:30,639] Trial 66 finished with value: 0.9451126075753538 and parameters: {'batch_size': 27, 'learning_rate': 3.230266688626314e-05, 'weight_decay': 3.856064545087712e-05, 'beta_0': 0.8871660026657164, 'beta_1': 0.9909545371894851, 'epsilon': 5.340141291963025e-07, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:46:07,493] Trial 67 finished with value: 0.9377686000975421 and parameters: {'batch_size': 9, 'learning_rate': 6.121915283925309e-05, 'weight_decay': 0.0003365035842650987, 'beta_0': 0.8795850327777215, 'beta_1': 0.9873632629131492, 'epsilon': 2.1747791647321573e-05, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:57:50,520] Trial 68 finished with value: 0.9359750967718243 and parameters: {'batch_size': 24, 'learning_rate': 5.473576380672032e-05, 'weight_decay': 0.00024455385416664003, 'beta_0': 0.8549337468957114, 'beta_1': 0.9933512099237587, 'epsilon': 9.318795257714294e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:19:34,680] Trial 69 finished with value: 0.9491509120320342 and parameters: {'batch_size': 20, 'learning_rate': 9.404475049734484e-05, 'weight_decay': 0.00013773943334711793, 'beta_0': 0.872688952299722, 'beta_1': 0.9853972209635998, 'epsilon': 7.597507264965416e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:31:32,136] Trial 70 finished with value: 0.9279638759621813 and parameters: {'batch_size': 20, 'learning_rate': 9.297077675295761e-05, 'weight_decay': 0.0001234542287791705, 'beta_0': 0.8719923351116692, 'beta_1': 0.983042366612111, 'epsilon': 7.270467254375253e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:47:17,848] Trial 71 finished with value: 0.9292786609032897 and parameters: {'batch_size': 19, 'learning_rate': 7.440898317442779e-05, 'weight_decay': 0.0001602029771211097, 'beta_0': 0.8672713134701383, 'beta_1': 0.9851398929434252, 'epsilon': 4.2943196595367954e-05, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:57:12,879] Trial 72 finished with value: 0.9377058095131096 and parameters: {'batch_size': 23, 'learning_rate': 7.890111157404374e-05, 'weight_decay': 7.777150362069926e-05, 'beta_0': 0.8862185155758324, 'beta_1': 0.98088479654994, 'epsilon': 5.070792619901053e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:12:55,540] Trial 73 finished with value: 0.9552641383895608 and parameters: {'batch_size': 21, 'learning_rate': 6.312410262969367e-05, 'weight_decay': 4.603696394916968e-06, 'beta_0': 0.8488311899215073, 'beta_1': 0.9887072729565951, 'epsilon': 7.780183756243794e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:34:51,478] Trial 74 finished with value: 0.9515448226377912 and parameters: {'batch_size': 17, 'learning_rate': 8.360692195778808e-05, 'weight_decay': 4.635102185985285e-06, 'beta_0': 0.8456111055936356, 'beta_1': 0.9865462401002352, 'epsilon': 8.708061503727504e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:54:46,663] Trial 75 finished with value: 0.9373503251080411 and parameters: {'batch_size': 17, 'learning_rate': 8.316405187640646e-05, 'weight_decay': 5.202326093809119e-06, 'beta_0': 0.846580213896994, 'beta_1': 0.986183850520574, 'epsilon': 5.671664314271589e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:04:56,387] Trial 76 finished with value: 0.9493416438367227 and parameters: {'batch_size': 18, 'learning_rate': 6.382498248905149e-05, 'weight_decay': 8.653970993342236e-06, 'beta_0': 0.8413141391428639, 'beta_1': 0.9880184932652095, 'epsilon': 9.973817668384933e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:19:15,893] Trial 77 finished with value: 0.29444404274335456 and parameters: {'batch_size': 12, 'learning_rate': 8.254665713182545e-05, 'weight_decay': 4.122186456669709e-06, 'beta_0': 0.8518342621615931, 'beta_1': 0.9868819147643729, 'epsilon': 3.222583311783071e-05, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:33:09,645] Trial 78 finished with value: 0.929624690687823 and parameters: {'batch_size': 22, 'learning_rate': 8.752050567507464e-05, 'weight_decay': 8.832839607426606e-06, 'beta_0': 0.8488353405153863, 'beta_1': 0.9891298131946658, 'epsilon': 7.704005944877088e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:46:59,961] Trial 79 finished with value: 0.9342592011324522 and parameters: {'batch_size': 21, 'learning_rate': 7.029242593095713e-05, 'weight_decay': 2.3344056820688935e-06, 'beta_0': 0.8448894114362148, 'beta_1': 0.9841050464553771, 'epsilon': 4.0404316302692085e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:02:58,157] Trial 80 finished with value: 0.9501112762497108 and parameters: {'batch_size': 14, 'learning_rate': 4.825297019813738e-05, 'weight_decay': 3.867060478324882e-06, 'beta_0': 0.8363241161330269, 'beta_1': 0.9887288473801488, 'epsilon': 5.4823487121459116e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:18:55,659] Trial 81 finished with value: 0.9530013779292789 and parameters: {'batch_size': 14, 'learning_rate': 4.712869595725382e-05, 'weight_decay': 4.26936470110695e-06, 'beta_0': 0.8355798815053331, 'beta_1': 0.9886866774585285, 'epsilon': 3.606969230578328e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:28:50,271] Trial 82 finished with value: 0.9291722470254644 and parameters: {'batch_size': 14, 'learning_rate': 5.8249875751794505e-05, 'weight_decay': 2.9218718748714277e-06, 'beta_0': 0.8250952032108257, 'beta_1': 0.9897407394830413, 'epsilon': 5.433400369200915e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:45:05,814] Trial 83 finished with value: 0.9361356651253043 and parameters: {'batch_size': 12, 'learning_rate': 4.7175658212057193e-05, 'weight_decay': 4.166841369286657e-06, 'beta_0': 0.8367107131834673, 'beta_1': 0.9887965086275209, 'epsilon': 3.6086282314642265e-05, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:59:10,454] Trial 84 finished with value: 0.9357639189240688 and parameters: {'batch_size': 16, 'learning_rate': 3.981441510591166e-05, 'weight_decay': 5.578661089802747e-06, 'beta_0': 0.8429662980697857, 'beta_1': 0.9886027617851912, 'epsilon': 8.480498676735143e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:15:14,541] Trial 85 finished with value: 0.9251808665170073 and parameters: {'batch_size': 18, 'learning_rate': 4.3360129649154576e-05, 'weight_decay': 4.265102349740142e-06, 'beta_0': 0.8295081463459214, 'beta_1': 0.9878948642339749, 'epsilon': 1.4618133547921915e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:36:14,916] Trial 86 finished with value: 0.9406202648768041 and parameters: {'batch_size': 11, 'learning_rate': 5.5498292503948617e-05, 'weight_decay': 8.131936962959819e-06, 'beta_0': 0.8406083701302893, 'beta_1': 0.9866407055233849, 'epsilon': 2.6957563807903206e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:51:21,931] Trial 87 finished with value: 0.9351057988024579 and parameters: {'batch_size': 9, 'learning_rate': 9.983048228333451e-05, 'weight_decay': 3.5709136361741134e-06, 'beta_0': 0.8141793024093881, 'beta_1': 0.9877496224198312, 'epsilon': 5.795459077973737e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:11:42,018] Trial 88 finished with value: 0.9499587134384484 and parameters: {'batch_size': 13, 'learning_rate': 5.067015941047298e-05, 'weight_decay': 1.4071305260794647e-05, 'beta_0': 0.8354064504501248, 'beta_1': 0.9902242010169978, 'epsilon': 1.7642552640881087e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:30:01,206] Trial 89 finished with value: 0.9327615445725901 and parameters: {'batch_size': 15, 'learning_rate': 4.852248797060588e-05, 'weight_decay': 6.420968429834674e-06, 'beta_0': 0.8537692276115593, 'beta_1': 0.9896870099691828, 'epsilon': 3.857902532160101e-05, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:45:54,742] Trial 90 finished with value: 0.9513564970847446 and parameters: {'batch_size': 14, 'learning_rate': 6.180071014284032e-05, 'weight_decay': 2.729917458172684e-06, 'beta_0': 0.8326205545741201, 'beta_1': 0.9893256679190331, 'epsilon': 4.70247294205966e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:57:53,425] Trial 91 finished with value: 0.9292671822781593 and parameters: {'batch_size': 17, 'learning_rate': 6.677679523399851e-05, 'weight_decay': 2.589926012854449e-06, 'beta_0': 0.8329915302976819, 'beta_1': 0.989229880025928, 'epsilon': 1.101908590561076e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:20:00,009] Trial 92 finished with value: 0.9486871989187937 and parameters: {'batch_size': 15, 'learning_rate': 6.321044134411991e-05, 'weight_decay': 5.058069521814911e-05, 'beta_0': 0.8280830694150058, 'beta_1': 0.9874257529692887, 'epsilon': 8.135329155836093e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:35:52,151] Trial 93 finished with value: 0.9395114822070565 and parameters: {'batch_size': 14, 'learning_rate': 9.167445260896303e-05, 'weight_decay': 3.399580015780471e-06, 'beta_0': 0.8379958259893435, 'beta_1': 0.9887438762870258, 'epsilon': 5.482902875997957e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:53:56,989] Trial 94 finished with value: 0.9318613978119933 and parameters: {'batch_size': 16, 'learning_rate': 4.418265874054356e-05, 'weight_decay': 1.9059437669938784e-06, 'beta_0': 0.8340706354398387, 'beta_1': 0.9904315083200743, 'epsilon': 4.5402055186097455e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:06:12,021] Trial 95 finished with value: 0.940850474828299 and parameters: {'batch_size': 12, 'learning_rate': 7.432043278333813e-05, 'weight_decay': 4.68974148874705e-06, 'beta_0': 0.8565569165307678, 'beta_1': 0.985893857001355, 'epsilon': 8.689361356461773e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:16:48,275] Trial 96 finished with value: 0.9353899787265114 and parameters: {'batch_size': 10, 'learning_rate': 5.2038149849596394e-05, 'weight_decay': 1.6246883331050982e-06, 'beta_0': 0.8612863982305821, 'beta_1': 0.9844205548893101, 'epsilon': 2.9444923725960564e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:31:24,258] Trial 97 finished with value: 0.9561909160044286 and parameters: {'batch_size': 10, 'learning_rate': 5.65351547694302e-05, 'weight_decay': 6.06440001439246e-06, 'beta_0': 0.8210117988867985, 'beta_1': 0.9895275881931475, 'epsilon': 1.55035524105897e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:54:54,236] Trial 98 finished with value: 0.940538754690635 and parameters: {'batch_size': 9, 'learning_rate': 6.160948026911635e-05, 'weight_decay': 3.0530614511943797e-06, 'beta_0': 0.8074149414932591, 'beta_1': 0.9870141528792962, 'epsilon': 1.584388761629005e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:05:23,973] Trial 99 finished with value: 0.9344389860020083 and parameters: {'batch_size': 10, 'learning_rate': 6.847276591433647e-05, 'weight_decay': 1.0130467028208478e-05, 'beta_0': 0.8203293557750905, 'beta_1': 0.9913449110676716, 'epsilon': 4.961573232049962e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:24:13,037] Trial 100 finished with value: 0.9566450575837858 and parameters: {'batch_size': 10, 'learning_rate': 3.7795465501199016e-05, 'weight_decay': 5.842588295816132e-06, 'beta_0': 0.8216250214369071, 'beta_1': 0.9894869033116579, 'epsilon': 2.2401068099912887e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:52:39,645] Trial 101 finished with value: 0.9506766730185634 and parameters: {'batch_size': 8, 'learning_rate': 3.146010449946637e-05, 'weight_decay': 7.2130734432857855e-06, 'beta_0': 0.8179610109335202, 'beta_1': 0.980053129440671, 'epsilon': 2.681316230012524e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:09:25,618] Trial 102 finished with value: 0.9358943572339764 and parameters: {'batch_size': 11, 'learning_rate': 3.821865655206694e-05, 'weight_decay': 6.080266516375146e-06, 'beta_0': 0.8215454614378976, 'beta_1': 0.9895931964402892, 'epsilon': 4.018260676261028e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:34:36,336] Trial 103 finished with value: 0.9549784876190945 and parameters: {'batch_size': 11, 'learning_rate': 3.454353966788077e-05, 'weight_decay': 5.410780127063679e-06, 'beta_0': 0.8272143310983773, 'beta_1': 0.998487343062393, 'epsilon': 1.3094831892231613e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:53:26,542] Trial 104 finished with value: 0.9435190483368259 and parameters: {'batch_size': 11, 'learning_rate': 3.491635622781656e-05, 'weight_decay': 5.9498424384302396e-06, 'beta_0': 0.8277244088584578, 'beta_1': 0.997463487082416, 'epsilon': 1.4847136049646884e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:20:20,752] Trial 105 finished with value: 0.9540078560000796 and parameters: {'batch_size': 10, 'learning_rate': 5.6499290347720736e-05, 'weight_decay': 4.8282388109308695e-06, 'beta_0': 0.8237161772921199, 'beta_1': 0.9985585146560731, 'epsilon': 2.1448658601497328e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:43:11,519] Trial 106 finished with value: 0.9438259508167677 and parameters: {'batch_size': 10, 'learning_rate': 2.7794906210818495e-05, 'weight_decay': 7.622435513689409e-06, 'beta_0': 0.8210320387607547, 'beta_1': 0.9985563225634007, 'epsilon': 2.0253131103916137e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:03:04,844] Trial 107 finished with value: 0.9448267070408555 and parameters: {'batch_size': 8, 'learning_rate': 3.137778339405502e-05, 'weight_decay': 4.6036289762556425e-06, 'beta_0': 0.8119897634068427, 'beta_1': 0.9984559035570222, 'epsilon': 1.296413159358741e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:18:10,009] Trial 108 finished with value: 0.9179413097571812 and parameters: {'batch_size': 9, 'learning_rate': 5.657945877509735e-05, 'weight_decay': 5.4049313350479975e-06, 'beta_0': 0.823600865636759, 'beta_1': 0.9972610944245387, 'epsilon': 7.708866382058211e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
slurmstepd: error: *** JOB 14454007 ON gpu053 CANCELLED AT 2025-01-15T14:38:21 DUE TO TIME LIMIT ***
