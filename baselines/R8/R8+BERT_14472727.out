[I 2025-01-15 14:56:37,970] Using an existing study with name 'R8-google-bert-bert-base-uncased' instead of creating a new one.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:21:37,941] Trial 110 finished with value: 0.9627314453282731 and parameters: {'batch_size': 10, 'learning_rate': 2.561928091317638e-05, 'weight_decay': 1.393369399786584e-05, 'beta_0': 0.8172349900880856, 'beta_1': 0.9969192546274411, 'epsilon': 1.2610336078817534e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:48:32,404] Trial 111 finished with value: 0.9552137440796558 and parameters: {'batch_size': 10, 'learning_rate': 2.173499221818131e-05, 'weight_decay': 1.0284408871353533e-05, 'beta_0': 0.8093562860825126, 'beta_1': 0.9966135903972444, 'epsilon': 1.1436440450948226e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:07:13,247] Trial 112 finished with value: 0.9348506046742178 and parameters: {'batch_size': 10, 'learning_rate': 2.027985054320217e-05, 'weight_decay': 1.5129085665048558e-05, 'beta_0': 0.8033118388512199, 'beta_1': 0.9967064007114088, 'epsilon': 1.140281502908431e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:23:58,837] Trial 113 finished with value: 0.9383636695616672 and parameters: {'batch_size': 11, 'learning_rate': 2.5174216896633778e-05, 'weight_decay': 2.352831625667499e-05, 'beta_0': 0.8167965049229962, 'beta_1': 0.998233549120281, 'epsilon': 2.0095263540176545e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:40:35,081] Trial 114 finished with value: 0.9452423591950709 and parameters: {'batch_size': 10, 'learning_rate': 2.4368119163131417e-05, 'weight_decay': 1.0108934373733983e-05, 'beta_0': 0.8091367053219004, 'beta_1': 0.9975523438201318, 'epsilon': 1.0982210887791115e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:00:51,107] Trial 115 finished with value: 0.9456672847937455 and parameters: {'batch_size': 12, 'learning_rate': 2.2888793235660688e-05, 'weight_decay': 9.566863445815772e-06, 'beta_0': 0.819105187241156, 'beta_1': 0.9988890809768969, 'epsilon': 2.5781573933000644e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:28:36,205] Trial 116 finished with value: 0.9589889362576733 and parameters: {'batch_size': 9, 'learning_rate': 2.1655739884929415e-05, 'weight_decay': 1.329222468245498e-05, 'beta_0': 0.8139061126420878, 'beta_1': 0.9970554084467734, 'epsilon': 5.0157994364713835e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:54:18,764] Trial 117 finished with value: 0.9526419068535599 and parameters: {'batch_size': 9, 'learning_rate': 1.9664213591126323e-05, 'weight_decay': 6.852720609274721e-06, 'beta_0': 0.8135698963264496, 'beta_1': 0.9967959638566911, 'epsilon': 5.508529043815313e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:18:19,625] Trial 118 finished with value: 0.9463350353853348 and parameters: {'batch_size': 8, 'learning_rate': 4.037775973557764e-05, 'weight_decay': 1.6880994222814924e-05, 'beta_0': 0.8077526930053707, 'beta_1': 0.9960685812935849, 'epsilon': 7.610987421726905e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:36:58,270] Trial 119 finished with value: 0.9407253845780865 and parameters: {'batch_size': 10, 'learning_rate': 1.83353374940103e-05, 'weight_decay': 1.1772203855953866e-05, 'beta_0': 0.8171395915411788, 'beta_1': 0.9978310529541645, 'epsilon': 3.312792948983776e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:54:04,541] Trial 120 finished with value: 0.9409455734445126 and parameters: {'batch_size': 9, 'learning_rate': 2.1871361788275063e-05, 'weight_decay': 1.3688329496188126e-05, 'beta_0': 0.8157507984413312, 'beta_1': 0.9957905542208089, 'epsilon': 7.066808455388163e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:10:48,496] Trial 121 finished with value: 0.9465666999596503 and parameters: {'batch_size': 11, 'learning_rate': 2.166670508007294e-05, 'weight_decay': 3.0025054080399696e-05, 'beta_0': 0.8221835158603017, 'beta_1': 0.9952853240321071, 'epsilon': 1.356360349768973e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:37:26,326] Trial 122 finished with value: 0.9454108199039233 and parameters: {'batch_size': 13, 'learning_rate': 3.3078072418767615e-05, 'weight_decay': 7.74589177885969e-06, 'beta_0': 0.8048779092923408, 'beta_1': 0.9971425076965844, 'epsilon': 2.316887191878378e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:03:08,069] Trial 123 finished with value: 0.9381795182020825 and parameters: {'batch_size': 9, 'learning_rate': 3.6618715383178986e-05, 'weight_decay': 6.737616203731866e-06, 'beta_0': 0.8121520696948139, 'beta_1': 0.9966798160161134, 'epsilon': 2.1727812468574633e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:22:47,520] Trial 124 finished with value: 0.9334845152004245 and parameters: {'batch_size': 8, 'learning_rate': 1.9070865639138622e-05, 'weight_decay': 5.71312794904773e-06, 'beta_0': 0.8132705447026092, 'beta_1': 0.9968773319114129, 'epsilon': 5.246711802997614e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:45:38,877] Trial 125 finished with value: 0.9565068543533007 and parameters: {'batch_size': 10, 'learning_rate': 1.7534465602044494e-05, 'weight_decay': 8.783536613211166e-06, 'beta_0': 0.8109392640758781, 'beta_1': 0.9978004409197252, 'epsilon': 5.486728619203164e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:06:28,050] Trial 126 finished with value: 0.9424538057680893 and parameters: {'batch_size': 10, 'learning_rate': 1.8090444809200843e-05, 'weight_decay': 2.034095116203171e-05, 'beta_0': 0.8006127890149078, 'beta_1': 0.9978028308149673, 'epsilon': 3.282922874085083e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:24:48,078] Trial 127 finished with value: 0.9535991503011321 and parameters: {'batch_size': 12, 'learning_rate': 3.510327004742648e-05, 'weight_decay': 8.915283318520212e-06, 'beta_0': 0.8102644397358365, 'beta_1': 0.9981403725923869, 'epsilon': 1.0075660279392333e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:43:32,058] Trial 128 finished with value: 0.9414005790658571 and parameters: {'batch_size': 11, 'learning_rate': 1.4941009159809988e-05, 'weight_decay': 1.085869103592352e-05, 'beta_0': 0.8106905319705259, 'beta_1': 0.9982236576833083, 'epsilon': 1.0109423767470243e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:06:25,632] Trial 129 finished with value: 0.9449390527987509 and parameters: {'batch_size': 10, 'learning_rate': 1.3341735560570008e-05, 'weight_decay': 9.05158763493275e-06, 'beta_0': 0.8054072146799113, 'beta_1': 0.9964915327404885, 'epsilon': 1.7370629148823816e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:28:46,371] Trial 130 finished with value: 0.9465050231124683 and parameters: {'batch_size': 12, 'learning_rate': 2.7450153965294944e-05, 'weight_decay': 1.2489092115456725e-05, 'beta_0': 0.8150447760874877, 'beta_1': 0.997696131380141, 'epsilon': 4.065511620504495e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:53:50,520] Trial 131 finished with value: 0.9481520306903852 and parameters: {'batch_size': 10, 'learning_rate': 1.7362783051032745e-05, 'weight_decay': 8.317919534842518e-06, 'beta_0': 0.8100920955261082, 'beta_1': 0.9984378218380632, 'epsilon': 8.490349655263319e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:15:30,191] Trial 132 finished with value: 0.9340001587955874 and parameters: {'batch_size': 9, 'learning_rate': 1.6855026149991847e-05, 'weight_decay': 1.6461976322781932e-05, 'beta_0': 0.8193028851933427, 'beta_1': 0.9980616037605172, 'epsilon': 6.20310960489928e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:36:21,252] Trial 133 finished with value: 0.9437980401241254 and parameters: {'batch_size': 11, 'learning_rate': 3.426406409750109e-05, 'weight_decay': 5.3954953538097625e-06, 'beta_0': 0.8257889068199106, 'beta_1': 0.9971881672425865, 'epsilon': 1.2565682664018652e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:55:12,255] Trial 134 finished with value: 0.9468927299124822 and parameters: {'batch_size': 11, 'learning_rate': 2.9760497063930275e-05, 'weight_decay': 3.911076297030178e-06, 'beta_0': 0.8235476922360513, 'beta_1': 0.9962141895180091, 'epsilon': 9.98322605448163e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:15:30,335] Trial 135 finished with value: 0.9341793608022291 and parameters: {'batch_size': 12, 'learning_rate': 2.0888261549149997e-05, 'weight_decay': 7.208161739029967e-06, 'beta_0': 0.8497697895336521, 'beta_1': 0.998611874164053, 'epsilon': 6.603560457148286e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:32:20,257] Trial 136 finished with value: 0.9353766522313989 and parameters: {'batch_size': 10, 'learning_rate': 3.743671464298681e-05, 'weight_decay': 4.749936085683506e-06, 'beta_0': 0.8187216195071502, 'beta_1': 0.9883371785424765, 'epsilon': 4.3375083731453475e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:50:34,613] Trial 137 finished with value: 0.950669564979371 and parameters: {'batch_size': 12, 'learning_rate': 1.5639698781299345e-05, 'weight_decay': 2.0192733730939266e-05, 'beta_0': 0.8161060626718969, 'beta_1': 0.9814754019834359, 'epsilon': 2.7846303300739376e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:10:42,961] Trial 138 finished with value: 0.9449013336234324 and parameters: {'batch_size': 13, 'learning_rate': 5.355684525239674e-05, 'weight_decay': 6.363024916343806e-06, 'beta_0': 0.8066656098990225, 'beta_1': 0.9974795216235907, 'epsilon': 1.4983592989002257e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:34:19,845] Trial 139 finished with value: 0.9510178154186191 and parameters: {'batch_size': 9, 'learning_rate': 4.5822903181316394e-05, 'weight_decay': 0.00018604061824201504, 'beta_0': 0.8273061974608286, 'beta_1': 0.9901085045820986, 'epsilon': 8.685993585872183e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:51:04,067] Trial 140 finished with value: 0.937436760118753 and parameters: {'batch_size': 11, 'learning_rate': 4.332353124720178e-05, 'weight_decay': 1.3458927593166258e-05, 'beta_0': 0.8639885700653988, 'beta_1': 0.982641849574188, 'epsilon': 3.552460791122815e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:07:42,986] Trial 141 finished with value: 0.9446495907987278 and parameters: {'batch_size': 10, 'learning_rate': 5.904251019478132e-05, 'weight_decay': 9.16868210632455e-06, 'beta_0': 0.8112933474724184, 'beta_1': 0.9989943684693512, 'epsilon': 1.8414564537588066e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:24:53,334] Trial 142 finished with value: 0.952642775700158 and parameters: {'batch_size': 9, 'learning_rate': 2.2866466719254486e-05, 'weight_decay': 1.0670629473880692e-05, 'beta_0': 0.8142354562774333, 'beta_1': 0.9979241736015267, 'epsilon': 5.480885459908142e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:44:07,963] Trial 143 finished with value: 0.9489732893757068 and parameters: {'batch_size': 9, 'learning_rate': 2.322093555412153e-05, 'weight_decay': 8.137822476050576e-06, 'beta_0': 0.8093016023525982, 'beta_1': 0.9979807103538337, 'epsilon': 4.3772015752881174e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:10:14,051] Trial 144 finished with value: 0.9421208681658988 and parameters: {'batch_size': 8, 'learning_rate': 2.5340546260880705e-05, 'weight_decay': 1.1093428710892331e-05, 'beta_0': 0.8217776908553069, 'beta_1': 0.9971485558529706, 'epsilon': 3.044836763491093e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:25:16,115] Trial 145 finished with value: 0.9415557881649672 and parameters: {'batch_size': 9, 'learning_rate': 1.398785963437085e-05, 'weight_decay': 5.134934167639367e-06, 'beta_0': 0.8141793024093881, 'beta_1': 0.9975921740312883, 'epsilon': 7.920883665473527e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:41:48,662] Trial 146 finished with value: 0.9401821972353545 and parameters: {'batch_size': 10, 'learning_rate': 2.178224887185656e-05, 'weight_decay': 3.6745865418972367e-06, 'beta_0': 0.8173921900945278, 'beta_1': 0.9982281542549124, 'epsilon': 1.5277787824013311e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:04:50,681] Trial 147 finished with value: 0.9381153936199953 and parameters: {'batch_size': 11, 'learning_rate': 3.868457122864866e-05, 'weight_decay': 1.0067766481088142e-05, 'beta_0': 0.824479400024573, 'beta_1': 0.9986158363933523, 'epsilon': 2.3228340600786876e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:20:00,253] Trial 148 finished with value: 0.9561554477055497 and parameters: {'batch_size': 8, 'learning_rate': 3.4614829120831004e-05, 'weight_decay': 9.876865961939695e-05, 'beta_0': 0.8131085466658841, 'beta_1': 0.9889828300274781, 'epsilon': 2.282991423015091e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:39:36,179] Trial 149 finished with value: 0.9500576505522998 and parameters: {'batch_size': 8, 'learning_rate': 3.1807636301963846e-05, 'weight_decay': 8.30946148038446e-05, 'beta_0': 0.8085705376620261, 'beta_1': 0.993739704009918, 'epsilon': 1.1271190058299186e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:59:09,196] Trial 150 finished with value: 0.9546522228852361 and parameters: {'batch_size': 8, 'learning_rate': 3.478466356024465e-05, 'weight_decay': 0.00011551300777012045, 'beta_0': 0.8571139746168577, 'beta_1': 0.9928719743460537, 'epsilon': 2.20640515539873e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:27:25,790] Trial 151 finished with value: 0.9486992080676628 and parameters: {'batch_size': 8, 'learning_rate': 3.567324270841646e-05, 'weight_decay': 9.758736326522057e-05, 'beta_0': 0.8570364730318974, 'beta_1': 0.9948810966429641, 'epsilon': 2.2657945844630477e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:44:48,743] Trial 152 finished with value: 0.9479158565344002 and parameters: {'batch_size': 8, 'learning_rate': 3.34301704361964e-05, 'weight_decay': 0.00014478679452852738, 'beta_0': 0.8556197138809098, 'beta_1': 0.9929732533049942, 'epsilon': 5.2195907981475836e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:10:30,388] Trial 153 finished with value: 0.9495277140613063 and parameters: {'batch_size': 9, 'learning_rate': 3.477461583645198e-05, 'weight_decay': 6.803777179430419e-05, 'beta_0': 0.8606347208724981, 'beta_1': 0.9891781510163481, 'epsilon': 3.3592940881722314e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:27:17,414] Trial 154 finished with value: 0.945867226204327 and parameters: {'batch_size': 10, 'learning_rate': 3.0474991485003873e-05, 'weight_decay': 0.00028642956739392556, 'beta_0': 0.8531090797917318, 'beta_1': 0.9898625833739468, 'epsilon': 5.014042634890902e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:48:11,798] Trial 155 finished with value: 0.9463540390724081 and parameters: {'batch_size': 11, 'learning_rate': 3.9631849601793446e-05, 'weight_decay': 0.00010706429457091454, 'beta_0': 0.8512201888990107, 'beta_1': 0.9905968492785197, 'epsilon': 1.3686072968231714e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:02:46,738] Trial 156 finished with value: 0.9249651434400397 and parameters: {'batch_size': 10, 'learning_rate': 2.8853322197097916e-05, 'weight_decay': 0.0002354733848778259, 'beta_0': 0.8581177101998784, 'beta_1': 0.9883461267662016, 'epsilon': 1.7971908871411341e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:18:56,564] Trial 157 finished with value: 0.9256529904513668 and parameters: {'batch_size': 12, 'learning_rate': 9.497488134248475e-05, 'weight_decay': 4.235408488063038e-05, 'beta_0': 0.8200003276404665, 'beta_1': 0.9889412360835739, 'epsilon': 1.1137164604065983e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:45:15,445] Trial 158 finished with value: 0.9492130550900351 and parameters: {'batch_size': 8, 'learning_rate': 4.233819377987009e-05, 'weight_decay': 0.00018273746758286604, 'beta_0': 0.812549788768281, 'beta_1': 0.9877690282704837, 'epsilon': 3.7919306946826914e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:08:46,891] Trial 159 finished with value: 0.9455560351334069 and parameters: {'batch_size': 9, 'learning_rate': 1.1759764093169579e-05, 'weight_decay': 0.0003516573341362836, 'beta_0': 0.8482810348612657, 'beta_1': 0.9955428920865149, 'epsilon': 7.704117589069895e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:26:27,474] Trial 160 finished with value: 0.953094074140069 and parameters: {'batch_size': 20, 'learning_rate': 3.688697994707271e-05, 'weight_decay': 4.469555339403207e-06, 'beta_0': 0.8227799002369597, 'beta_1': 0.9964259635007384, 'epsilon': 2.9630534463952145e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:42:09,750] Trial 161 finished with value: 0.9504428080985501 and parameters: {'batch_size': 20, 'learning_rate': 3.697207133109039e-05, 'weight_decay': 0.00012781432520071214, 'beta_0': 0.8223734797605166, 'beta_1': 0.9921232373311415, 'epsilon': 2.22044649142596e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:57:48,097] Trial 162 finished with value: 0.9304440831257884 and parameters: {'batch_size': 19, 'learning_rate': 3.279426374197851e-05, 'weight_decay': 5.581975134439568e-06, 'beta_0': 0.8053243090136519, 'beta_1': 0.9965379642188511, 'epsilon': 6.289158218616337e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:14:32,442] Trial 163 finished with value: 0.941323171607076 and parameters: {'batch_size': 10, 'learning_rate': 3.509302715026209e-05, 'weight_decay': 4.18736348662169e-06, 'beta_0': 0.8296929735155907, 'beta_1': 0.9958223612643262, 'epsilon': 3.828382777075894e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:34:08,659] Trial 164 finished with value: 0.9508458056606387 and parameters: {'batch_size': 22, 'learning_rate': 4.11009642058469e-05, 'weight_decay': 5.7488077759732805e-05, 'beta_0': 0.8160855601612645, 'beta_1': 0.9894793134027983, 'epsilon': 2.7740279555670554e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:51:01,024] Trial 165 finished with value: 0.9448250922634274 and parameters: {'batch_size': 13, 'learning_rate': 3.739249348308993e-05, 'weight_decay': 6.441672158001691e-06, 'beta_0': 0.8250521970939069, 'beta_1': 0.9969992192584674, 'epsilon': 1.3342335268526364e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:07:03,448] Trial 166 finished with value: 0.9462718276993631 and parameters: {'batch_size': 18, 'learning_rate': 3.903746477647911e-05, 'weight_decay': 4.290552845753717e-06, 'beta_0': 0.8630373662449243, 'beta_1': 0.9974441813739736, 'epsilon': 1.5420578792818028e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:23:52,831] Trial 167 finished with value: 0.9555089607392833 and parameters: {'batch_size': 11, 'learning_rate': 3.1299093889067304e-05, 'weight_decay': 3.3180348908263543e-06, 'beta_0': 0.8187720926987967, 'beta_1': 0.9963641349592203, 'epsilon': 1.0453530593046375e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:42:35,379] Trial 168 finished with value: 0.9422450154761373 and parameters: {'batch_size': 11, 'learning_rate': 3.1122434727394604e-05, 'weight_decay': 5.274872196124723e-06, 'beta_0': 0.8180995738661662, 'beta_1': 0.996284332142642, 'epsilon': 1.0897529804518512e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.9632763298514234.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:59:18,538] Trial 169 finished with value: 0.9644723116113745 and parameters: {'batch_size': 10, 'learning_rate': 2.8048348341730308e-05, 'weight_decay': 7.539586492746159e-06, 'beta_0': 0.8206360148762586, 'beta_1': 0.9969032281145132, 'epsilon': 2.913965464166105e-06, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
File /checkpoint/pimentel/14472727/best-model-R8.pth.tar cannot be opened.
[I 2025-01-16 11:16:59,943] Trial 170 finished with value: -1.0 and parameters: {'batch_size': 9, 'learning_rate': 2.6826416045938122e-05, 'weight_decay': 7.4056212629977135e-06, 'beta_0': 0.8204268920049711, 'beta_1': 0.997076054784836, 'epsilon': 6.077532724499948e-08, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 11:35:39,333] Trial 171 finished with value: 0.9339151538666803 and parameters: {'batch_size': 10, 'learning_rate': 2.7990020177973876e-05, 'weight_decay': 8.686832911273851e-06, 'beta_0': 0.8107850681743757, 'beta_1': 0.9968067006236494, 'epsilon': 9.050667317733391e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 11:52:15,580] Trial 172 finished with value: 0.934409685658316 and parameters: {'batch_size': 10, 'learning_rate': 2.481494124049597e-05, 'weight_decay': 6.104751703944869e-06, 'beta_0': 0.8541137459617216, 'beta_1': 0.9977353882832828, 'epsilon': 1.8619770034857474e-06, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 12:07:54,046] Trial 173 finished with value: 0.9338869044599408 and parameters: {'batch_size': 20, 'learning_rate': 3.0105392655692436e-05, 'weight_decay': 4.8775495265822375e-06, 'beta_0': 0.8194702344193012, 'beta_1': 0.9963626949588178, 'epsilon': 8.523908505565965e-06, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 12:22:32,200] Trial 174 finished with value: 0.9371520691205039 and parameters: {'batch_size': 11, 'learning_rate': 3.2755869016695654e-05, 'weight_decay': 3.2677432267546445e-06, 'beta_0': 0.8228499607260044, 'beta_1': 0.995929584005271, 'epsilon': 3.011476658392019e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 12:40:12,700] Trial 175 finished with value: 0.9347758016841892 and parameters: {'batch_size': 21, 'learning_rate': 3.530542958439217e-05, 'weight_decay': 9.377563014000326e-05, 'beta_0': 0.8152757048033007, 'beta_1': 0.9983591938725683, 'epsilon': 1.7587528110310614e-06, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 13:03:42,145] Trial 176 finished with value: 0.939100757566214 and parameters: {'batch_size': 9, 'learning_rate': 3.2322810124457365e-05, 'weight_decay': 7.382027548531672e-06, 'beta_0': 0.8273190589234987, 'beta_1': 0.9973106483706443, 'epsilon': 2.5736208704927618e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 13:28:55,148] Trial 177 finished with value: 0.9205047893993439 and parameters: {'batch_size': 10, 'learning_rate': 8.857044194511158e-05, 'weight_decay': 3.6739428766739585e-06, 'beta_0': 0.8177931293582563, 'beta_1': 0.9945570356503933, 'epsilon': 9.96400427066737e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 13:47:42,527] Trial 178 finished with value: 0.9447305771073052 and parameters: {'batch_size': 11, 'learning_rate': 2.84642269249153e-05, 'weight_decay': 0.00012088431789862504, 'beta_0': 0.8215659581397917, 'beta_1': 0.9967434541215071, 'epsilon': 2.0431130591323064e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 14:13:20,932] Trial 179 finished with value: 0.9551621510196966 and parameters: {'batch_size': 9, 'learning_rate': 3.4098425335641895e-05, 'weight_decay': 0.00046345266826529433, 'beta_0': 0.8114208444112234, 'beta_1': 0.9980722616841902, 'epsilon': 4.5884870831720096e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 14:39:46,560] Trial 180 finished with value: 0.9549178902267765 and parameters: {'batch_size': 8, 'learning_rate': 3.4059897451826674e-05, 'weight_decay': 0.0005052760139970455, 'beta_0': 0.8128767430527789, 'beta_1': 0.997905266117728, 'epsilon': 4.814415449154686e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 15:07:33,734] Trial 181 finished with value: 0.9491406900455475 and parameters: {'batch_size': 8, 'learning_rate': 2.931650694139016e-05, 'weight_decay': 0.00048655263840290423, 'beta_0': 0.8142530242177873, 'beta_1': 0.9987357206761742, 'epsilon': 4.460763493242971e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 15:22:46,647] Trial 182 finished with value: 0.9317466854335998 and parameters: {'batch_size': 8, 'learning_rate': 2.610393376423981e-05, 'weight_decay': 0.00043409276681330184, 'beta_0': 0.8129738864174929, 'beta_1': 0.9975678982451581, 'epsilon': 6.373194065861287e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 15:44:03,319] Trial 183 finished with value: 0.9560796792410182 and parameters: {'batch_size': 9, 'learning_rate': 3.4112063863145906e-05, 'weight_decay': 0.0004913191301395545, 'beta_0': 0.8122630382467592, 'beta_1': 0.9980697504485184, 'epsilon': 4.4103081941291105e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 16:07:45,382] Trial 184 finished with value: 0.9510669672225551 and parameters: {'batch_size': 9, 'learning_rate': 3.372271893514071e-05, 'weight_decay': 0.0005514431806213803, 'beta_0': 0.8108534251833346, 'beta_1': 0.998003972740533, 'epsilon': 4.635476673748154e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 16:29:10,713] Trial 185 finished with value: 0.9390959130653174 and parameters: {'batch_size': 9, 'learning_rate': 3.05400843068194e-05, 'weight_decay': 0.0007210402621854079, 'beta_0': 0.8074676660810941, 'beta_1': 0.9976530208863249, 'epsilon': 3.957166424735373e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 16:55:26,037] Trial 186 finished with value: 0.9516320978228912 and parameters: {'batch_size': 8, 'learning_rate': 3.1536604083141566e-05, 'weight_decay': 0.0006226411782983285, 'beta_0': 0.8124280834593963, 'beta_1': 0.9984304084683853, 'epsilon': 5.2111556939087786e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 17:10:21,111] Trial 187 finished with value: 0.9248224487247791 and parameters: {'batch_size': 9, 'learning_rate': 3.515249604628848e-05, 'weight_decay': 0.0003584740414204616, 'beta_0': 0.8593658723537629, 'beta_1': 0.997278380047348, 'epsilon': 7.395596676188188e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 17:29:56,288] Trial 188 finished with value: 0.8661667736306755 and parameters: {'batch_size': 8, 'learning_rate': 9.708832585788274e-05, 'weight_decay': 0.0004167430708918943, 'beta_0': 0.8090164632084388, 'beta_1': 0.9978130996255112, 'epsilon': 3.229749531618945e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 17:52:56,216] Trial 189 finished with value: 0.9506509626805223 and parameters: {'batch_size': 10, 'learning_rate': 3.376513053422434e-05, 'weight_decay': 0.0006330693496935166, 'beta_0': 0.8157599322341144, 'beta_1': 0.9981804823440874, 'epsilon': 4.058525133509385e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 18:18:35,842] Trial 190 finished with value: 0.9505728180052575 and parameters: {'batch_size': 9, 'learning_rate': 3.286240673965926e-05, 'weight_decay': 0.000532724636991155, 'beta_0': 0.8160031583978076, 'beta_1': 0.9987724528329903, 'epsilon': 4.6389674106791706e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 18:41:14,815] Trial 191 finished with value: 0.9399529543594424 and parameters: {'batch_size': 10, 'learning_rate': 1.9592012769187517e-05, 'weight_decay': 0.0004496287282154882, 'beta_0': 0.8032457600651786, 'beta_1': 0.983693599097188, 'epsilon': 1.2853900911429466e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 19:00:47,778] Trial 192 finished with value: 0.9232770933151848 and parameters: {'batch_size': 8, 'learning_rate': 7.930399034815328e-05, 'weight_decay': 0.0003037811150960315, 'beta_0': 0.8176713220175263, 'beta_1': 0.996892616908478, 'epsilon': 2.21111171960173e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 19:23:41,205] Trial 193 finished with value: 0.9622681800756139 and parameters: {'batch_size': 10, 'learning_rate': 3.566939358326762e-05, 'weight_decay': 0.00039778665386304505, 'beta_0': 0.8105091713733487, 'beta_1': 0.9982049526949771, 'epsilon': 1.3409653822963373e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 19:46:37,749] Trial 194 finished with value: 0.9442390164163966 and parameters: {'batch_size': 10, 'learning_rate': 3.8322273190957246e-05, 'weight_decay': 0.0003644709359642711, 'beta_0': 0.8108677842222057, 'beta_1': 0.9983645657770629, 'epsilon': 1.6339248952637328e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 20:10:13,996] Trial 195 finished with value: 0.9546435213682791 and parameters: {'batch_size': 9, 'learning_rate': 3.391036512742562e-05, 'weight_decay': 0.00039860730077543005, 'beta_0': 0.8140764234145857, 'beta_1': 0.9978965233261947, 'epsilon': 5.887665987615459e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 20:31:36,448] Trial 196 finished with value: 0.9351421209526356 and parameters: {'batch_size': 9, 'learning_rate': 3.6357365829658014e-05, 'weight_decay': 0.00031706409553795844, 'beta_0': 0.8128355641247221, 'beta_1': 0.9973299499499381, 'epsilon': 5.557637498765632e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 20:53:04,617] Trial 197 finished with value: 0.9468450298122055 and parameters: {'batch_size': 9, 'learning_rate': 3.468854752707516e-05, 'weight_decay': 0.0004700474205982267, 'beta_0': 0.8143868154411608, 'beta_1': 0.9977827584284797, 'epsilon': 1.1291161814276829e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 21:17:00,840] Trial 198 finished with value: 0.9582426157631526 and parameters: {'batch_size': 8, 'learning_rate': 3.220434374506662e-05, 'weight_decay': 0.0005293586119010887, 'beta_0': 0.8066747886658836, 'beta_1': 0.9910131531933252, 'epsilon': 6.176543883141614e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 21:34:25,237] Trial 199 finished with value: 0.9488032611907216 and parameters: {'batch_size': 8, 'learning_rate': 3.194618723833927e-05, 'weight_decay': 0.0006466295834956985, 'beta_0': 0.8083396344263074, 'beta_1': 0.9911294407267717, 'epsilon': 8.645949094108237e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 21:56:12,756] Trial 200 finished with value: 0.9571156424048679 and parameters: {'batch_size': 8, 'learning_rate': 3.0041336247934856e-05, 'weight_decay': 0.0008138353399997911, 'beta_0': 0.8060770436802059, 'beta_1': 0.9924918966576036, 'epsilon': 3.4708111289786564e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 22:11:31,736] Trial 201 finished with value: 0.9419256525297127 and parameters: {'batch_size': 8, 'learning_rate': 3.059546625504859e-05, 'weight_decay': 0.0005595111944218712, 'beta_0': 0.8068548547374027, 'beta_1': 0.9895209498109183, 'epsilon': 7.296861413804252e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 22:32:21,409] Trial 202 finished with value: 0.9594093226498885 and parameters: {'batch_size': 10, 'learning_rate': 2.940606164979976e-05, 'weight_decay': 0.0009493427702476526, 'beta_0': 0.8033150092053203, 'beta_1': 0.9906494670367382, 'epsilon': 4.670887094242194e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 22:49:06,130] Trial 203 finished with value: 0.9295433225991291 and parameters: {'batch_size': 10, 'learning_rate': 2.9111498831196252e-05, 'weight_decay': 0.0009302817752473759, 'beta_0': 0.8020959663629685, 'beta_1': 0.9908133328302748, 'epsilon': 5.400001595766649e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 23:03:41,970] Trial 204 finished with value: 0.9377360763518205 and parameters: {'batch_size': 11, 'learning_rate': 2.7591508381535536e-05, 'weight_decay': 0.0008694210822327681, 'beta_0': 0.8045156033680251, 'beta_1': 0.9902381961161342, 'epsilon': 3.837585638409891e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 23:18:51,611] Trial 205 finished with value: 0.9550326519661638 and parameters: {'batch_size': 9, 'learning_rate': 2.9802442827211176e-05, 'weight_decay': 0.0007837566248148281, 'beta_0': 0.8011140137369308, 'beta_1': 0.9914951075627004, 'epsilon': 4.430559953826506e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 23:40:19,067] Trial 206 finished with value: 0.9526997962847955 and parameters: {'batch_size': 9, 'learning_rate': 2.861513844259461e-05, 'weight_decay': 0.0007349724892276354, 'beta_0': 0.8002638425856815, 'beta_1': 0.9906165472797867, 'epsilon': 3.1481025593066086e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 23:59:01,835] Trial 207 finished with value: 0.9367832464402568 and parameters: {'batch_size': 10, 'learning_rate': 3.083071287782994e-05, 'weight_decay': 0.0008552689226491681, 'beta_0': 0.8034937694791667, 'beta_1': 0.9912484298646793, 'epsilon': 6.689472779468788e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 00:19:42,938] Trial 208 finished with value: 0.9407466078483022 and parameters: {'batch_size': 10, 'learning_rate': 2.695118441018756e-05, 'weight_decay': 0.0008363356997531204, 'beta_0': 0.801497067041542, 'beta_1': 0.9913937505286764, 'epsilon': 4.220166344631239e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 00:37:06,945] Trial 209 finished with value: 0.9395491995280582 and parameters: {'batch_size': 28, 'learning_rate': 3.178794342946294e-05, 'weight_decay': 0.0007533019499866504, 'beta_0': 0.8060395453047395, 'beta_1': 0.9898038336447994, 'epsilon': 1.350019393757532e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 00:51:40,303] Trial 210 finished with value: 0.9394359992837065 and parameters: {'batch_size': 11, 'learning_rate': 2.951705977348992e-05, 'weight_decay': 0.00099824879008617, 'beta_0': 0.8026105479775023, 'beta_1': 0.9888915274937631, 'epsilon': 3.341912624459057e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 01:08:46,031] Trial 211 finished with value: 0.9479923686334387 and parameters: {'batch_size': 9, 'learning_rate': 3.008300780521721e-05, 'weight_decay': 0.0007842807783155584, 'beta_0': 0.804472031255074, 'beta_1': 0.9924718773858417, 'epsilon': 3.661338808731024e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 01:23:11,281] Trial 212 finished with value: 0.9449749061512898 and parameters: {'batch_size': 10, 'learning_rate': 2.5955027908011685e-05, 'weight_decay': 0.00025307976367653447, 'beta_0': 0.8090695994985481, 'beta_1': 0.9922298728136465, 'epsilon': 8.91136628051723e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 01:38:20,009] Trial 213 finished with value: 0.9375646156314102 and parameters: {'batch_size': 8, 'learning_rate': 3.2984678162232955e-05, 'weight_decay': 0.0005486582835656393, 'beta_0': 0.806347429913877, 'beta_1': 0.9915307413346823, 'epsilon': 4.887253271096617e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 01:55:16,747] Trial 214 finished with value: 0.9465428957333795 and parameters: {'batch_size': 9, 'learning_rate': 3.203729258397302e-05, 'weight_decay': 0.0004922361679221243, 'beta_0': 0.8117628642819618, 'beta_1': 0.9918645028329053, 'epsilon': 6.818692902959704e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 02:10:18,413] Trial 215 finished with value: 0.938394453715547 and parameters: {'batch_size': 9, 'learning_rate': 2.3739045839780997e-05, 'weight_decay': 0.0006131939112960777, 'beta_0': 0.8091917317544223, 'beta_1': 0.9898974562181504, 'epsilon': 3.692783332084437e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 02:25:11,335] Trial 216 finished with value: 0.9412875342409581 and parameters: {'batch_size': 9, 'learning_rate': 2.0740293104144408e-05, 'weight_decay': 0.0006971675399400233, 'beta_0': 0.8081464132387932, 'beta_1': 0.9894142881474947, 'epsilon': 5.471007723832435e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 02:50:03,990] Trial 217 finished with value: 0.939742671125588 and parameters: {'batch_size': 10, 'learning_rate': 9.997313621008257e-05, 'weight_decay': 0.0003894552397613261, 'beta_0': 0.80008350531885, 'beta_1': 0.9908724687777604, 'epsilon': 4.570750578303147e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 03:03:04,307] Trial 218 finished with value: 0.9262215175786621 and parameters: {'batch_size': 8, 'learning_rate': 3.6257094363858614e-05, 'weight_decay': 0.000427683378791237, 'beta_0': 0.8109625312693329, 'beta_1': 0.9890737833008789, 'epsilon': 2.718520694513179e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 03:19:30,755] Trial 219 finished with value: 0.9370290990155126 and parameters: {'batch_size': 10, 'learning_rate': 2.746925639423651e-05, 'weight_decay': 0.000531044416168131, 'beta_0': 0.8064945155305919, 'beta_1': 0.9917695100404349, 'epsilon': 5.0833973503909944e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 03:34:04,073] Trial 220 finished with value: 0.9358344015911159 and parameters: {'batch_size': 11, 'learning_rate': 3.387340136687355e-05, 'weight_decay': 3.4454273190914897e-05, 'beta_0': 0.8475724198891609, 'beta_1': 0.990310047558638, 'epsilon': 8.088947468691345e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 03:48:56,340] Trial 221 finished with value: 0.9455082238461259 and parameters: {'batch_size': 9, 'learning_rate': 3.07402988592746e-05, 'weight_decay': 0.0006364885759087558, 'beta_0': 0.8123990118260823, 'beta_1': 0.9881155646708943, 'epsilon': 4.863810795939082e-06, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 04:06:26,062] Trial 222 finished with value: 0.9467334180261914 and parameters: {'batch_size': 8, 'learning_rate': 2.8698312318297454e-05, 'weight_decay': 0.000968045652574532, 'beta_0': 0.8168038127848952, 'beta_1': 0.9886341178152017, 'epsilon': 1.7278949136050607e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 04:23:48,000] Trial 223 finished with value: 0.9429646952462527 and parameters: {'batch_size': 8, 'learning_rate': 3.565688789793365e-05, 'weight_decay': 0.00048014203347669247, 'beta_0': 0.8570551322493547, 'beta_1': 0.9973277115411768, 'epsilon': 2.13698489642134e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 04:47:28,780] Trial 224 finished with value: 0.8987660055195245 and parameters: {'batch_size': 8, 'learning_rate': 9.089678774856239e-05, 'weight_decay': 0.00021038994217853222, 'beta_0': 0.8451806551687973, 'beta_1': 0.9928697275330514, 'epsilon': 2.6063433455498632e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 04:58:07,123] Trial 225 finished with value: 0.9499876909509226 and parameters: {'batch_size': 9, 'learning_rate': 3.772702458614927e-05, 'weight_decay': 1.2063868291765017e-05, 'beta_0': 0.8506050135131836, 'beta_1': 0.9926292586163061, 'epsilon': 1.175023421804954e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 05:21:56,801] Trial 226 finished with value: 0.9462766430079104 and parameters: {'batch_size': 8, 'learning_rate': 3.3919577853180635e-05, 'weight_decay': 0.00016322839570029365, 'beta_0': 0.8612951123556701, 'beta_1': 0.9970047205217822, 'epsilon': 3.57010713522e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 05:40:28,705] Trial 227 finished with value: 0.9520336091305348 and parameters: {'batch_size': 10, 'learning_rate': 3.229173752841995e-05, 'weight_decay': 7.205148251402202e-05, 'beta_0': 0.8549779960893952, 'beta_1': 0.9919522854215242, 'epsilon': 4.347713320943731e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 05:57:35,112] Trial 228 finished with value: 0.9408575269090061 and parameters: {'batch_size': 9, 'learning_rate': 3.5896787265938594e-05, 'weight_decay': 6.485775280313875e-06, 'beta_0': 0.8102496826006095, 'beta_1': 0.9932685454710585, 'epsilon': 1.5544510089351565e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 06:16:11,695] Trial 229 finished with value: 0.9443328413902852 and parameters: {'batch_size': 11, 'learning_rate': 1.715329167480529e-05, 'weight_decay': 1.4693121290160529e-05, 'beta_0': 0.8038908117511867, 'beta_1': 0.9980608610096237, 'epsilon': 1.34174583292293e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 06:39:59,656] Trial 230 finished with value: 0.9383170678728561 and parameters: {'batch_size': 8, 'learning_rate': 3.4290127472048405e-05, 'weight_decay': 0.0008352629930413273, 'beta_0': 0.8190438024069454, 'beta_1': 0.9805866239094798, 'epsilon': 4.474387169259439e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 06:56:29,192] Trial 231 finished with value: 0.9359673780550788 and parameters: {'batch_size': 10, 'learning_rate': 2.9852015679787755e-05, 'weight_decay': 5.695188589585594e-06, 'beta_0': 0.8123897294027644, 'beta_1': 0.9905688712087573, 'epsilon': 3.0642790425148194e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 07:17:37,257] Trial 232 finished with value: 0.948146139872346 and parameters: {'batch_size': 9, 'learning_rate': 3.2534104337233146e-05, 'weight_decay': 7.980622786027882e-06, 'beta_0': 0.8148393011284029, 'beta_1': 0.9975635870056548, 'epsilon': 9.67069716160821e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 07:34:40,989] Trial 233 finished with value: 0.9493493908467685 and parameters: {'batch_size': 9, 'learning_rate': 3.4051904051277094e-05, 'weight_decay': 0.0003715189673829825, 'beta_0': 0.8142999605362475, 'beta_1': 0.9984709695057827, 'epsilon': 6.46551097214272e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 07:58:10,315] Trial 234 finished with value: 0.9407649077918578 and parameters: {'batch_size': 9, 'learning_rate': 3.1105096587584225e-05, 'weight_decay': 0.0003136501869500258, 'beta_0': 0.8132805513818732, 'beta_1': 0.998997322396679, 'epsilon': 5.786299387295374e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 08:14:39,870] Trial 235 finished with value: 0.9343972423900169 and parameters: {'batch_size': 10, 'learning_rate': 3.7248662476300264e-05, 'weight_decay': 0.00046588472361584434, 'beta_0': 0.8094600597917755, 'beta_1': 0.9936660992785472, 'epsilon': 5.817698856866724e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 08:29:48,023] Trial 236 finished with value: 0.9547044979983059 and parameters: {'batch_size': 8, 'learning_rate': 3.426153492103461e-05, 'weight_decay': 0.00038931369479550695, 'beta_0': 0.8587666212918853, 'beta_1': 0.9978875692210272, 'epsilon': 2.8801029517627663e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 08:44:58,775] Trial 237 finished with value: 0.9334739970354096 and parameters: {'batch_size': 8, 'learning_rate': 3.518345215770423e-05, 'weight_decay': 0.0005762195318790634, 'beta_0': 0.8584415063749461, 'beta_1': 0.9980800317655563, 'epsilon': 3.030453116964406e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 09:00:09,370] Trial 238 finished with value: 0.9285933254258546 and parameters: {'batch_size': 8, 'learning_rate': 3.9612316596805554e-05, 'weight_decay': 0.00011122192606111071, 'beta_0': 0.8632787618782667, 'beta_1': 0.9966852609691227, 'epsilon': 2.2896589325190363e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 09:13:35,163] Trial 239 finished with value: 0.9380273475874235 and parameters: {'batch_size': 31, 'learning_rate': 3.293247641566014e-05, 'weight_decay': 0.00041474003701074715, 'beta_0': 0.8653647068789243, 'beta_1': 0.9974708876569774, 'epsilon': 3.343254819963226e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 09:22:16,865] Trial 240 finished with value: 0.8841353959640414 and parameters: {'batch_size': 8, 'learning_rate': 7.092117399158028e-05, 'weight_decay': 0.00027969809538110063, 'beta_0': 0.855482056440613, 'beta_1': 0.9971617020878463, 'epsilon': 2.706016639951434e-06, 'balanced_loss': True, 'epochs': 4, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 09:34:44,070] Trial 241 finished with value: 0.9359223309059805 and parameters: {'batch_size': 10, 'learning_rate': 6.529175468391335e-05, 'weight_decay': 9.705332728664346e-06, 'beta_0': 0.8070747473889367, 'beta_1': 0.9910996022007827, 'epsilon': 1.908149162203376e-06, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 09:51:18,493] Trial 242 finished with value: 0.9331586497920057 and parameters: {'batch_size': 11, 'learning_rate': 2.817766031372503e-05, 'weight_decay': 6.764453892189546e-06, 'beta_0': 0.8602411691670824, 'beta_1': 0.9980277767478755, 'epsilon': 4.378996532246648e-06, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 10:08:16,176] Trial 243 finished with value: 0.9540383808327156 and parameters: {'batch_size': 9, 'learning_rate': 3.510143969287272e-05, 'weight_decay': 0.00045380137355385196, 'beta_0': 0.852567146648012, 'beta_1': 0.9978055988411689, 'epsilon': 5.837829807647044e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 10:25:19,920] Trial 244 finished with value: 0.9258386051348838 and parameters: {'batch_size': 9, 'learning_rate': 3.332996314292977e-05, 'weight_decay': 0.0003825016878603704, 'beta_0': 0.8202456194200844, 'beta_1': 0.9982629173032557, 'epsilon': 9.859258610226204e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 10:42:37,851] Trial 245 finished with value: 0.9337552007816186 and parameters: {'batch_size': 8, 'learning_rate': 3.150331167148543e-05, 'weight_decay': 0.00032518812184296976, 'beta_0': 0.8166654717000108, 'beta_1': 0.9977085107751716, 'epsilon': 7.738397430014094e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 10:59:21,603] Trial 246 finished with value: 0.9574281756937143 and parameters: {'batch_size': 10, 'learning_rate': 3.6156576723561e-05, 'weight_decay': 0.00040496635576864695, 'beta_0': 0.8113096402061177, 'beta_1': 0.9978843024891023, 'epsilon': 3.570373042365861e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 11:15:56,786] Trial 247 finished with value: 0.9466455575603154 and parameters: {'batch_size': 10, 'learning_rate': 3.7100895256041454e-05, 'weight_decay': 0.0005143047933794792, 'beta_0': 0.8107669631078678, 'beta_1': 0.9855207460247187, 'epsilon': 3.2444902392969476e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 11:28:20,813] Trial 248 finished with value: 0.9360068198641653 and parameters: {'batch_size': 10, 'learning_rate': 1.5963764851899713e-05, 'weight_decay': 0.0006792459046823012, 'beta_0': 0.8084224741417858, 'beta_1': 0.996993643976662, 'epsilon': 4.264047354024507e-06, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 11:55:27,156] Trial 249 finished with value: 0.9565068979596916 and parameters: {'batch_size': 11, 'learning_rate': 3.598740739985473e-05, 'weight_decay': 0.00037363022767675626, 'beta_0': 0.8051309671286303, 'beta_1': 0.9893248440716197, 'epsilon': 3.6387410146207234e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 12:12:06,933] Trial 250 finished with value: 0.942897026272591 and parameters: {'batch_size': 11, 'learning_rate': 3.636510281430419e-05, 'weight_decay': 0.0003891827419382894, 'beta_0': 0.8051627025115705, 'beta_1': 0.9893097928857799, 'epsilon': 4.615599496484574e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 12:37:03,820] Trial 251 finished with value: 0.9458047152928957 and parameters: {'batch_size': 11, 'learning_rate': 3.7613062534528385e-05, 'weight_decay': 0.0003398014384072408, 'beta_0': 0.8070826950944998, 'beta_1': 0.9899693527680699, 'epsilon': 3.7927192708255686e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-17 12:51:36,253] Trial 252 finished with value: 0.9412146323202906 and parameters: {'batch_size': 10, 'learning_rate': 2.1759701617911166e-05, 'weight_decay': 0.0004188497245648098, 'beta_0': 0.8111014056410671, 'beta_1': 0.988943977729859, 'epsilon': 2.8352854665988326e-06, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 169 with value: 0.9644723116113745.

[TRIAL] 169 [VALIDATION PERFORMANCE] 0.9644723116113745 [TRAINING LOSS] 0.017854883373982288 [VALIDATION LOSS] 0.08599120847939049 

number                                 169
value                             0.964472
params_balanced_loss                 False
params_batch_size                       10
params_early_stopping_patience           5
params_epochs                            8
params_learning_rate              0.000028
params_plateau_divider                   7
params_plateau_patience                  4
params_weight_decay               0.000008
params_beta_0                     0.820636
params_beta_1                     0.996903
params_epsilon                    0.000003
user_attrs_epoch                       7.0
user_attrs_training_loss          0.017855
user_attrs_validation_loss        0.085991
Name: 169, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.9558122000465813 Test: 0.9462116635630302
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.9274309106060772 Test: 0.9400766771101379
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.9407018461648898 Test: 0.9539781223868107
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.9371918153466587 Test: 0.9395731303197554
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.9436521487449641 Test: 0.9452589091921264
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.9644723116113745 Test: 0.9390166589701412
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.9401454791729205 Test: 0.9475433348437398
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
slurmstepd: error: *** JOB 14472727 ON gpu056 CANCELLED AT 2025-01-17T14:56:34 DUE TO TIME LIMIT ***
