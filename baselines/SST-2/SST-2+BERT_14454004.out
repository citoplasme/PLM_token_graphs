[I 2025-01-15 18:30:51,051] Using an existing study with name 'SST-2-google-bert-bert-base-uncased' instead of creating a new one.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:52:06,719] Trial 116 finished with value: 0.9185779816513762 and parameters: {'batch_size': 16, 'learning_rate': 1.1630370925579621e-05, 'weight_decay': 0.0004394458068241617, 'beta_0': 0.8464736605356763, 'beta_1': 0.9931922733953252, 'epsilon': 7.8681800501275e-05, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:07:38,854] Trial 117 finished with value: 0.9231651376146789 and parameters: {'batch_size': 19, 'learning_rate': 1.2210190578236362e-05, 'weight_decay': 0.00020044498767227042, 'beta_0': 0.8357723965473522, 'beta_1': 0.9953772597254381, 'epsilon': 5.1205778742396355e-05, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:25:20,454] Trial 118 finished with value: 0.9174311926605505 and parameters: {'batch_size': 23, 'learning_rate': 1.1065857962322535e-05, 'weight_decay': 0.0008226712259454274, 'beta_0': 0.8424006236369623, 'beta_1': 0.9915998360524182, 'epsilon': 7.810217641061298e-05, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:46:19,706] Trial 119 finished with value: 0.9197247706422018 and parameters: {'batch_size': 14, 'learning_rate': 1.2433626487948468e-05, 'weight_decay': 0.00032084396238452707, 'beta_0': 0.8542254588881317, 'beta_1': 0.9943570673436365, 'epsilon': 2.65514495187403e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:09:03,164] Trial 120 finished with value: 0.9185779816513762 and parameters: {'batch_size': 11, 'learning_rate': 1.4233491035919058e-05, 'weight_decay': 0.00058374580593341, 'beta_0': 0.8517153111516258, 'beta_1': 0.9931928374416832, 'epsilon': 2.696233396373973e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:23:51,259] Trial 121 finished with value: 0.9185779816513762 and parameters: {'batch_size': 14, 'learning_rate': 1.0005255141338422e-05, 'weight_decay': 0.0003464615394813761, 'beta_0': 0.849540135858219, 'beta_1': 0.9938661232971717, 'epsilon': 1.616927366168387e-05, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:36:05,359] Trial 122 finished with value: 0.911697247706422 and parameters: {'batch_size': 16, 'learning_rate': 5.010523020214343e-05, 'weight_decay': 6.890729528428029e-05, 'beta_0': 0.8713812841038436, 'beta_1': 0.9924079442442388, 'epsilon': 9.75822781790646e-07, 'balanced_loss': True, 'epochs': 4, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:57:34,708] Trial 123 finished with value: 0.9220183486238532 and parameters: {'batch_size': 13, 'learning_rate': 1.3486752852467903e-05, 'weight_decay': 5.3262818595462966e-05, 'beta_0': 0.839658012353663, 'beta_1': 0.9881334951299372, 'epsilon': 3.29490007893718e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:15:51,651] Trial 124 finished with value: 0.9220183486238532 and parameters: {'batch_size': 13, 'learning_rate': 1.1588138214587914e-05, 'weight_decay': 4.4579541733016866e-05, 'beta_0': 0.8365366930217383, 'beta_1': 0.9918989062295155, 'epsilon': 7.63673026108592e-06, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:31:07,767] Trial 125 finished with value: 0.9151376146788991 and parameters: {'batch_size': 12, 'learning_rate': 1.0897184774264702e-05, 'weight_decay': 0.0005226875557935547, 'beta_0': 0.8422136563556328, 'beta_1': 0.9947560628711686, 'epsilon': 9.808866480477109e-06, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:52:17,185] Trial 126 finished with value: 0.9105504587155964 and parameters: {'batch_size': 15, 'learning_rate': 1.2736754905844128e-05, 'weight_decay': 0.0002614274268952057, 'beta_0': 0.8448334893182796, 'beta_1': 0.9887976887168711, 'epsilon': 6.069143177210811e-06, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:06:56,579] Trial 127 finished with value: 0.9185779816513762 and parameters: {'batch_size': 31, 'learning_rate': 1.0464531355661572e-05, 'weight_decay': 2.2020758264015226e-05, 'beta_0': 0.8888018655390547, 'beta_1': 0.9909806579149666, 'epsilon': 5.69883288966138e-07, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:21:40,889] Trial 128 finished with value: 0.9197247706422018 and parameters: {'batch_size': 14, 'learning_rate': 1.1997843920682797e-05, 'weight_decay': 2.873619138815141e-05, 'beta_0': 0.8405854836289863, 'beta_1': 0.9928075906871422, 'epsilon': 3.302496654442276e-06, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 8}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:40:10,451] Trial 129 finished with value: 0.9220183486238532 and parameters: {'batch_size': 12, 'learning_rate': 1.1218767223582866e-05, 'weight_decay': 3.8249365053177424e-05, 'beta_0': 0.8585509895906346, 'beta_1': 0.9934921816510772, 'epsilon': 2.2882615249657302e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:55:16,573] Trial 130 finished with value: 0.9197247706422018 and parameters: {'batch_size': 18, 'learning_rate': 1.0843810462275116e-05, 'weight_decay': 0.00019982373934756325, 'beta_0': 0.8471489482348903, 'beta_1': 0.994118546859493, 'epsilon': 4.149972523652881e-07, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:13:34,828] Trial 131 finished with value: 0.9231651376146789 and parameters: {'batch_size': 13, 'learning_rate': 1.535774849267074e-05, 'weight_decay': 3.4083345533567956e-06, 'beta_0': 0.8341044516948151, 'beta_1': 0.9914046112880831, 'epsilon': 1.5258862626954884e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:34:12,982] Trial 132 finished with value: 0.9197247706422018 and parameters: {'batch_size': 25, 'learning_rate': 2.1835012481248494e-05, 'weight_decay': 5.833555401331454e-05, 'beta_0': 0.8434932240322348, 'beta_1': 0.9903750244011973, 'epsilon': 3.026441000531445e-07, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:52:41,050] Trial 133 finished with value: 0.9254587155963303 and parameters: {'batch_size': 13, 'learning_rate': 1.0491908716385902e-05, 'weight_decay': 3.279324213588979e-05, 'beta_0': 0.894159639934021, 'beta_1': 0.995278268138964, 'epsilon': 7.966791568763894e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:11:01,199] Trial 134 finished with value: 0.9151376146788991 and parameters: {'batch_size': 13, 'learning_rate': 1.0419305583689485e-05, 'weight_decay': 3.184314677238527e-05, 'beta_0': 0.8905076454559139, 'beta_1': 0.9963439254217256, 'epsilon': 7.991333756840392e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:28:43,336] Trial 135 finished with value: 0.9174311926605505 and parameters: {'batch_size': 14, 'learning_rate': 1.1674092359214097e-05, 'weight_decay': 1.762278540813585e-05, 'beta_0': 0.8939992176552343, 'beta_1': 0.995690680913053, 'epsilon': 7.373098160807779e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:47:11,379] Trial 136 finished with value: 0.9243119266055045 and parameters: {'batch_size': 12, 'learning_rate': 1.0404698413280777e-05, 'weight_decay': 4.6376830535303066e-05, 'beta_0': 0.8977020133289247, 'beta_1': 0.9950187003763356, 'epsilon': 1.7150204532634787e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:03:08,657] Trial 137 finished with value: 0.9208715596330275 and parameters: {'batch_size': 11, 'learning_rate': 1.0056538609591558e-05, 'weight_decay': 4.467255030718763e-05, 'beta_0': 0.8957578928168445, 'beta_1': 0.9946304844769054, 'epsilon': 8.479226520543893e-08, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:21:41,787] Trial 138 finished with value: 0.9231651376146789 and parameters: {'batch_size': 12, 'learning_rate': 1.1209548366253987e-05, 'weight_decay': 3.831883363705576e-05, 'beta_0': 0.893021470597034, 'beta_1': 0.9952525800846923, 'epsilon': 1.2014072926428578e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:36:39,356] Trial 139 finished with value: 0.9220183486238532 and parameters: {'batch_size': 14, 'learning_rate': 1.0458508479326773e-05, 'weight_decay': 5.241116013373364e-05, 'beta_0': 0.8979641292981073, 'beta_1': 0.995062592489088, 'epsilon': 1.8091935203974727e-07, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:58:19,538] Trial 140 finished with value: 0.9254587155963303 and parameters: {'batch_size': 12, 'learning_rate': 1.2566353262549646e-05, 'weight_decay': 6.482760214849523e-05, 'beta_0': 0.892210167168351, 'beta_1': 0.993621072736918, 'epsilon': 5.113017092605489e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:20:26,772] Trial 141 finished with value: 0.9185779816513762 and parameters: {'batch_size': 10, 'learning_rate': 1.2483480877701449e-05, 'weight_decay': 7.668058964630984e-05, 'beta_0': 0.8923897178651755, 'beta_1': 0.9937085544604198, 'epsilon': 4.7043581860491715e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:41:43,909] Trial 142 finished with value: 0.9208715596330275 and parameters: {'batch_size': 12, 'learning_rate': 1.339943268966798e-05, 'weight_decay': 6.0628105025931124e-05, 'beta_0': 0.8863974982704736, 'beta_1': 0.994271691630803, 'epsilon': 6.178362466088573e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:03:38,944] Trial 143 finished with value: 0.9197247706422018 and parameters: {'batch_size': 11, 'learning_rate': 1.1451493224343986e-05, 'weight_decay': 2.518447577742804e-05, 'beta_0': 0.8831875669008968, 'beta_1': 0.9932896433541517, 'epsilon': 3.7444286915126266e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:27:54,775] Trial 144 finished with value: 0.9254587155963303 and parameters: {'batch_size': 13, 'learning_rate': 1.0866968479884418e-05, 'weight_decay': 4.5612773589433e-05, 'beta_0': 0.8976243135329005, 'beta_1': 0.9922576867126961, 'epsilon': 5.082978550061693e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:49:10,892] Trial 145 finished with value: 0.9231651376146789 and parameters: {'batch_size': 13, 'learning_rate': 1.081682412457783e-05, 'weight_decay': 4.692568650112893e-05, 'beta_0': 0.8980363320757353, 'beta_1': 0.9925869152128378, 'epsilon': 5.447889233028032e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:10:27,880] Trial 146 finished with value: 0.9174311926605505 and parameters: {'batch_size': 12, 'learning_rate': 1.2156776820900813e-05, 'weight_decay': 3.535305143433107e-05, 'beta_0': 0.8953386706080969, 'beta_1': 0.9921670100062098, 'epsilon': 4.524588713231931e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:33:49,215] Trial 147 finished with value: 0.9208715596330275 and parameters: {'batch_size': 21, 'learning_rate': 1.035585351988411e-05, 'weight_decay': 0.00011964034553429195, 'beta_0': 0.8997562522534923, 'beta_1': 0.9929574211920956, 'epsilon': 1.0224208062963476e-06, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:55:11,050] Trial 148 finished with value: 0.9220183486238532 and parameters: {'batch_size': 13, 'learning_rate': 1.0901483913189164e-05, 'weight_decay': 6.531504250005012e-05, 'beta_0': 0.8857045011585414, 'beta_1': 0.9939064015584113, 'epsilon': 2.5221053664871134e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:16:09,657] Trial 149 finished with value: 0.9185779816513762 and parameters: {'batch_size': 14, 'learning_rate': 1.1524889314772148e-05, 'weight_decay': 3.0141056888957166e-05, 'beta_0': 0.8898459729956331, 'beta_1': 0.9947155256885092, 'epsilon': 8.318499677407108e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:38:05,421] Trial 150 finished with value: 0.9197247706422018 and parameters: {'batch_size': 11, 'learning_rate': 1.2788790899771105e-05, 'weight_decay': 4.336877506963155e-05, 'beta_0': 0.895297851490022, 'beta_1': 0.991727949366645, 'epsilon': 1.3541533727489753e-06, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:59:27,882] Trial 151 finished with value: 0.9220183486238532 and parameters: {'batch_size': 12, 'learning_rate': 1.0451254462420602e-05, 'weight_decay': 8.470086516210221e-05, 'beta_0': 0.8918924068292128, 'beta_1': 0.9935449641898088, 'epsilon': 6.352635541538009e-07, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:20:54,809] Trial 152 finished with value: 0.9231651376146789 and parameters: {'batch_size': 13, 'learning_rate': 1.1908746961451665e-05, 'weight_decay': 5.1767228181318256e-05, 'beta_0': 0.8495845570718209, 'beta_1': 0.995718161979408, 'epsilon': 1.861768216505274e-06, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:42:16,145] Trial 153 finished with value: 0.9174311926605505 and parameters: {'batch_size': 12, 'learning_rate': 1.1141956502731937e-05, 'weight_decay': 4.0243035909466584e-05, 'beta_0': 0.8984881576932083, 'beta_1': 0.992358230764918, 'epsilon': 3.397572871159768e-07, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:00:43,945] Trial 154 finished with value: 0.9105504587155964 and parameters: {'batch_size': 13, 'learning_rate': 3.955688190016757e-05, 'weight_decay': 3.4612979417984804e-05, 'beta_0': 0.896335572463423, 'beta_1': 0.993153609468711, 'epsilon': 1.6536957866274487e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:18:55,305] Trial 155 finished with value: 0.9128440366972477 and parameters: {'batch_size': 15, 'learning_rate': 4.5331175280246945e-05, 'weight_decay': 4.9826898541903074e-05, 'beta_0': 0.8804680918599233, 'beta_1': 0.9941616394124263, 'epsilon': 5.155057379223768e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:39:37,288] Trial 156 finished with value: 0.9220183486238532 and parameters: {'batch_size': 14, 'learning_rate': 1.0183322674755034e-05, 'weight_decay': 0.0004078433097403572, 'beta_0': 0.8936324289242512, 'beta_1': 0.992757169391978, 'epsilon': 3.3324845177919196e-05, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 5}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 07:48:54,516] Trial 157 finished with value: 0.9151376146788991 and parameters: {'batch_size': 13, 'learning_rate': 3.203364687236155e-05, 'weight_decay': 7.014747194832942e-05, 'beta_0': 0.889177261363297, 'beta_1': 0.9934614242572961, 'epsilon': 5.330303937460121e-05, 'balanced_loss': True, 'epochs': 3, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:01:13,078] Trial 158 finished with value: 0.9151376146788991 and parameters: {'batch_size': 15, 'learning_rate': 1.0923692971606247e-05, 'weight_decay': 5.645919332988991e-05, 'beta_0': 0.8946804275442952, 'beta_1': 0.9920283099956418, 'epsilon': 4.0159658575674167e-07, 'balanced_loss': True, 'epochs': 4, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:16:36,176] Trial 159 finished with value: 0.9231651376146789 and parameters: {'batch_size': 13, 'learning_rate': 1.0033877022994977e-05, 'weight_decay': 4.388655259942059e-05, 'beta_0': 0.8518477992770471, 'beta_1': 0.9950115662657489, 'epsilon': 2.1119227456654923e-05, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:38:30,791] Trial 160 finished with value: 0.9197247706422018 and parameters: {'batch_size': 10, 'learning_rate': 4.1500737857119966e-05, 'weight_decay': 2.59853755291944e-05, 'beta_0': 0.8975323517971963, 'beta_1': 0.9839332098279168, 'epsilon': 3.049108987790815e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 08:50:46,539] Trial 161 finished with value: 0.9139908256880734 and parameters: {'batch_size': 12, 'learning_rate': 1.2305643802028305e-05, 'weight_decay': 0.00010154551053892317, 'beta_0': 0.810628774377485, 'beta_1': 0.9944832784631095, 'epsilon': 2.1366354448540013e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:08:32,855] Trial 162 finished with value: 0.9185779816513762 and parameters: {'batch_size': 14, 'learning_rate': 5.160090826677861e-05, 'weight_decay': 3.7268135432995e-05, 'beta_0': 0.8478688536579618, 'beta_1': 0.9924816181407089, 'epsilon': 9.757326203684256e-05, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:32:34,365] Trial 163 finished with value: 0.9162844036697247 and parameters: {'batch_size': 24, 'learning_rate': 1.1357220746498813e-05, 'weight_decay': 3.092993783691538e-05, 'beta_0': 0.8916371694915467, 'beta_1': 0.9938566979558568, 'epsilon': 6.481989902608665e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 09:50:56,744] Trial 164 finished with value: 0.9254587155963303 and parameters: {'batch_size': 13, 'learning_rate': 1.1678081714702778e-05, 'weight_decay': 1.6852695329683245e-06, 'beta_0': 0.8376718785278685, 'beta_1': 0.9957092986325324, 'epsilon': 1.0331503425767365e-06, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:09:14,783] Trial 165 finished with value: 0.9243119266055045 and parameters: {'batch_size': 13, 'learning_rate': 1.0656408312991597e-05, 'weight_decay': 0.0004391899504405885, 'beta_0': 0.8390987592479452, 'beta_1': 0.9953326219571426, 'epsilon': 1.2605379367347843e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 10:27:05,905] Trial 166 finished with value: 0.9151376146788991 and parameters: {'batch_size': 14, 'learning_rate': 1.0702326935399883e-05, 'weight_decay': 1.8620874863692736e-06, 'beta_0': 0.8361014800771712, 'beta_1': 0.9964928343111545, 'epsilon': 1.3573148750971254e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 63 with value: 0.9277522935779816.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
slurmstepd: error: *** JOB 14454004 ON gpu053 CANCELLED AT 2025-01-16T10:30:41 DUE TO TIME LIMIT ***
