[I 2025-01-14 10:31:17,245] A new study created in RDB with name: IMDb-top_1000-google-bert-bert-base-uncased
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:33:53,474] Trial 0 finished with value: 0.7878787878787878 and parameters: {'batch_size': 17, 'learning_rate': 8.927180304353642e-05, 'weight_decay': 0.000157029708840554, 'beta_0': 0.8584457968114998, 'beta_1': 0.982940386538606, 'epsilon': 4.207053950287927e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 0 with value: 0.7878787878787878.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:36:22,187] Trial 1 finished with value: 0.793939393939394 and parameters: {'batch_size': 28, 'learning_rate': 1.6305687346221468e-05, 'weight_decay': 3.5113563139704077e-06, 'beta_0': 0.817469560807162, 'beta_1': 0.9857420365812439, 'epsilon': 1.2561043700013556e-06, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:37:13,114] Trial 2 finished with value: 0.7515151515151515 and parameters: {'batch_size': 19, 'learning_rate': 6.097839109531521e-05, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 3, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:38:55,330] Trial 3 finished with value: 0.7515151515151515 and parameters: {'batch_size': 15, 'learning_rate': 1.2521954287060383e-05, 'weight_decay': 0.00011290133559092664, 'beta_0': 0.8425678764767325, 'beta_1': 0.9822992283341428, 'epsilon': 9.565499215943814e-07, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:40:02,986] Trial 4 finished with value: 0.7696969696969697 and parameters: {'batch_size': 21, 'learning_rate': 1.5305744365500174e-05, 'weight_decay': 0.0008105016126411582, 'beta_0': 0.8764759143554902, 'beta_1': 0.9978400818071855, 'epsilon': 3.79585314267064e-05, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:40:56,939] Trial 5 finished with value: 0.7757575757575758 and parameters: {'batch_size': 17, 'learning_rate': 1.8678802571070677e-05, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 3, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:43:30,847] Trial 6 finished with value: 0.5333333333333333 and parameters: {'batch_size': 8, 'learning_rate': 6.538248584518044e-05, 'weight_decay': 0.00013199942261535007, 'beta_0': 0.8717270901699462, 'beta_1': 0.9946218995643121, 'epsilon': 1.9777828512462707e-08, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 1 with value: 0.793939393939394.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:45:45,322] Trial 7 finished with value: 0.8303030303030303 and parameters: {'batch_size': 15, 'learning_rate': 2.1143813626634362e-05, 'weight_decay': 0.0001544608907504709, 'beta_0': 0.8623879108708258, 'beta_1': 0.9968387427337244, 'epsilon': 7.742116473996242e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:47:21,713] Trial 8 finished with value: 0.7878787878787878 and parameters: {'batch_size': 21, 'learning_rate': 2.676338356766077e-05, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:49:24,942] Trial 9 finished with value: 0.7757575757575758 and parameters: {'batch_size': 13, 'learning_rate': 1.193932872653544e-05, 'weight_decay': 7.400385759087375e-06, 'beta_0': 0.8153364582789058, 'beta_1': 0.9976522996046951, 'epsilon': 1.7079750342958218e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:51:15,481] Trial 10 finished with value: 0.7757575757575758 and parameters: {'batch_size': 27, 'learning_rate': 3.621186546958565e-05, 'weight_decay': 3.105109516983293e-05, 'beta_0': 0.8928510005356737, 'beta_1': 0.9921196188932835, 'epsilon': 1.5931596269242544e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:53:09,574] Trial 11 finished with value: 0.7757575757575758 and parameters: {'batch_size': 31, 'learning_rate': 2.4088862286846696e-05, 'weight_decay': 2.5907951129151548e-05, 'beta_0': 0.82509856079587, 'beta_1': 0.9871444797989524, 'epsilon': 5.110333258151407e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:55:01,008] Trial 12 finished with value: 0.8121212121212121 and parameters: {'batch_size': 25, 'learning_rate': 3.838340232589207e-05, 'weight_decay': 7.667817733535309e-06, 'beta_0': 0.8031709457399874, 'beta_1': 0.987685241135843, 'epsilon': 7.0454900978099716e-06, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:57:09,532] Trial 13 finished with value: 0.8121212121212121 and parameters: {'batch_size': 24, 'learning_rate': 4.0170095541939625e-05, 'weight_decay': 1.736911268865492e-05, 'beta_0': 0.8012391236721735, 'beta_1': 0.9898006722448077, 'epsilon': 7.384417907619309e-06, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:59:31,616] Trial 14 finished with value: 0.793939393939394 and parameters: {'batch_size': 11, 'learning_rate': 4.6245805136055444e-05, 'weight_decay': 5.872127889434875e-05, 'beta_0': 0.8585996401312975, 'beta_1': 0.994488204264689, 'epsilon': 2.5473383418482205e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:02:20,962] Trial 15 finished with value: 0.806060606060606 and parameters: {'batch_size': 24, 'learning_rate': 2.1858737455540064e-05, 'weight_decay': 9.902838489335508e-06, 'beta_0': 0.8900814115808905, 'beta_1': 0.9872326681482421, 'epsilon': 8.667237605443695e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:04:47,712] Trial 16 finished with value: 0.8303030303030303 and parameters: {'batch_size': 25, 'learning_rate': 2.967491551940229e-05, 'weight_decay': 0.0006655692967202516, 'beta_0': 0.873293400550193, 'beta_1': 0.9939346835987791, 'epsilon': 4.8411425337539145e-06, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:07:38,955] Trial 17 finished with value: 0.8 and parameters: {'batch_size': 31, 'learning_rate': 2.8032566884885152e-05, 'weight_decay': 0.0006748310344695067, 'beta_0': 0.8780244129496778, 'beta_1': 0.9951229431279005, 'epsilon': 8.582657323759748e-08, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:10:40,630] Trial 18 finished with value: 0.8 and parameters: {'batch_size': 10, 'learning_rate': 1.9246873275243454e-05, 'weight_decay': 0.0003402715975019958, 'beta_0': 0.8678158051352584, 'beta_1': 0.9989275941550687, 'epsilon': 3.208627477483364e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:12:53,943] Trial 19 finished with value: 0.8 and parameters: {'batch_size': 14, 'learning_rate': 3.0070977669574785e-05, 'weight_decay': 0.00036131139759568203, 'beta_0': 0.8832994146625218, 'beta_1': 0.9928146251106333, 'epsilon': 3.445349859964118e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 2}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:14:50,196] Trial 20 finished with value: 0.8242424242424242 and parameters: {'batch_size': 18, 'learning_rate': 5.811246581208494e-05, 'weight_decay': 6.262451332882486e-05, 'beta_0': 0.8978599719411914, 'beta_1': 0.996320517583881, 'epsilon': 1.86647593155435e-05, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:17:20,101] Trial 21 finished with value: 0.806060606060606 and parameters: {'batch_size': 19, 'learning_rate': 5.323488169241438e-05, 'weight_decay': 5.8524697452190556e-05, 'beta_0': 0.8954889609714233, 'beta_1': 0.9963078659838899, 'epsilon': 1.5575294293783444e-05, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:19:29,236] Trial 22 finished with value: 0.7878787878787878 and parameters: {'batch_size': 16, 'learning_rate': 9.03150704565401e-05, 'weight_decay': 6.808735221672077e-05, 'beta_0': 0.8996251389694556, 'beta_1': 0.9966628533488048, 'epsilon': 2.727710994546435e-05, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:21:53,345] Trial 23 finished with value: 0.7575757575757576 and parameters: {'batch_size': 22, 'learning_rate': 7.245041114409591e-05, 'weight_decay': 0.00022371159615275577, 'beta_0': 0.8636098802917475, 'beta_1': 0.9939012282781566, 'epsilon': 3.3543086464663798e-06, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:24:23,739] Trial 24 finished with value: 0.8121212121212121 and parameters: {'batch_size': 12, 'learning_rate': 3.373361317080348e-05, 'weight_decay': 0.0006664488835756058, 'beta_0': 0.8848899233169466, 'beta_1': 0.995908761752888, 'epsilon': 7.043714072308651e-05, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:26:19,856] Trial 25 finished with value: 0.793939393939394 and parameters: {'batch_size': 18, 'learning_rate': 4.79449594781219e-05, 'weight_decay': 8.930195650930669e-05, 'beta_0': 0.848491532634582, 'beta_1': 0.9909248105731975, 'epsilon': 8.028224069558606e-06, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:28:58,397] Trial 26 finished with value: 0.8303030303030303 and parameters: {'batch_size': 28, 'learning_rate': 2.2798372406124074e-05, 'weight_decay': 0.00020501050058361445, 'beta_0': 0.8826106331026674, 'beta_1': 0.9929831805523113, 'epsilon': 1.944509788290651e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:31:22,950] Trial 27 finished with value: 0.8242424242424242 and parameters: {'batch_size': 28, 'learning_rate': 2.2945418176493397e-05, 'weight_decay': 0.0004227451794483051, 'beta_0': 0.8729359505470714, 'beta_1': 0.9934568515184998, 'epsilon': 7.428141841987381e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:34:53,625] Trial 28 finished with value: 0.806060606060606 and parameters: {'batch_size': 26, 'learning_rate': 1.4576027806628203e-05, 'weight_decay': 0.00018099550379319866, 'beta_0': 0.8599643568515766, 'beta_1': 0.9919173968351561, 'epsilon': 1.9865924868723366e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:37:19,716] Trial 29 finished with value: 0.7757575757575758 and parameters: {'batch_size': 29, 'learning_rate': 1.7813180727761445e-05, 'weight_decay': 0.00022076314896277046, 'beta_0': 0.8522712436937954, 'beta_1': 0.9896018430695239, 'epsilon': 6.236039920397267e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:40:19,062] Trial 30 finished with value: 0.8242424242424242 and parameters: {'batch_size': 23, 'learning_rate': 2.120009648779377e-05, 'weight_decay': 0.0009609522161575113, 'beta_0': 0.8828617910085215, 'beta_1': 0.9986779019254786, 'epsilon': 5.69748295896848e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:42:41,675] Trial 31 finished with value: 0.806060606060606 and parameters: {'batch_size': 30, 'learning_rate': 2.488873724496871e-05, 'weight_decay': 4.318245101243105e-05, 'beta_0': 0.8882702329215669, 'beta_1': 0.9959428291813089, 'epsilon': 1.366806587886506e-05, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:45:24,649] Trial 32 finished with value: 0.7818181818181819 and parameters: {'batch_size': 32, 'learning_rate': 3.200163691365065e-05, 'weight_decay': 0.0005119469704032863, 'beta_0': 0.8676411461515143, 'beta_1': 0.9973916727256574, 'epsilon': 2.057780378969415e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:47:35,023] Trial 33 finished with value: 0.8 and parameters: {'batch_size': 20, 'learning_rate': 7.799321377079026e-05, 'weight_decay': 0.00014069240015196358, 'beta_0': 0.8992523196366248, 'beta_1': 0.9953755186617486, 'epsilon': 4.0647804642749595e-06, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:49:44,785] Trial 34 finished with value: 0.8121212121212121 and parameters: {'batch_size': 16, 'learning_rate': 4.32424555473761e-05, 'weight_decay': 0.00010347625325215321, 'beta_0': 0.8799748070807196, 'beta_1': 0.9927916051634621, 'epsilon': 1.0361425084991099e-06, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:53:15,461] Trial 35 finished with value: 0.793939393939394 and parameters: {'batch_size': 26, 'learning_rate': 1.0033443554580133e-05, 'weight_decay': 0.00021292334661266252, 'beta_0': 0.874660069140514, 'beta_1': 0.9969826064663713, 'epsilon': 2.8807790366263072e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:54:58,195] Trial 36 finished with value: 0.8303030303030303 and parameters: {'batch_size': 18, 'learning_rate': 1.6220066033637363e-05, 'weight_decay': 8.719368798452339e-05, 'beta_0': 0.842815653017118, 'beta_1': 0.9907245426136498, 'epsilon': 6.650635613443073e-06, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:56:48,246] Trial 37 finished with value: 0.8181818181818182 and parameters: {'batch_size': 15, 'learning_rate': 1.4141811054556724e-05, 'weight_decay': 0.00029193028240484314, 'beta_0': 0.8367280209337772, 'beta_1': 0.9904520617630486, 'epsilon': 1.5864046041469659e-06, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:58:17,677] Trial 38 finished with value: 0.7575757575757576 and parameters: {'batch_size': 21, 'learning_rate': 1.5912513087890073e-05, 'weight_decay': 0.0005351659194374098, 'beta_0': 0.8518912076253811, 'beta_1': 0.988880868785213, 'epsilon': 5.5589084116782984e-06, 'balanced_loss': False, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:00:14,522] Trial 39 finished with value: 0.793939393939394 and parameters: {'batch_size': 28, 'learning_rate': 1.9579748921437175e-05, 'weight_decay': 0.0001536787772530126, 'beta_0': 0.8450755263759191, 'beta_1': 0.9847436824993729, 'epsilon': 2.148127089119545e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:01:27,775] Trial 40 finished with value: 0.7818181818181819 and parameters: {'batch_size': 17, 'learning_rate': 1.2913319261486663e-05, 'weight_decay': 8.790676608178612e-05, 'beta_0': 0.8383646761463661, 'beta_1': 0.9917685898955844, 'epsilon': 1.6440090217165886e-07, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:03:38,546] Trial 41 finished with value: 0.8121212121212121 and parameters: {'batch_size': 19, 'learning_rate': 2.648913328360983e-05, 'weight_decay': 4.280318250128985e-05, 'beta_0': 0.8660260292245319, 'beta_1': 0.993627370412882, 'epsilon': 1.1469078639300306e-05, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:06:09,710] Trial 42 finished with value: 0.8242424242424242 and parameters: {'batch_size': 18, 'learning_rate': 1.8240458330726482e-05, 'weight_decay': 2.082596637098653e-05, 'beta_0': 0.8251406109807499, 'beta_1': 0.994658759939509, 'epsilon': 4.503392522569584e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:08:20,587] Trial 43 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 1.6772034344292497e-05, 'weight_decay': 4.3314971982487083e-05, 'beta_0': 0.8558458361208715, 'beta_1': 0.9930453196975086, 'epsilon': 1.2105834347279663e-06, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:09:50,567] Trial 44 finished with value: 0.7878787878787878 and parameters: {'batch_size': 9, 'learning_rate': 1.672006142456894e-05, 'weight_decay': 3.932888273161097e-05, 'beta_0': 0.8303297298099518, 'beta_1': 0.9887470517772764, 'epsilon': 1.2072397440759818e-06, 'balanced_loss': False, 'epochs': 5, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:12:07,437] Trial 45 finished with value: 0.8181818181818182 and parameters: {'batch_size': 13, 'learning_rate': 2.078833720718991e-05, 'weight_decay': 0.00025386793124165124, 'beta_0': 0.8561886121619315, 'beta_1': 0.9924949104262281, 'epsilon': 7.665050404370307e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:14:46,353] Trial 46 finished with value: 0.8242424242424242 and parameters: {'batch_size': 14, 'learning_rate': 1.2947857908109658e-05, 'weight_decay': 1.4071305260794647e-05, 'beta_0': 0.8432919714666874, 'beta_1': 0.9909731572110504, 'epsilon': 4.775611897705907e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:17:09,365] Trial 47 finished with value: 0.7878787878787878 and parameters: {'batch_size': 15, 'learning_rate': 1.116192395277432e-05, 'weight_decay': 0.00013091018879989716, 'beta_0': 0.8610163144154674, 'beta_1': 0.9939588306222017, 'epsilon': 5.2408785666367575e-06, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.8303030303030303.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:19:11,743] Trial 48 finished with value: 0.8484848484848485 and parameters: {'batch_size': 12, 'learning_rate': 1.6847551101487917e-05, 'weight_decay': 1.5591856176215378e-06, 'beta_0': 0.8552604840481348, 'beta_1': 0.9902764985579153, 'epsilon': 2.865037179509621e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:20:45,991] Trial 49 finished with value: 0.806060606060606 and parameters: {'batch_size': 11, 'learning_rate': 2.9149839409600227e-05, 'weight_decay': 1.3292535248706845e-06, 'beta_0': 0.8715381160514089, 'beta_1': 0.9880839575886288, 'epsilon': 2.5481548591302875e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:22:45,807] Trial 50 finished with value: 0.8242424242424242 and parameters: {'batch_size': 22, 'learning_rate': 2.519667688257847e-05, 'weight_decay': 3.2597088649928885e-06, 'beta_0': 0.8481548835470055, 'beta_1': 0.9863663009814385, 'epsilon': 9.985716409833285e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:24:47,788] Trial 51 finished with value: 0.8 and parameters: {'batch_size': 13, 'learning_rate': 1.7013771754384806e-05, 'weight_decay': 1.6480649728492464e-06, 'beta_0': 0.8565112206677908, 'beta_1': 0.9901283550989909, 'epsilon': 5.023771854481066e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:26:37,689] Trial 52 finished with value: 0.8 and parameters: {'batch_size': 11, 'learning_rate': 1.525954170807188e-05, 'weight_decay': 4.3182290340359655e-06, 'beta_0': 0.8539449490547685, 'beta_1': 0.9910298765080613, 'epsilon': 1.3985066243431105e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 2, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:28:51,144] Trial 53 finished with value: 0.8303030303030303 and parameters: {'batch_size': 16, 'learning_rate': 1.9810697601169144e-05, 'weight_decay': 2.785911222061171e-05, 'beta_0': 0.863553950956237, 'beta_1': 0.9917322391675262, 'epsilon': 3.2589591174534846e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:29:45,369] Trial 54 finished with value: 0.806060606060606 and parameters: {'batch_size': 14, 'learning_rate': 2.3066850341815712e-05, 'weight_decay': 6.995626818335715e-05, 'beta_0': 0.8404063924038153, 'beta_1': 0.9931248790747048, 'epsilon': 2.851892858158003e-06, 'balanced_loss': False, 'epochs': 3, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:32:24,266] Trial 55 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.442341647534507e-05, 'weight_decay': 0.00010944567469968862, 'beta_0': 0.8713117309951355, 'beta_1': 0.9945272547722773, 'epsilon': 8.772528352989846e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:34:09,757] Trial 56 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.3875998089388805e-05, 'weight_decay': 0.00017421101668393832, 'beta_0': 0.8697270622049476, 'beta_1': 0.9981026575506563, 'epsilon': 6.526779273228942e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:36:04,976] Trial 57 finished with value: 0.793939393939394 and parameters: {'batch_size': 8, 'learning_rate': 1.3653443030364112e-05, 'weight_decay': 0.00041693034976307525, 'beta_0': 0.8703963574670941, 'beta_1': 0.998169070731761, 'epsilon': 4.2630353881877563e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:38:05,390] Trial 58 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 1.1605839769055685e-05, 'weight_decay': 0.00018273746758286604, 'beta_0': 0.8777746945891161, 'beta_1': 0.9953455388942536, 'epsilon': 1.78551676269161e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:39:52,953] Trial 59 finished with value: 0.8121212121212121 and parameters: {'batch_size': 10, 'learning_rate': 3.5569660916569524e-05, 'weight_decay': 0.00029697879165048, 'beta_0': 0.8753668485348886, 'beta_1': 0.9979248444349883, 'epsilon': 7.153566356096514e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:41:53,162] Trial 60 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 1.5172692205160736e-05, 'weight_decay': 0.00011433735051426743, 'beta_0': 0.8865830203243203, 'beta_1': 0.994334735890216, 'epsilon': 1.550569337937302e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:43:59,392] Trial 61 finished with value: 0.7878787878787878 and parameters: {'batch_size': 10, 'learning_rate': 1.861727872150287e-05, 'weight_decay': 9.30255236993518e-05, 'beta_0': 0.8628386921869189, 'beta_1': 0.9973409162009081, 'epsilon': 3.838553129767359e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:45:59,014] Trial 62 finished with value: 0.793939393939394 and parameters: {'batch_size': 15, 'learning_rate': 2.209724615274563e-05, 'weight_decay': 0.00016383938345961697, 'beta_0': 0.8694778830796323, 'beta_1': 0.9950170537642845, 'epsilon': 9.264468914278088e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:48:41,298] Trial 63 finished with value: 0.7818181818181819 and parameters: {'batch_size': 27, 'learning_rate': 1.0620075783304924e-05, 'weight_decay': 0.0007420418408693287, 'beta_0': 0.8810042559503258, 'beta_1': 0.996723576577887, 'epsilon': 1.820692073960338e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:52:17,389] Trial 64 finished with value: 0.8181818181818182 and parameters: {'batch_size': 13, 'learning_rate': 1.2356958818178454e-05, 'weight_decay': 7.179213019344635e-05, 'beta_0': 0.8659684543480524, 'beta_1': 0.9958173420967399, 'epsilon': 6.640588740421342e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:54:44,948] Trial 65 finished with value: 0.8303030303030303 and parameters: {'batch_size': 17, 'learning_rate': 1.399589590938804e-05, 'weight_decay': 0.00012986815640508165, 'beta_0': 0.8904782881680363, 'beta_1': 0.9891231866994589, 'epsilon': 3.154113547344451e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:56:45,688] Trial 66 finished with value: 0.806060606060606 and parameters: {'batch_size': 25, 'learning_rate': 2.0153831597042938e-05, 'weight_decay': 5.474262578255495e-05, 'beta_0': 0.8756207081940236, 'beta_1': 0.9810428328198362, 'epsilon': 2.5596041840579235e-06, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:59:44,113] Trial 67 finished with value: 0.8363636363636363 and parameters: {'batch_size': 29, 'learning_rate': 2.691506701293036e-05, 'weight_decay': 0.00039829851366654257, 'beta_0': 0.8590650157437963, 'beta_1': 0.9921785234886106, 'epsilon': 2.830421253257902e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:02:07,848] Trial 68 finished with value: 0.8303030303030303 and parameters: {'batch_size': 30, 'learning_rate': 3.1394971670134684e-05, 'weight_decay': 0.0005786146111966606, 'beta_0': 0.8731977265851049, 'beta_1': 0.994312294603793, 'epsilon': 2.121056799339261e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:04:55,180] Trial 69 finished with value: 0.8181818181818182 and parameters: {'batch_size': 29, 'learning_rate': 2.589449276633256e-05, 'weight_decay': 0.0009995294133914364, 'beta_0': 0.8674096712402045, 'beta_1': 0.9985384352530713, 'epsilon': 1.23094379696604e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:07:24,451] Trial 70 finished with value: 0.7636363636363637 and parameters: {'batch_size': 32, 'learning_rate': 2.405570410523185e-05, 'weight_decay': 0.0004529657770822858, 'beta_0': 0.8800878306449543, 'beta_1': 0.9923811120680134, 'epsilon': 3.316338552898982e-08, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 10}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:10:05,527] Trial 71 finished with value: 0.8121212121212121 and parameters: {'batch_size': 29, 'learning_rate': 2.915528557552418e-05, 'weight_decay': 0.00034768013301533513, 'beta_0': 0.8600915046791066, 'beta_1': 0.9903517738190161, 'epsilon': 1.0384688816250503e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:12:57,703] Trial 72 finished with value: 0.8303030303030303 and parameters: {'batch_size': 26, 'learning_rate': 1.7630858200259252e-05, 'weight_decay': 0.00025231277923728617, 'beta_0': 0.8580446434826243, 'beta_1': 0.9917028531992687, 'epsilon': 8.682786889638883e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:15:38,235] Trial 73 finished with value: 0.793939393939394 and parameters: {'batch_size': 30, 'learning_rate': 1.595406228028288e-05, 'weight_decay': 0.0001985670693277943, 'beta_0': 0.8505230995028883, 'beta_1': 0.9914129317451936, 'epsilon': 2.5194488472133564e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:18:03,761] Trial 74 finished with value: 0.8121212121212121 and parameters: {'batch_size': 9, 'learning_rate': 2.8280209095901767e-05, 'weight_decay': 0.0002658380006800727, 'beta_0': 0.8449317889737705, 'beta_1': 0.9924935679214437, 'epsilon': 4.178301394027676e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:20:59,833] Trial 75 finished with value: 0.8303030303030303 and parameters: {'batch_size': 20, 'learning_rate': 2.2045858424737824e-05, 'weight_decay': 0.00011444208809114493, 'beta_0': 0.8638269970017155, 'beta_1': 0.9897371017672499, 'epsilon': 4.1552404395076466e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:23:11,583] Trial 76 finished with value: 0.7757575757575758 and parameters: {'batch_size': 27, 'learning_rate': 3.8295438403634466e-05, 'weight_decay': 0.00034192104648629726, 'beta_0': 0.8472352586782337, 'beta_1': 0.9935908300078559, 'epsilon': 6.572472116179283e-07, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:25:33,538] Trial 77 finished with value: 0.7818181818181819 and parameters: {'batch_size': 12, 'learning_rate': 2.7003042968828405e-05, 'weight_decay': 0.000638740558966806, 'beta_0': 0.8695422954255656, 'beta_1': 0.9963865175765403, 'epsilon': 1.0169840015495366e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 2, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:28:18,635] Trial 78 finished with value: 0.8181818181818182 and parameters: {'batch_size': 23, 'learning_rate': 1.3430976104527886e-05, 'weight_decay': 0.00015357004526849168, 'beta_0': 0.8098068934849582, 'beta_1': 0.9948785332341398, 'epsilon': 1.9445028477676984e-05, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:30:52,446] Trial 79 finished with value: 0.8181818181818182 and parameters: {'batch_size': 13, 'learning_rate': 1.5120624630852715e-05, 'weight_decay': 0.0004032044993784464, 'beta_0': 0.8338313756003018, 'beta_1': 0.9905495150678781, 'epsilon': 1.595503522435672e-06, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:33:46,973] Trial 80 finished with value: 0.8363636363636363 and parameters: {'batch_size': 31, 'learning_rate': 3.187272935896239e-05, 'weight_decay': 0.00022961968369944577, 'beta_0': 0.8537764462092466, 'beta_1': 0.9931776482853167, 'epsilon': 7.026330405127141e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:36:45,143] Trial 81 finished with value: 0.8 and parameters: {'batch_size': 31, 'learning_rate': 3.308430999330189e-05, 'weight_decay': 0.0002221347851850158, 'beta_0': 0.8600002123010883, 'beta_1': 0.9932767441918673, 'epsilon': 8.181307193450433e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:39:27,208] Trial 82 finished with value: 0.8181818181818182 and parameters: {'batch_size': 28, 'learning_rate': 2.330382687367114e-05, 'weight_decay': 0.00017493661615125374, 'beta_0': 0.8526894987769084, 'beta_1': 0.9939212480125079, 'epsilon': 2.7749341304067443e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:41:50,279] Trial 83 finished with value: 0.8 and parameters: {'batch_size': 30, 'learning_rate': 3.0804551764113395e-05, 'weight_decay': 8.140583554332983e-05, 'beta_0': 0.8541036603977256, 'beta_1': 0.9956592049792614, 'epsilon': 3.1667384533167164e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:44:16,461] Trial 84 finished with value: 0.8121212121212121 and parameters: {'batch_size': 31, 'learning_rate': 3.59568533012417e-05, 'weight_decay': 0.00012921369720362747, 'beta_0': 0.8630046011546754, 'beta_1': 0.9921293002170354, 'epsilon': 6.363932753240448e-08, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:47:00,079] Trial 85 finished with value: 0.7636363636363637 and parameters: {'batch_size': 32, 'learning_rate': 4.238898756807621e-05, 'weight_decay': 0.00010837019229290787, 'beta_0': 0.8658170531010829, 'beta_1': 0.9972811568821913, 'epsilon': 4.742427418393991e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:49:11,165] Trial 86 finished with value: 0.806060606060606 and parameters: {'batch_size': 28, 'learning_rate': 2.7457692863322927e-05, 'weight_decay': 5.3035007040861956e-05, 'beta_0': 0.8577036164630621, 'beta_1': 0.9929870140382686, 'epsilon': 2.1816820429656253e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:51:16,536] Trial 87 finished with value: 0.8303030303030303 and parameters: {'batch_size': 11, 'learning_rate': 1.8864428585626686e-05, 'weight_decay': 0.00023916602933825052, 'beta_0': 0.8492544132657048, 'beta_1': 0.9913755101908918, 'epsilon': 8.821833614274209e-07, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:54:30,231] Trial 88 finished with value: 0.7818181818181819 and parameters: {'batch_size': 29, 'learning_rate': 2.108778672998381e-05, 'weight_decay': 0.0004871203184128917, 'beta_0': 0.8772666019696225, 'beta_1': 0.9838849737330857, 'epsilon': 6.761536567825903e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:55:42,181] Trial 89 finished with value: 0.7757575757575758 and parameters: {'batch_size': 25, 'learning_rate': 1.7373122966488887e-05, 'weight_decay': 8.769957596994547e-06, 'beta_0': 0.8846661897012121, 'beta_1': 0.99071035581579, 'epsilon': 5.487738049677202e-07, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:58:12,344] Trial 90 finished with value: 0.8363636363636363 and parameters: {'batch_size': 18, 'learning_rate': 1.4541245100736772e-05, 'weight_decay': 4.632160481981447e-06, 'beta_0': 0.8727445706830765, 'beta_1': 0.9880457291039334, 'epsilon': 1.3866319025727352e-06, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:00:42,844] Trial 91 finished with value: 0.8181818181818182 and parameters: {'batch_size': 18, 'learning_rate': 1.5701716851128968e-05, 'weight_decay': 1.980935952792357e-06, 'beta_0': 0.8724752143999464, 'beta_1': 0.9892164059993632, 'epsilon': 1.4057122562421935e-06, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:02:42,619] Trial 92 finished with value: 0.7878787878787878 and parameters: {'batch_size': 19, 'learning_rate': 1.4611702646161403e-05, 'weight_decay': 3.652241370283817e-06, 'beta_0': 0.8825705491895417, 'beta_1': 0.9885103207438817, 'epsilon': 1.179080507707816e-06, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:04:24,538] Trial 93 finished with value: 0.8 and parameters: {'batch_size': 17, 'learning_rate': 1.2851262603915394e-05, 'weight_decay': 4.927078012928198e-06, 'beta_0': 0.8683037178322467, 'beta_1': 0.9875242525008301, 'epsilon': 2.716026460006811e-06, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:05:54,938] Trial 94 finished with value: 0.7575757575757576 and parameters: {'batch_size': 20, 'learning_rate': 1.1882760451390366e-05, 'weight_decay': 1.0549344065736246e-06, 'beta_0': 0.8413232930093819, 'beta_1': 0.986433421337728, 'epsilon': 4.4689618594477445e-07, 'balanced_loss': True, 'epochs': 5, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:07:35,466] Trial 95 finished with value: 0.8181818181818182 and parameters: {'batch_size': 16, 'learning_rate': 3.329646765135887e-05, 'weight_decay': 2.732562981675359e-06, 'beta_0': 0.87163398889458, 'beta_1': 0.9898814825020792, 'epsilon': 1.7960820505458737e-06, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:10:34,976] Trial 96 finished with value: 0.8 and parameters: {'batch_size': 31, 'learning_rate': 1.441230821071667e-05, 'weight_decay': 0.0008085937122069767, 'beta_0': 0.8654522266388903, 'beta_1': 0.9941585995885737, 'epsilon': 2.216249624783589e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:13:07,506] Trial 97 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 1.65206934780818e-05, 'weight_decay': 2.1194924321474577e-06, 'beta_0': 0.8611947376912961, 'beta_1': 0.9989683241090911, 'epsilon': 1.0564016791686243e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:15:39,591] Trial 98 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 2.4700646709782528e-05, 'weight_decay': 6.185689637081039e-06, 'beta_0': 0.8619626415974028, 'beta_1': 0.9982683025300708, 'epsilon': 4.724760479261862e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:18:11,608] Trial 99 finished with value: 0.8181818181818182 and parameters: {'batch_size': 10, 'learning_rate': 2.5670296198958467e-05, 'weight_decay': 2.375819850523655e-06, 'beta_0': 0.8595698774818675, 'beta_1': 0.9982842755547973, 'epsilon': 5.8896953492800847e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:21:02,158] Trial 100 finished with value: 0.8303030303030303 and parameters: {'batch_size': 13, 'learning_rate': 1.646267253385831e-05, 'weight_decay': 5.8650979428663135e-06, 'beta_0': 0.8557376328667583, 'beta_1': 0.9977516608201271, 'epsilon': 4.913919480047737e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:23:38,359] Trial 101 finished with value: 0.793939393939394 and parameters: {'batch_size': 11, 'learning_rate': 2.0338694764938564e-05, 'weight_decay': 1.2672611381464455e-05, 'beta_0': 0.8621334107862362, 'beta_1': 0.9987887622431024, 'epsilon': 9.320690774431015e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:26:21,439] Trial 102 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 2.38888450044075e-05, 'weight_decay': 2.021207175171603e-06, 'beta_0': 0.8648028595003311, 'beta_1': 0.998934689865269, 'epsilon': 2.7622339392969355e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:28:57,306] Trial 103 finished with value: 0.8242424242424242 and parameters: {'batch_size': 9, 'learning_rate': 2.4648865094984606e-05, 'weight_decay': 1.6488828871015873e-06, 'beta_0': 0.873815562362718, 'beta_1': 0.9981345589701588, 'epsilon': 3.957995257481725e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 48 with value: 0.8484848484848485.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:31:50,632] Trial 104 finished with value: 0.8545454545454545 and parameters: {'batch_size': 12, 'learning_rate': 2.2353444227969362e-05, 'weight_decay': 1.2852859718187088e-06, 'beta_0': 0.8610646111352334, 'beta_1': 0.9970362974181114, 'epsilon': 1.1454976889461078e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:34:37,186] Trial 105 finished with value: 0.8181818181818182 and parameters: {'batch_size': 13, 'learning_rate': 3.0045083026520122e-05, 'weight_decay': 1.4362393654208613e-06, 'beta_0': 0.8677676780444135, 'beta_1': 0.9972976799371837, 'epsilon': 9.124047337372729e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:37:20,917] Trial 106 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.8912450674379774e-05, 'weight_decay': 1.0080829867206487e-06, 'beta_0': 0.8615438737349178, 'beta_1': 0.9968276506724978, 'epsilon': 5.228654137309122e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:40:11,479] Trial 107 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 2.1754126373948726e-05, 'weight_decay': 6.557921242627201e-06, 'beta_0': 0.8576547486742044, 'beta_1': 0.9976749772521251, 'epsilon': 1.0412788277696614e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:42:39,600] Trial 108 finished with value: 0.8303030303030303 and parameters: {'batch_size': 15, 'learning_rate': 2.8753452469132925e-05, 'weight_decay': 1.5745409502953663e-06, 'beta_0': 0.8534164890356303, 'beta_1': 0.9970321218768646, 'epsilon': 1.2899045758936003e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:45:15,787] Trial 109 finished with value: 0.8121212121212121 and parameters: {'batch_size': 11, 'learning_rate': 1.827278516836181e-05, 'weight_decay': 1.9999333295291807e-06, 'beta_0': 0.8515299963636195, 'beta_1': 0.996123712486937, 'epsilon': 1.3869323205318676e-07, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:46:57,794] Trial 110 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.3400075346746678e-05, 'weight_decay': 1.1983222924873656e-06, 'beta_0': 0.8696979161570622, 'beta_1': 0.9964658720311509, 'epsilon': 3.2258120048550296e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:48:42,790] Trial 111 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.2215796148637303e-05, 'weight_decay': 2.9133727339715002e-06, 'beta_0': 0.8696438481442256, 'beta_1': 0.9985282452560833, 'epsilon': 7.27015750537636e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:50:28,209] Trial 112 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.3415051200462989e-05, 'weight_decay': 1.3165968405627725e-06, 'beta_0': 0.8549788978890224, 'beta_1': 0.9979629025505541, 'epsilon': 2.036341778786553e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:52:28,206] Trial 113 finished with value: 0.8181818181818182 and parameters: {'batch_size': 14, 'learning_rate': 1.356125101885782e-05, 'weight_decay': 1.2361504742979187e-06, 'beta_0': 0.8553014003358232, 'beta_1': 0.9966103155404593, 'epsilon': 1.6184581555373184e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:54:45,362] Trial 114 finished with value: 0.7696969696969697 and parameters: {'batch_size': 13, 'learning_rate': 1.0796108419861733e-05, 'weight_decay': 1.1598992211918923e-06, 'beta_0': 0.8586148090718596, 'beta_1': 0.9979013395388158, 'epsilon': 2.149204979970922e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:56:30,693] Trial 115 finished with value: 0.8242424242424242 and parameters: {'batch_size': 14, 'learning_rate': 1.3188406600989472e-05, 'weight_decay': 1.4516828798196654e-06, 'beta_0': 0.8606819714314553, 'beta_1': 0.998331415117969, 'epsilon': 3.0695013593986307e-08, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:58:32,594] Trial 116 finished with value: 0.8363636363636363 and parameters: {'batch_size': 15, 'learning_rate': 1.4866874053059323e-05, 'weight_decay': 2.2315159568391865e-06, 'beta_0': 0.8464021738224089, 'beta_1': 0.9955800788566409, 'epsilon': 2.009285966270581e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:00:46,359] Trial 117 finished with value: 0.8303030303030303 and parameters: {'batch_size': 16, 'learning_rate': 1.460400991388713e-05, 'weight_decay': 2.237554530378724e-06, 'beta_0': 0.8462866160739171, 'beta_1': 0.9955899548755235, 'epsilon': 1.2499784479733978e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:02:51,790] Trial 118 finished with value: 0.7878787878787878 and parameters: {'batch_size': 11, 'learning_rate': 1.4107372700525868e-05, 'weight_decay': 1.883286964673185e-06, 'beta_0': 0.8500036357769342, 'beta_1': 0.9962772697520993, 'epsilon': 1.7930824615358994e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:04:40,539] Trial 119 finished with value: 0.8121212121212121 and parameters: {'batch_size': 10, 'learning_rate': 1.1416386699740277e-05, 'weight_decay': 2.786469961959905e-06, 'beta_0': 0.8546947127585357, 'beta_1': 0.9974750763157254, 'epsilon': 2.3096686789398358e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:07:17,117] Trial 120 finished with value: 0.8121212121212121 and parameters: {'batch_size': 15, 'learning_rate': 1.247102012924269e-05, 'weight_decay': 1.7189816751687426e-06, 'beta_0': 0.8667420317927266, 'beta_1': 0.9951792223696031, 'epsilon': 3.5603597172318576e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:08:54,104] Trial 121 finished with value: 0.5333333333333333 and parameters: {'batch_size': 12, 'learning_rate': 9.755738450337172e-05, 'weight_decay': 1.363657883051165e-06, 'beta_0': 0.8633090009946869, 'beta_1': 0.9970009268878386, 'epsilon': 4.751692994308232e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:10:56,198] Trial 122 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.5337270469343932e-05, 'weight_decay': 1.1895638624649728e-06, 'beta_0': 0.8571122564002572, 'beta_1': 0.994568498285683, 'epsilon': 1.3289122613970942e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:12:58,264] Trial 123 finished with value: 0.8242424242424242 and parameters: {'batch_size': 13, 'learning_rate': 1.5489158918719818e-05, 'weight_decay': 1.1592295249481522e-06, 'beta_0': 0.8570859736439567, 'beta_1': 0.9945705237383563, 'epsilon': 1.4091315576796879e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:14:42,270] Trial 124 finished with value: 0.8121212121212121 and parameters: {'batch_size': 12, 'learning_rate': 1.6881379574573168e-05, 'weight_decay': 2.277808422443532e-06, 'beta_0': 0.852572219523747, 'beta_1': 0.9959362836308963, 'epsilon': 1.0267500426192344e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:16:47,206] Trial 125 finished with value: 0.8303030303030303 and parameters: {'batch_size': 15, 'learning_rate': 1.4717571406143405e-05, 'weight_decay': 1.375996647325714e-06, 'beta_0': 0.8589936786952321, 'beta_1': 0.99807465741152, 'epsilon': 1.8507540046615927e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:18:29,506] Trial 126 finished with value: 0.8121212121212121 and parameters: {'batch_size': 14, 'learning_rate': 1.3396074949806275e-05, 'weight_decay': 1.772858649373667e-06, 'beta_0': 0.8482920417646295, 'beta_1': 0.9947019089296779, 'epsilon': 2.4617879636517116e-08, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:20:22,790] Trial 127 finished with value: 0.793939393939394 and parameters: {'batch_size': 11, 'learning_rate': 1.601481039775987e-05, 'weight_decay': 1.1387090743224342e-06, 'beta_0': 0.8612247560930149, 'beta_1': 0.9989881871504547, 'epsilon': 3.7434236377946616e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 10}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:22:04,800] Trial 128 finished with value: 0.8181818181818182 and parameters: {'batch_size': 14, 'learning_rate': 1.4070563640803519e-05, 'weight_decay': 4.859298594569594e-06, 'beta_0': 0.8445602222938849, 'beta_1': 0.9954253914123573, 'epsilon': 1.2611148633843341e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:24:06,922] Trial 129 finished with value: 0.8242424242424242 and parameters: {'batch_size': 16, 'learning_rate': 1.481215317963955e-05, 'weight_decay': 2.5545098337324623e-06, 'beta_0': 0.8714054934917019, 'beta_1': 0.9965060465328723, 'epsilon': 2.721776250937377e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:26:05,199] Trial 130 finished with value: 0.7515151515151515 and parameters: {'batch_size': 13, 'learning_rate': 1.253987144691906e-05, 'weight_decay': 1.54549492788456e-06, 'beta_0': 0.8508787884459085, 'beta_1': 0.986911591324535, 'epsilon': 7.570592958186466e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:27:47,266] Trial 131 finished with value: 0.8 and parameters: {'batch_size': 15, 'learning_rate': 1.762196631451498e-05, 'weight_decay': 3.4098660602208307e-06, 'beta_0': 0.8562990048124139, 'beta_1': 0.9970987427529142, 'epsilon': 1.9902043256478852e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:29:44,906] Trial 132 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.6352772550547415e-05, 'weight_decay': 1.2168072278153372e-06, 'beta_0': 0.8648603216413643, 'beta_1': 0.997889393314405, 'epsilon': 3.6343443125009526e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:31:57,790] Trial 133 finished with value: 0.8121212121212121 and parameters: {'batch_size': 13, 'learning_rate': 1.6338012881464086e-05, 'weight_decay': 1.2587326781217441e-06, 'beta_0': 0.8644336603168564, 'beta_1': 0.998586375652484, 'epsilon': 3.4813852066417476e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:34:12,967] Trial 134 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.5202791758956952e-05, 'weight_decay': 1.0492936924254626e-06, 'beta_0': 0.8688733239630637, 'beta_1': 0.997658750118033, 'epsilon': 6.589875755083575e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:36:10,606] Trial 135 finished with value: 0.806060606060606 and parameters: {'batch_size': 13, 'learning_rate': 1.3618311818331798e-05, 'weight_decay': 1.2606349961688314e-06, 'beta_0': 0.8763935075834185, 'beta_1': 0.9880434929987397, 'epsilon': 6.415860734691213e-08, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:37:54,063] Trial 136 finished with value: 0.8242424242424242 and parameters: {'batch_size': 14, 'learning_rate': 1.5867633977539536e-05, 'weight_decay': 2.1046336027334334e-06, 'beta_0': 0.8621508765918053, 'beta_1': 0.9937532214265139, 'epsilon': 1.1314608569246578e-07, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:39:39,680] Trial 137 finished with value: 0.806060606060606 and parameters: {'batch_size': 12, 'learning_rate': 1.697555237399536e-05, 'weight_decay': 3.1199726420796332e-06, 'beta_0': 0.8668210793323738, 'beta_1': 0.9980058201883152, 'epsilon': 4.399812734923446e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:41:51,115] Trial 138 finished with value: 0.8 and parameters: {'batch_size': 15, 'learning_rate': 1.2924304677033936e-05, 'weight_decay': 4.0608578685292884e-06, 'beta_0': 0.8740523891574148, 'beta_1': 0.9949683791027815, 'epsilon': 3.009714562519462e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:43:36,635] Trial 139 finished with value: 0.8181818181818182 and parameters: {'batch_size': 10, 'learning_rate': 1.5012779940659506e-05, 'weight_decay': 2.286346257996214e-05, 'beta_0': 0.8652122092063732, 'beta_1': 0.9983962277552173, 'epsilon': 8.41215390063746e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:45:17,205] Trial 140 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 1.4276972123495751e-05, 'weight_decay': 1.8060831501804718e-06, 'beta_0': 0.859268220040833, 'beta_1': 0.9974593153175181, 'epsilon': 1.7492240682723196e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:47:58,799] Trial 141 finished with value: 0.8121212121212121 and parameters: {'batch_size': 17, 'learning_rate': 1.7544050737063422e-05, 'weight_decay': 0.0002982721315830309, 'beta_0': 0.8709940423538874, 'beta_1': 0.9962870624949466, 'epsilon': 1.0403229927763512e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:50:12,766] Trial 142 finished with value: 0.8181818181818182 and parameters: {'batch_size': 21, 'learning_rate': 1.9189144457652063e-05, 'weight_decay': 1.5272075554243927e-06, 'beta_0': 0.8621214272641353, 'beta_1': 0.9969042288596389, 'epsilon': 5.263469872552454e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:53:14,678] Trial 143 finished with value: 0.8181818181818182 and parameters: {'batch_size': 15, 'learning_rate': 1.3663344235376823e-05, 'weight_decay': 1.0126342642660758e-06, 'beta_0': 0.8570241887339245, 'beta_1': 0.9958982324246577, 'epsilon': 3.721964760552132e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:55:41,583] Trial 144 finished with value: 0.8303030303030303 and parameters: {'batch_size': 16, 'learning_rate': 2.2596980833793323e-05, 'weight_decay': 1.1005172056243805e-05, 'beta_0': 0.8635000218011202, 'beta_1': 0.9975573474473565, 'epsilon': 5.544579989740925e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:57:57,009] Trial 145 finished with value: 0.8303030303030303 and parameters: {'batch_size': 13, 'learning_rate': 1.565741053522274e-05, 'weight_decay': 1.441831222120166e-06, 'beta_0': 0.860171790901784, 'beta_1': 0.9965353067224525, 'epsilon': 6.548812780926421e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:59:42,369] Trial 146 finished with value: 0.806060606060606 and parameters: {'batch_size': 11, 'learning_rate': 2.5779080635932368e-05, 'weight_decay': 0.00014060124005312556, 'beta_0': 0.854438461269326, 'beta_1': 0.9979084579663507, 'epsilon': 2.8124110427771085e-07, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:02:14,179] Trial 147 finished with value: 0.8242424242424242 and parameters: {'batch_size': 12, 'learning_rate': 2.069431183142922e-05, 'weight_decay': 0.00018592866863297643, 'beta_0': 0.8681019015883327, 'beta_1': 0.9989948658121661, 'epsilon': 2.139129998304274e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:04:33,777] Trial 148 finished with value: 0.8181818181818182 and parameters: {'batch_size': 14, 'learning_rate': 2.35546291285841e-05, 'weight_decay': 1.2953582778672411e-06, 'beta_0': 0.8531426579262135, 'beta_1': 0.9953393274415439, 'epsilon': 1.51975750047931e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:07:18,683] Trial 149 finished with value: 0.8303030303030303 and parameters: {'batch_size': 13, 'learning_rate': 1.8180854750870992e-05, 'weight_decay': 1.7871619286181509e-06, 'beta_0': 0.878139325765584, 'beta_1': 0.9942368234109428, 'epsilon': 1.3239095490095016e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:09:01,432] Trial 150 finished with value: 0.8 and parameters: {'batch_size': 11, 'learning_rate': 1.659956970261257e-05, 'weight_decay': 2.3907115446292805e-06, 'beta_0': 0.8579820472762819, 'beta_1': 0.9935096288355096, 'epsilon': 3.2883443026289576e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:11:25,724] Trial 151 finished with value: 0.8242424242424242 and parameters: {'batch_size': 14, 'learning_rate': 2.9193187157054235e-05, 'weight_decay': 1.6991948179197078e-05, 'beta_0': 0.8738026714628195, 'beta_1': 0.993269032336453, 'epsilon': 1.292657503088891e-06, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:13:50,083] Trial 152 finished with value: 0.8181818181818182 and parameters: {'batch_size': 22, 'learning_rate': 3.1048038965451916e-05, 'weight_decay': 0.0003989008566214522, 'beta_0': 0.8698858639432386, 'beta_1': 0.9925405952405705, 'epsilon': 8.43101117429463e-07, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:16:37,871] Trial 153 finished with value: 0.8121212121212121 and parameters: {'batch_size': 17, 'learning_rate': 2.4606315946037485e-05, 'weight_decay': 0.0005454891713439496, 'beta_0': 0.8657664269051012, 'beta_1': 0.9928247572559343, 'epsilon': 2.464898023454061e-08, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:19:02,082] Trial 154 finished with value: 0.793939393939394 and parameters: {'batch_size': 23, 'learning_rate': 3.494454923082495e-05, 'weight_decay': 0.00016218198311379514, 'beta_0': 0.8727208756320552, 'beta_1': 0.994450137501635, 'epsilon': 3.4697734161600663e-06, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 104 with value: 0.8545454545454545.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:21:34,961] Trial 155 finished with value: 0.8606060606060606 and parameters: {'batch_size': 15, 'learning_rate': 3.268334555017631e-05, 'weight_decay': 3.588554188261765e-05, 'beta_0': 0.8791527511514217, 'beta_1': 0.9941035174168883, 'epsilon': 4.05005857745538e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:24:10,694] Trial 156 finished with value: 0.8303030303030303 and parameters: {'batch_size': 15, 'learning_rate': 3.252726168226205e-05, 'weight_decay': 1.1518767612758162e-06, 'beta_0': 0.87855147325527, 'beta_1': 0.9940092164366413, 'epsilon': 4.783989086699898e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:26:49,136] Trial 157 finished with value: 0.8484848484848485 and parameters: {'batch_size': 14, 'learning_rate': 2.723844055037719e-05, 'weight_decay': 9.145670292080271e-06, 'beta_0': 0.8601870535459999, 'beta_1': 0.9984090999535713, 'epsilon': 3.8475897109945335e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:29:01,403] Trial 158 finished with value: 0.8121212121212121 and parameters: {'batch_size': 16, 'learning_rate': 2.6596270852821438e-05, 'weight_decay': 1.0563374791276932e-05, 'beta_0': 0.8607391792101994, 'beta_1': 0.9985545660051751, 'epsilon': 4.305577879898289e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:31:46,940] Trial 159 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.1775336701934391e-05, 'weight_decay': 6.146663716807725e-06, 'beta_0': 0.8501990564769972, 'beta_1': 0.9982982572441164, 'epsilon': 5.942096806886074e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:34:20,433] Trial 160 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 1.1882684266023382e-05, 'weight_decay': 7.954211594019142e-06, 'beta_0': 0.8501875692158003, 'beta_1': 0.9981625816283048, 'epsilon': 5.679258924976847e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:37:02,064] Trial 161 finished with value: 0.8484848484848485 and parameters: {'batch_size': 14, 'learning_rate': 1.4068379836865152e-05, 'weight_decay': 6.519343566565952e-06, 'beta_0': 0.8553067436672238, 'beta_1': 0.9983439996603165, 'epsilon': 2.9241412759003076e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:39:50,507] Trial 162 finished with value: 0.8 and parameters: {'batch_size': 13, 'learning_rate': 1.09446733551495e-05, 'weight_decay': 5.7939848653528e-06, 'beta_0': 0.8550556765124079, 'beta_1': 0.998528260821133, 'epsilon': 2.470261455178465e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:42:32,990] Trial 163 finished with value: 0.806060606060606 and parameters: {'batch_size': 12, 'learning_rate': 3.409523399232907e-05, 'weight_decay': 7.349481844449025e-06, 'beta_0': 0.8479251815620662, 'beta_1': 0.9972580458120806, 'epsilon': 1.826206061540123e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:45:30,022] Trial 164 finished with value: 0.8121212121212121 and parameters: {'batch_size': 13, 'learning_rate': 1.2799269794221488e-05, 'weight_decay': 5.736152542165118e-06, 'beta_0': 0.8526733812023327, 'beta_1': 0.9986678249391124, 'epsilon': 2.894944002734495e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:48:04,177] Trial 165 finished with value: 0.8 and parameters: {'batch_size': 11, 'learning_rate': 2.7810746309541903e-05, 'weight_decay': 4.416110322582255e-06, 'beta_0': 0.8512878676369608, 'beta_1': 0.9982065156057007, 'epsilon': 7.166879859536094e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:50:37,797] Trial 166 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 3.700794538550378e-05, 'weight_decay': 9.348485678760553e-06, 'beta_0': 0.8558907501106352, 'beta_1': 0.9974965634878893, 'epsilon': 5.960067648568602e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:53:04,748] Trial 167 finished with value: 0.8181818181818182 and parameters: {'batch_size': 14, 'learning_rate': 1.3885704807899355e-05, 'weight_decay': 6.801913989125249e-06, 'beta_0': 0.8462129428060591, 'beta_1': 0.9883140319270358, 'epsilon': 4.1996579657887043e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:55:55,981] Trial 168 finished with value: 0.8242424242424242 and parameters: {'batch_size': 10, 'learning_rate': 1.3227794671785951e-05, 'weight_decay': 4.875453058968649e-06, 'beta_0': 0.8571821587635883, 'beta_1': 0.9948444167752838, 'epsilon': 1.0457393885275069e-06, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:58:28,619] Trial 169 finished with value: 0.7757575757575758 and parameters: {'batch_size': 15, 'learning_rate': 1.1924955831656506e-05, 'weight_decay': 6.300568652265248e-06, 'beta_0': 0.8491328935245627, 'beta_1': 0.9989974668047755, 'epsilon': 9.54161406775434e-08, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:01:38,855] Trial 170 finished with value: 0.8121212121212121 and parameters: {'batch_size': 13, 'learning_rate': 1.4458768030367707e-05, 'weight_decay': 8.311116585617409e-06, 'beta_0': 0.88099301474948, 'beta_1': 0.998269538876967, 'epsilon': 4.211333901240403e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:04:20,634] Trial 171 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 1.53061955144352e-05, 'weight_decay': 3.152000990132394e-05, 'beta_0': 0.8583144617696851, 'beta_1': 0.9977436673865681, 'epsilon': 3.777184797224915e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:06:42,033] Trial 172 finished with value: 0.7757575757575758 and parameters: {'batch_size': 14, 'learning_rate': 3.184661160015248e-05, 'weight_decay': 1.4756935626313661e-06, 'beta_0': 0.854519591857624, 'beta_1': 0.9978952163854791, 'epsilon': 3.117559128206683e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:09:30,607] Trial 173 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 1.4290326779051762e-05, 'weight_decay': 5.306798750334044e-06, 'beta_0': 0.8621641783978475, 'beta_1': 0.998579232166157, 'epsilon': 7.011245404416089e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:11:48,677] Trial 174 finished with value: 0.7818181818181819 and parameters: {'batch_size': 13, 'learning_rate': 1.1419826430138118e-05, 'weight_decay': 3.7613622271435246e-06, 'beta_0': 0.8643520172356429, 'beta_1': 0.9856016891350716, 'epsilon': 5.326900202608605e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:14:38,514] Trial 175 finished with value: 0.8121212121212121 and parameters: {'batch_size': 11, 'learning_rate': 1.0078227152609347e-05, 'weight_decay': 7.377559253094512e-06, 'beta_0': 0.8527362386419167, 'beta_1': 0.9970727068301241, 'epsilon': 1.8571171656900305e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:16:20,777] Trial 176 finished with value: 0.793939393939394 and parameters: {'batch_size': 15, 'learning_rate': 1.592053200147644e-05, 'weight_decay': 1.3000431738410002e-06, 'beta_0': 0.8590317927627047, 'beta_1': 0.9981681890198167, 'epsilon': 2.4849637949998817e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:18:47,903] Trial 177 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 1.483550343220832e-05, 'weight_decay': 1.6163517406623035e-06, 'beta_0': 0.8755333697739833, 'beta_1': 0.9978231622104965, 'epsilon': 8.040279685867998e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:20:33,365] Trial 178 finished with value: 0.8121212121212121 and parameters: {'batch_size': 13, 'learning_rate': 2.7465971923596413e-05, 'weight_decay': 1.011382077683282e-06, 'beta_0': 0.8602719787604588, 'beta_1': 0.9894775165371466, 'epsilon': 1.515351206627789e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:23:06,617] Trial 179 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.259935859332042e-05, 'weight_decay': 3.487905485277087e-05, 'beta_0': 0.8699836464572313, 'beta_1': 0.9937885567776585, 'epsilon': 2.12987872218172e-07, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 10}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:25:49,178] Trial 180 finished with value: 0.8242424242424242 and parameters: {'batch_size': 32, 'learning_rate': 3.0288252276797605e-05, 'weight_decay': 1.6866446889244127e-05, 'beta_0': 0.8552280356466235, 'beta_1': 0.9967137996631699, 'epsilon': 3.6128564279581487e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:28:14,063] Trial 181 finished with value: 0.806060606060606 and parameters: {'batch_size': 12, 'learning_rate': 1.413530856587584e-05, 'weight_decay': 4.4218926274947185e-06, 'beta_0': 0.8626275391834355, 'beta_1': 0.9984505738734812, 'epsilon': 7.979291651219978e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:30:59,249] Trial 182 finished with value: 0.8484848484848485 and parameters: {'batch_size': 12, 'learning_rate': 1.349512505920292e-05, 'weight_decay': 5.547880186761045e-06, 'beta_0': 0.8672500984007078, 'beta_1': 0.9985748992435827, 'epsilon': 6.944339117029052e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:33:51,087] Trial 183 finished with value: 0.793939393939394 and parameters: {'batch_size': 13, 'learning_rate': 1.3261908840779643e-05, 'weight_decay': 6.43624117990089e-06, 'beta_0': 0.8673135864619739, 'beta_1': 0.9975446276171919, 'epsilon': 5.1747413249566746e-08, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:36:28,743] Trial 184 finished with value: 0.8121212121212121 and parameters: {'batch_size': 11, 'learning_rate': 1.5257139990827386e-05, 'weight_decay': 5.570228675170453e-06, 'beta_0': 0.8664041486834555, 'beta_1': 0.9979561714677724, 'epsilon': 3.279152937615555e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:39:16,361] Trial 185 finished with value: 0.8424242424242424 and parameters: {'batch_size': 14, 'learning_rate': 1.2278115976547429e-05, 'weight_decay': 9.145848999501801e-06, 'beta_0': 0.8643473697367475, 'beta_1': 0.9987880218231086, 'epsilon': 6.337085147499864e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:42:04,731] Trial 186 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 1.2205590344316008e-05, 'weight_decay': 7.190974700031646e-06, 'beta_0': 0.8712237355006813, 'beta_1': 0.9989827234843783, 'epsilon': 1.0331671368850126e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:44:37,941] Trial 187 finished with value: 0.8121212121212121 and parameters: {'batch_size': 15, 'learning_rate': 1.3631359264257162e-05, 'weight_decay': 1.2382815610195509e-05, 'beta_0': 0.8567805066029994, 'beta_1': 0.9986479767732445, 'epsilon': 6.208463895955889e-08, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:47:31,920] Trial 188 finished with value: 0.8242424242424242 and parameters: {'batch_size': 13, 'learning_rate': 1.312802015129711e-05, 'weight_decay': 8.993306937732096e-06, 'beta_0': 0.8605855806786773, 'beta_1': 0.9983491454231371, 'epsilon': 4.442906400726447e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:50:42,894] Trial 189 finished with value: 0.8121212121212121 and parameters: {'batch_size': 11, 'learning_rate': 1.218489186989054e-05, 'weight_decay': 1.0163870290489211e-05, 'beta_0': 0.8631881617171776, 'beta_1': 0.9987278655319971, 'epsilon': 5.3880705008257784e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:53:17,115] Trial 190 finished with value: 0.8181818181818182 and parameters: {'batch_size': 16, 'learning_rate': 1.1566006273700659e-05, 'weight_decay': 1.4295127210816914e-05, 'beta_0': 0.8439036492368964, 'beta_1': 0.9920051908729377, 'epsilon': 7.087710875483575e-08, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:54:58,320] Trial 191 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.374361023912593e-05, 'weight_decay': 1.1695855846387058e-06, 'beta_0': 0.8689529248547546, 'beta_1': 0.9981072147711443, 'epsilon': 1.2893272408057976e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:57:40,992] Trial 192 finished with value: 0.8424242424242424 and parameters: {'batch_size': 14, 'learning_rate': 1.276537384393758e-05, 'weight_decay': 7.931459179305648e-06, 'beta_0': 0.8646618528431039, 'beta_1': 0.9973814263048115, 'epsilon': 2.0825759858520094e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:00:28,369] Trial 193 finished with value: 0.8242424242424242 and parameters: {'batch_size': 14, 'learning_rate': 1.2852952319988088e-05, 'weight_decay': 6.234318841741779e-06, 'beta_0': 0.8589863944749673, 'beta_1': 0.9976496238982808, 'epsilon': 2.5877459475505974e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:03:17,797] Trial 194 finished with value: 0.806060606060606 and parameters: {'batch_size': 15, 'learning_rate': 1.262676273240527e-05, 'weight_decay': 7.821761292168498e-06, 'beta_0': 0.8669108455389133, 'beta_1': 0.9971664562593652, 'epsilon': 1.2268522969527855e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:05:41,762] Trial 195 finished with value: 0.8303030303030303 and parameters: {'batch_size': 14, 'learning_rate': 1.4117550548954e-05, 'weight_decay': 5.187191748502974e-06, 'beta_0': 0.8648926268211466, 'beta_1': 0.9817903301025571, 'epsilon': 1.6021068418496176e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:08:24,325] Trial 196 finished with value: 0.8303030303030303 and parameters: {'batch_size': 19, 'learning_rate': 1.4719441289653256e-05, 'weight_decay': 8.726544970864357e-06, 'beta_0': 0.8608352633912425, 'beta_1': 0.9943394047352397, 'epsilon': 2.0215985626550968e-08, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:11:08,254] Trial 197 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.3438365461987015e-05, 'weight_decay': 1.2370122082337348e-05, 'beta_0': 0.8684774162446646, 'beta_1': 0.9989908672476422, 'epsilon': 2.07969551566865e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:13:57,776] Trial 198 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.3292098886333454e-05, 'weight_decay': 1.199531034622273e-05, 'beta_0': 0.8686376536325878, 'beta_1': 0.9987773694576689, 'epsilon': 2.2371772826656336e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:16:49,903] Trial 199 finished with value: 0.8242424242424242 and parameters: {'batch_size': 12, 'learning_rate': 1.21941133162101e-05, 'weight_decay': 1.1803186210383063e-05, 'beta_0': 0.8681872144592652, 'beta_1': 0.998705560174313, 'epsilon': 3.35076665129219e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:19:32,015] Trial 200 finished with value: 0.8363636363636363 and parameters: {'batch_size': 11, 'learning_rate': 1.1310386035665728e-05, 'weight_decay': 9.895276617944475e-06, 'beta_0': 0.8716093410214113, 'beta_1': 0.9989930783930125, 'epsilon': 2.3822666968411717e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:22:16,747] Trial 201 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 1.3203001458508524e-05, 'weight_decay': 1.4992113117838489e-05, 'beta_0': 0.869179874371784, 'beta_1': 0.9984285787465866, 'epsilon': 2.224585576402236e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:24:59,961] Trial 202 finished with value: 0.8363636363636363 and parameters: {'batch_size': 12, 'learning_rate': 1.2694883220107728e-05, 'weight_decay': 2.0661572484308e-05, 'beta_0': 0.873202730092071, 'beta_1': 0.9982570835727486, 'epsilon': 2.8060405529061823e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:27:46,696] Trial 203 finished with value: 0.8363636363636363 and parameters: {'batch_size': 13, 'learning_rate': 1.3966954559983183e-05, 'weight_decay': 7.035280915054263e-06, 'beta_0': 0.8639103687681703, 'beta_1': 0.9986214896600477, 'epsilon': 1.8451828592406042e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:30:51,331] Trial 204 finished with value: 0.8363636363636363 and parameters: {'batch_size': 13, 'learning_rate': 1.3455838990442713e-05, 'weight_decay': 9.324952722465875e-06, 'beta_0': 0.8660438532031673, 'beta_1': 0.9980737028450937, 'epsilon': 3.971598337668205e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:33:50,940] Trial 205 finished with value: 0.8606060606060606 and parameters: {'batch_size': 11, 'learning_rate': 1.183671488994847e-05, 'weight_decay': 1.2049409306735719e-05, 'beta_0': 0.8673047592301346, 'beta_1': 0.9989757268055985, 'epsilon': 8.465835630342165e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:36:48,113] Trial 206 finished with value: 0.8484848484848485 and parameters: {'batch_size': 11, 'learning_rate': 1.2101922640467202e-05, 'weight_decay': 1.3542692423356149e-05, 'beta_0': 0.8707753001992747, 'beta_1': 0.9987705389601151, 'epsilon': 1.117056791296564e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:39:38,131] Trial 207 finished with value: 0.8242424242424242 and parameters: {'batch_size': 10, 'learning_rate': 1.0802109905192809e-05, 'weight_decay': 1.1040424269626693e-05, 'beta_0': 0.8709686258386067, 'beta_1': 0.9989997089091072, 'epsilon': 8.220101432536764e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:42:36,116] Trial 208 finished with value: 0.8363636363636363 and parameters: {'batch_size': 11, 'learning_rate': 1.2198931955635169e-05, 'weight_decay': 1.2840771328783541e-05, 'beta_0': 0.8754627253668597, 'beta_1': 0.9986248668748235, 'epsilon': 1.1237033863893675e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:45:18,466] Trial 209 finished with value: 0.806060606060606 and parameters: {'batch_size': 11, 'learning_rate': 1.1531984554935452e-05, 'weight_decay': 1.4636938801006523e-05, 'beta_0': 0.8678169953726164, 'beta_1': 0.9983198221260186, 'epsilon': 1.5318042232386098e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:48:08,916] Trial 210 finished with value: 0.8181818181818182 and parameters: {'batch_size': 11, 'learning_rate': 5.938823644751425e-05, 'weight_decay': 1.284350962938586e-05, 'beta_0': 0.8699715191582199, 'beta_1': 0.9987014068277819, 'epsilon': 6.118596000751341e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:51:00,732] Trial 211 finished with value: 0.8424242424242424 and parameters: {'batch_size': 12, 'learning_rate': 1.2931120094926671e-05, 'weight_decay': 1.052883326799954e-05, 'beta_0': 0.8655850942424127, 'beta_1': 0.9983088658151545, 'epsilon': 7.308030476130347e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:53:28,836] Trial 212 finished with value: 0.7878787878787878 and parameters: {'batch_size': 12, 'learning_rate': 1.1722137783482981e-05, 'weight_decay': 1.0449627166644413e-05, 'beta_0': 0.8662745232667863, 'beta_1': 0.9989936937430439, 'epsilon': 1.0171310696396923e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:56:38,631] Trial 213 finished with value: 0.8 and parameters: {'batch_size': 10, 'learning_rate': 1.0436173893111766e-05, 'weight_decay': 8.484131368580245e-06, 'beta_0': 0.8725980991505601, 'beta_1': 0.9983109983291887, 'epsilon': 8.502903465620936e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:59:26,627] Trial 214 finished with value: 0.8303030303030303 and parameters: {'batch_size': 12, 'learning_rate': 1.2615501542481691e-05, 'weight_decay': 1.188964334179668e-05, 'beta_0': 0.8695480812710782, 'beta_1': 0.9976915033260784, 'epsilon': 8.410768162903029e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:02:01,970] Trial 215 finished with value: 0.806060606060606 and parameters: {'batch_size': 11, 'learning_rate': 1.1069850839168276e-05, 'weight_decay': 7.730338962716523e-06, 'beta_0': 0.8634811999265118, 'beta_1': 0.9985097871146412, 'epsilon': 6.918041988716946e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:04:44,125] Trial 216 finished with value: 0.7757575757575758 and parameters: {'batch_size': 12, 'learning_rate': 2.5284373987034375e-05, 'weight_decay': 1.9203595959568142e-05, 'beta_0': 0.8671884800607861, 'beta_1': 0.9888385985309988, 'epsilon': 5.3386748039127714e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:07:12,166] Trial 217 finished with value: 0.7878787878787878 and parameters: {'batch_size': 12, 'learning_rate': 1.3022314688504138e-05, 'weight_decay': 8.8579213635119e-06, 'beta_0': 0.8651012941899104, 'beta_1': 0.9980450891557102, 'epsilon': 1.1919650009051552e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:09:44,650] Trial 218 finished with value: 0.8363636363636363 and parameters: {'batch_size': 11, 'learning_rate': 1.1937930205327312e-05, 'weight_decay': 1.594783942848877e-05, 'beta_0': 0.8741815603235386, 'beta_1': 0.9986917811913237, 'epsilon': 9.216399670588443e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:12:35,587] Trial 219 finished with value: 0.8121212121212121 and parameters: {'batch_size': 13, 'learning_rate': 1.2458640919559907e-05, 'weight_decay': 1.0631465522050687e-05, 'beta_0': 0.868514458639494, 'beta_1': 0.9974166002968465, 'epsilon': 6.700951901798603e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:15:16,651] Trial 220 finished with value: 0.8121212121212121 and parameters: {'batch_size': 9, 'learning_rate': 2.661454842967052e-05, 'weight_decay': 6.6795558470979644e-06, 'beta_0': 0.8620070775908992, 'beta_1': 0.9982691639780688, 'epsilon': 4.722066428999802e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:17:59,902] Trial 221 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.3172913420532113e-05, 'weight_decay': 4.782035087479703e-05, 'beta_0': 0.87102800007698, 'beta_1': 0.9979107778501877, 'epsilon': 7.164920005259182e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:20:47,820] Trial 222 finished with value: 0.8363636363636363 and parameters: {'batch_size': 13, 'learning_rate': 1.3535042670784315e-05, 'weight_decay': 7.813241830381898e-06, 'beta_0': 0.8657799211398443, 'beta_1': 0.9987483259164901, 'epsilon': 4.561133738931704e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:23:24,354] Trial 223 finished with value: 0.8242424242424242 and parameters: {'batch_size': 12, 'learning_rate': 1.3927790537409218e-05, 'weight_decay': 1.2972722615584229e-05, 'beta_0': 0.8182706780676046, 'beta_1': 0.9978280146963353, 'epsilon': 3.61947786785493e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:26:17,605] Trial 224 finished with value: 0.8363636363636363 and parameters: {'batch_size': 11, 'learning_rate': 1.1873191292111137e-05, 'weight_decay': 0.00021183877334043425, 'beta_0': 0.8637721627030741, 'beta_1': 0.9982837718219929, 'epsilon': 6.237929983029309e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:28:51,495] Trial 225 finished with value: 0.806060606060606 and parameters: {'batch_size': 12, 'learning_rate': 2.8639104926790782e-05, 'weight_decay': 9.424547445177564e-06, 'beta_0': 0.8677621519676153, 'beta_1': 0.9989415813737524, 'epsilon': 2.964688215051793e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:31:35,474] Trial 226 finished with value: 0.8 and parameters: {'batch_size': 30, 'learning_rate': 3.3470862490342204e-05, 'weight_decay': 0.00027405656945062533, 'beta_0': 0.858743900172098, 'beta_1': 0.9973088568098941, 'epsilon': 9.964796091615152e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:34:20,624] Trial 227 finished with value: 0.806060606060606 and parameters: {'batch_size': 13, 'learning_rate': 1.2942742954190312e-05, 'weight_decay': 2.379969690560348e-05, 'beta_0': 0.8619808537178993, 'beta_1': 0.9984944155462235, 'epsilon': 1.3953326458030088e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:37:09,997] Trial 228 finished with value: 0.8484848484848485 and parameters: {'batch_size': 10, 'learning_rate': 1.2397409932965814e-05, 'weight_decay': 7.749196157676208e-05, 'beta_0': 0.8723847887615526, 'beta_1': 0.9913687664330093, 'epsilon': 8.324322129123593e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:39:44,052] Trial 229 finished with value: 0.8242424242424242 and parameters: {'batch_size': 10, 'learning_rate': 1.1423051824185841e-05, 'weight_decay': 0.00011608340520282074, 'beta_0': 0.8724170752413953, 'beta_1': 0.9912133824020923, 'epsilon': 8.887665202573685e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:42:19,179] Trial 230 finished with value: 0.8242424242424242 and parameters: {'batch_size': 11, 'learning_rate': 1.2371601439415219e-05, 'weight_decay': 7.976292140901792e-05, 'beta_0': 0.8774643075916537, 'beta_1': 0.9922694383149011, 'epsilon': 7.738933520255515e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:44:55,907] Trial 231 finished with value: 0.8484848484848485 and parameters: {'batch_size': 10, 'learning_rate': 1.3480988509417411e-05, 'weight_decay': 9.764292005400062e-05, 'beta_0': 0.8711546039960102, 'beta_1': 0.9907982342291034, 'epsilon': 1.1519791602792662e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:47:32,780] Trial 232 finished with value: 0.8181818181818182 and parameters: {'batch_size': 10, 'learning_rate': 1.2783600116853398e-05, 'weight_decay': 9.42980466183508e-05, 'beta_0': 0.8702726144181191, 'beta_1': 0.9901831659634159, 'epsilon': 1.1230845736109747e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:50:00,340] Trial 233 finished with value: 0.7575757575757576 and parameters: {'batch_size': 10, 'learning_rate': 6.554345803588799e-05, 'weight_decay': 6.408017653631867e-05, 'beta_0': 0.8742115112888388, 'beta_1': 0.9910670318057349, 'epsilon': 1.3603727712576516e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:52:37,214] Trial 234 finished with value: 0.8181818181818182 and parameters: {'batch_size': 10, 'learning_rate': 1.3971310016675563e-05, 'weight_decay': 0.0001079715731463879, 'beta_0': 0.8718325762636101, 'beta_1': 0.9928230806045895, 'epsilon': 9.464694775511134e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:55:12,149] Trial 235 finished with value: 0.8363636363636363 and parameters: {'batch_size': 11, 'learning_rate': 1.202113538173012e-05, 'weight_decay': 8.044300206055447e-05, 'beta_0': 0.868619316897828, 'beta_1': 0.9916301244059958, 'epsilon': 5.5630666764547347e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:57:42,602] Trial 236 finished with value: 0.806060606060606 and parameters: {'batch_size': 12, 'learning_rate': 3.123456542051108e-05, 'weight_decay': 0.0001289759023965196, 'beta_0': 0.8650103964750103, 'beta_1': 0.9902609760768077, 'epsilon': 1.607197562500525e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:00:38,804] Trial 237 finished with value: 0.8242424242424242 and parameters: {'batch_size': 9, 'learning_rate': 1.3472072568001415e-05, 'weight_decay': 6.1387271222197094e-06, 'beta_0': 0.8754293602144173, 'beta_1': 0.9909256242605889, 'epsilon': 7.610653693788597e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:03:34,680] Trial 238 finished with value: 0.8181818181818182 and parameters: {'batch_size': 11, 'learning_rate': 1.450502537751842e-05, 'weight_decay': 1.1910495998295935e-05, 'beta_0': 0.8702380065734121, 'beta_1': 0.990878153779404, 'epsilon': 9.991965343580513e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:06:11,715] Trial 239 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.245137410354615e-05, 'weight_decay': 9.903900300401301e-05, 'beta_0': 0.8676909710231114, 'beta_1': 0.9916056713902199, 'epsilon': 1.20626848276548e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 9}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:08:46,221] Trial 240 finished with value: 0.806060606060606 and parameters: {'batch_size': 11, 'learning_rate': 1.3118219322520815e-05, 'weight_decay': 5.494698520612271e-06, 'beta_0': 0.8870964136912514, 'beta_1': 0.9903762102937335, 'epsilon': 1.678224890365395e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:12:04,371] Trial 241 finished with value: 0.8181818181818182 and parameters: {'batch_size': 24, 'learning_rate': 1.3496146396908118e-05, 'weight_decay': 4.597310243917334e-06, 'beta_0': 0.8715285188653284, 'beta_1': 0.9981154603818667, 'epsilon': 6.629907511404844e-08, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:14:39,848] Trial 242 finished with value: 0.8242424242424242 and parameters: {'batch_size': 9, 'learning_rate': 1.4133723567783735e-05, 'weight_decay': 1.0160359510015932e-05, 'beta_0': 0.8539368364698146, 'beta_1': 0.9986418808200747, 'epsilon': 2.308693118473612e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:17:13,569] Trial 243 finished with value: 0.7757575757575758 and parameters: {'batch_size': 13, 'learning_rate': 1.2754716896448101e-05, 'weight_decay': 8.861949811582643e-06, 'beta_0': 0.8666682712341527, 'beta_1': 0.9899669261029043, 'epsilon': 5.168747425394156e-07, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:20:32,204] Trial 244 finished with value: 0.8303030303030303 and parameters: {'batch_size': 13, 'learning_rate': 1.3509305909275976e-05, 'weight_decay': 2.8218950871660043e-05, 'beta_0': 0.8605290108226087, 'beta_1': 0.9976920544349049, 'epsilon': 9.026439667119121e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:23:30,067] Trial 245 finished with value: 0.8181818181818182 and parameters: {'batch_size': 12, 'learning_rate': 1.1859825934171515e-05, 'weight_decay': 7.280079434610532e-06, 'beta_0': 0.851791280432684, 'beta_1': 0.9907079956234647, 'epsilon': 7.454804761263234e-07, 'balanced_loss': True, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:26:13,238] Trial 246 finished with value: 0.8121212121212121 and parameters: {'batch_size': 31, 'learning_rate': 2.4225536403054562e-05, 'weight_decay': 0.00015931531650337935, 'beta_0': 0.8565964662404593, 'beta_1': 0.9989971342184051, 'epsilon': 2.372550521361283e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:28:46,820] Trial 247 finished with value: 0.8363636363636363 and parameters: {'batch_size': 14, 'learning_rate': 1.4819355910023275e-05, 'weight_decay': 0.0003383008839324634, 'beta_0': 0.8630336532957407, 'beta_1': 0.9983329433533193, 'epsilon': 4.649219382865861e-08, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:31:32,502] Trial 248 finished with value: 0.8121212121212121 and parameters: {'batch_size': 10, 'learning_rate': 1.1190435686237787e-05, 'weight_decay': 3.6934918677777625e-05, 'beta_0': 0.8692609116655683, 'beta_1': 0.9985479121846422, 'epsilon': 6.015443304855432e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 155 with value: 0.8606060606060606.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:34:09,749] Trial 249 finished with value: 0.8242424242424242 and parameters: {'batch_size': 12, 'learning_rate': 5.1427724141842675e-05, 'weight_decay': 7.085604058017023e-05, 'beta_0': 0.8732760905799339, 'beta_1': 0.9912855684954737, 'epsilon': 1.1998345330803478e-06, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 155 with value: 0.8606060606060606.

[TRIAL] 205 [VALIDATION PERFORMANCE] 0.8606060606060606 [TRAINING LOSS] 0.02583280726052497 [VALIDATION LOSS] 0.5084118129685521 

number                                 205
value                             0.860606
params_balanced_loss                 False
params_batch_size                       11
params_early_stopping_patience           5
params_epochs                           12
params_learning_rate              0.000012
params_plateau_divider                   8
params_plateau_patience                  3
params_weight_decay               0.000012
params_beta_0                     0.867305
params_beta_1                     0.998976
params_epsilon                         0.0
user_attrs_epoch                        10
user_attrs_training_loss          0.025833
user_attrs_validation_loss        0.508412
Name: 205, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.8181818181818182 Test: 0.8895522388059701
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.8303030303030303 Test: 0.7791044776119403
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.7818181818181819 Test: 0.8119402985074626
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.8181818181818182 Test: 0.8417910447761194
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.8242424242424242 Test: 0.8537313432835821
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.8606060606060606 Test: 0.8567164179104477
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.7818181818181819 Test: 0.808955223880597
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.8363636363636363 Test: 0.826865671641791
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.8181818181818182 Test: 0.7701492537313432
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.806060606060606 Test: 0.7791044776119403
Validation performance: 78.18 & 81.76 ± 2.38 & 86.06
Testing performance: 77.01 & 82.18 ± 3.92 & 88.96

[TRIAL] 155 [VALIDATION PERFORMANCE] 0.8606060606060606 [TRAINING LOSS] 0.014606437139103519 [VALIDATION LOSS] 0.5499361462213777 

number                                 155
value                             0.860606
params_balanced_loss                  True
params_batch_size                       15
params_early_stopping_patience           5
params_epochs                           11
params_learning_rate              0.000033
params_plateau_divider                   5
params_plateau_patience                  3
params_weight_decay               0.000036
params_beta_0                     0.879153
params_beta_1                     0.994104
params_epsilon                         0.0
user_attrs_epoch                         8
user_attrs_training_loss          0.014606
user_attrs_validation_loss        0.549936
Name: 155, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.8242424242424242 Test: 0.8149253731343283
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.806060606060606 Test: 0.7671641791044777
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.806060606060606 Test: 0.8656716417910447
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.8 Test: 0.7880597014925373
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.8121212121212121 Test: 0.8
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.8606060606060606 Test: 0.8776119402985074
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.8424242424242424 Test: 0.8238805970149253
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.7636363636363637 Test: 0.8
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.8 Test: 0.7940298507462686
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.8242424242424242 Test: 0.8029850746268656
Validation performance: 76.36 & 81.39 ± 2.63 & 86.06
Testing performance: 76.72 & 81.34 ± 3.43 & 87.76

[TRIAL] 104 [VALIDATION PERFORMANCE] 0.8545454545454545 [TRAINING LOSS] 0.001711582587588401 [VALIDATION LOSS] 0.7388326066845495 

number                                 104
value                             0.854545
params_balanced_loss                  True
params_batch_size                       12
params_early_stopping_patience           5
params_epochs                           12
params_learning_rate              0.000022
params_plateau_divider                   6
params_plateau_patience                  5
params_weight_decay               0.000001
params_beta_0                     0.861065
params_beta_1                     0.997036
params_epsilon                         0.0
user_attrs_epoch                        10
user_attrs_training_loss          0.001712
user_attrs_validation_loss        0.738833
Name: 104, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.8363636363636363 Test: 0.8388059701492537
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.8363636363636363 Test: 0.8029850746268656
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.8 Test: 0.8149253731343283
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.8545454545454545 Test: 0.8388059701492537
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.8181818181818182 Test: 0.8298507462686567
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.8545454545454545 Test: 0.8626865671641791
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.8 Test: 0.8298507462686567
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.8545454545454545 Test: 0.8716417910447761
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.793939393939394 Test: 0.8059701492537313
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.806060606060606 Test: 0.817910447761194
Validation performance: 79.39 & 82.55 ± 2.47 & 85.45
Testing performance: 80.3 & 83.13 ± 2.27 & 87.16

[TRIAL] 48 [VALIDATION PERFORMANCE] 0.8484848484848485 [TRAINING LOSS] 0.0810806656123272 [VALIDATION LOSS] 0.48847907156284365 

number                                  48
value                             0.848485
params_balanced_loss                 False
params_batch_size                       12
params_early_stopping_patience           2
params_epochs                           12
params_learning_rate              0.000017
params_plateau_divider                   7
params_plateau_patience                  4
params_weight_decay               0.000002
params_beta_0                      0.85526
params_beta_1                     0.990276
params_epsilon                    0.000003
user_attrs_epoch                         6
user_attrs_training_loss          0.081081
user_attrs_validation_loss        0.488479
Name: 48, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.8242424242424242 Test: 0.826865671641791
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.8303030303030303 Test: 0.7850746268656716
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.8 Test: 0.8238805970149253
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.8242424242424242 Test: 0.8388059701492537
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.8 Test: 0.8298507462686567
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.8484848484848485 Test: 0.844776119402985
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.8242424242424242 Test: 0.8477611940298507
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.8363636363636363 Test: 0.8298507462686567
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.806060606060606 Test: 0.7910447761194029
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.8303030303030303 Test: 0.8059701492537313
Validation performance: 80.0 & 82.24 ± 1.59 & 84.85
Testing performance: 78.51 & 82.24 ± 2.16 & 84.78

[TRIAL] 206 [VALIDATION PERFORMANCE] 0.8484848484848485 [TRAINING LOSS] 0.01864036916674155 [VALIDATION LOSS] 0.5070792473852634 

number                                 206
value                             0.848485
params_balanced_loss                 False
params_batch_size                       11
params_early_stopping_patience           5
params_epochs                           12
params_learning_rate              0.000012
params_plateau_divider                   8
params_plateau_patience                  3
params_weight_decay               0.000014
params_beta_0                     0.870775
params_beta_1                     0.998771
params_epsilon                         0.0
user_attrs_epoch                         9
user_attrs_training_loss           0.01864
user_attrs_validation_loss        0.507079
Name: 206, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.8181818181818182 Test: 0.8686567164179104
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.8242424242424242 Test: 0.7701492537313432
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.793939393939394 Test: 0.808955223880597
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.8121212121212121 Test: 0.835820895522388
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.8121212121212121 Test: 0.8537313432835821
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.8484848484848485 Test: 0.8656716417910447
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.7878787878787878 Test: 0.8417910447761194
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.8363636363636363 Test: 0.8119402985074626
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.8181818181818182 Test: 0.7791044776119403
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.8121212121212121 Test: 0.7761194029850746
Validation performance: 78.79 & 81.64 ± 1.79 & 84.85
Testing performance: 77.01 & 82.12 ± 3.74 & 86.87

[IMDb-top_1000] Elapsed time: 737.1548672954242 minutes.
