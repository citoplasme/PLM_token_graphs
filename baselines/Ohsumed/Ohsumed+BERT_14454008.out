[I 2025-01-14 10:31:14,230] A new study created in RDB with name: Ohsumed-google-bert-bert-base-uncased
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:42:35,090] Trial 0 finished with value: 0.5828195009772988 and parameters: {'batch_size': 17, 'learning_rate': 8.927180304353642e-05, 'weight_decay': 0.000157029708840554, 'beta_0': 0.8584457968114998, 'beta_1': 0.982940386538606, 'epsilon': 4.207053950287927e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:54:47,944] Trial 1 finished with value: 0.5798829788360159 and parameters: {'batch_size': 28, 'learning_rate': 1.6305687346221468e-05, 'weight_decay': 3.5113563139704077e-06, 'beta_0': 0.817469560807162, 'beta_1': 0.9857420365812439, 'epsilon': 1.2561043700013556e-06, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 10:58:34,918] Trial 2 finished with value: 0.4529769372164429 and parameters: {'batch_size': 19, 'learning_rate': 6.097839109531521e-05, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 3, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:06:19,340] Trial 3 finished with value: 0.5222401127622506 and parameters: {'batch_size': 15, 'learning_rate': 1.2521954287060383e-05, 'weight_decay': 0.00011290133559092664, 'beta_0': 0.8425678764767325, 'beta_1': 0.9822992283341428, 'epsilon': 9.565499215943814e-07, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:11:17,822] Trial 4 finished with value: 0.43326823656121194 and parameters: {'batch_size': 21, 'learning_rate': 1.5305744365500174e-05, 'weight_decay': 0.0008105016126411582, 'beta_0': 0.8764759143554902, 'beta_1': 0.9978400818071855, 'epsilon': 3.79585314267064e-05, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:15:05,636] Trial 5 finished with value: 0.442885183069779 and parameters: {'batch_size': 17, 'learning_rate': 1.8678802571070677e-05, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 3, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:24:31,701] Trial 6 finished with value: 0.012894693337741774 and parameters: {'batch_size': 8, 'learning_rate': 6.538248584518044e-05, 'weight_decay': 0.00013199942261535007, 'beta_0': 0.8717270901699462, 'beta_1': 0.9946218995643121, 'epsilon': 1.9777828512462707e-08, 'balanced_loss': True, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 0 with value: 0.5828195009772988.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:36:01,981] Trial 7 finished with value: 0.6129724426786131 and parameters: {'batch_size': 15, 'learning_rate': 2.1143813626634362e-05, 'weight_decay': 0.0001544608907504709, 'beta_0': 0.8623879108708258, 'beta_1': 0.9968387427337244, 'epsilon': 7.742116473996242e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.6129724426786131.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:44:36,834] Trial 8 finished with value: 0.5924466340798755 and parameters: {'batch_size': 21, 'learning_rate': 2.676338356766077e-05, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 8}. Best is trial 7 with value: 0.6129724426786131.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 11:54:56,859] Trial 9 finished with value: 0.5693677998295706 and parameters: {'batch_size': 13, 'learning_rate': 1.193932872653544e-05, 'weight_decay': 7.400385759087375e-06, 'beta_0': 0.8153364582789058, 'beta_1': 0.9976522996046951, 'epsilon': 1.7079750342958218e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 7 with value: 0.6129724426786131.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:04:42,980] Trial 10 finished with value: 0.5881213376191723 and parameters: {'batch_size': 27, 'learning_rate': 3.621186546958565e-05, 'weight_decay': 3.105109516983293e-05, 'beta_0': 0.8928510005356737, 'beta_1': 0.9921196188932835, 'epsilon': 1.5931596269242544e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 7 with value: 0.6129724426786131.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:13:31,417] Trial 11 finished with value: 0.612597123075311 and parameters: {'batch_size': 23, 'learning_rate': 2.752869487865569e-05, 'weight_decay': 2.2622914395545082e-05, 'beta_0': 0.8029516623385496, 'beta_1': 0.9876670568719408, 'epsilon': 5.748594743060449e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 8}. Best is trial 7 with value: 0.6129724426786131.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:25:44,297] Trial 12 finished with value: 0.6224597727168378 and parameters: {'batch_size': 32, 'learning_rate': 2.8919428620001574e-05, 'weight_decay': 3.3329093487229174e-05, 'beta_0': 0.8031709457399874, 'beta_1': 0.9879133978268432, 'epsilon': 9.037982255425014e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 12 with value: 0.6224597727168378.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:35:26,685] Trial 13 finished with value: 0.5800358815504643 and parameters: {'batch_size': 31, 'learning_rate': 3.486907167913028e-05, 'weight_decay': 5.986931545119392e-05, 'beta_0': 0.8293159080590211, 'beta_1': 0.989819457844325, 'epsilon': 2.3448827903671333e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 12 with value: 0.6224597727168378.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:48:37,274] Trial 14 finished with value: 0.6224488795640679 and parameters: {'batch_size': 11, 'learning_rate': 2.1479917089099742e-05, 'weight_decay': 1.4017768327991968e-05, 'beta_0': 0.8585996401312975, 'beta_1': 0.994488204264689, 'epsilon': 1.330928045120995e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 12 with value: 0.6224597727168378.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 12:58:18,308] Trial 15 finished with value: 0.6243110328847853 and parameters: {'batch_size': 8, 'learning_rate': 4.4936725874878804e-05, 'weight_decay': 1.3463899860788798e-05, 'beta_0': 0.8832539423532408, 'beta_1': 0.9941607496286264, 'epsilon': 9.032685047597417e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:08:08,457] Trial 16 finished with value: 0.61184305437467 and parameters: {'batch_size': 32, 'learning_rate': 4.894117875814321e-05, 'weight_decay': 9.811535887377362e-06, 'beta_0': 0.8927893595616114, 'beta_1': 0.9934078734818789, 'epsilon': 9.640001971864439e-05, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:15:33,807] Trial 17 finished with value: 0.6099713991345933 and parameters: {'batch_size': 26, 'learning_rate': 4.4966042778124036e-05, 'weight_decay': 6.37166182091669e-05, 'beta_0': 0.8798717297151603, 'beta_1': 0.9881396507398861, 'epsilon': 9.246870883536907e-05, 'balanced_loss': False, 'epochs': 6, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 2}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:27:39,810] Trial 18 finished with value: 0.012894693337741774 and parameters: {'batch_size': 8, 'learning_rate': 9.135915648650754e-05, 'weight_decay': 1.7906920258152189e-06, 'beta_0': 0.8295084046035371, 'beta_1': 0.9899924527326762, 'epsilon': 2.5584229086682364e-05, 'balanced_loss': True, 'epochs': 9, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:36:21,574] Trial 19 finished with value: 0.5758286074705048 and parameters: {'batch_size': 23, 'learning_rate': 4.26684626409619e-05, 'weight_decay': 1.7009389433431733e-05, 'beta_0': 0.8832994146625218, 'beta_1': 0.9957720338859586, 'epsilon': 7.4982192155955535e-06, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 2, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:45:37,895] Trial 20 finished with value: 0.604390023483042 and parameters: {'batch_size': 11, 'learning_rate': 6.110321090129487e-05, 'weight_decay': 4.297892826828267e-05, 'beta_0': 0.8984164422833314, 'beta_1': 0.9862125829986438, 'epsilon': 5.918952579175052e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 15 with value: 0.6243110328847853.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 13:57:30,500] Trial 21 finished with value: 0.6295804277406244 and parameters: {'batch_size': 11, 'learning_rate': 2.4646952885450372e-05, 'weight_decay': 1.316943449263147e-05, 'beta_0': 0.865821369235346, 'beta_1': 0.99338640681645, 'epsilon': 1.3456579142529311e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:10:40,043] Trial 22 finished with value: 0.6107896980375915 and parameters: {'batch_size': 11, 'learning_rate': 2.454288646354181e-05, 'weight_decay': 6.076672186194426e-06, 'beta_0': 0.8649616216483029, 'beta_1': 0.992656631804031, 'epsilon': 1.1214201168623417e-05, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:21:36,265] Trial 23 finished with value: 0.5900809764968298 and parameters: {'batch_size': 8, 'learning_rate': 3.058675803650249e-05, 'weight_decay': 1.2577448085058273e-05, 'beta_0': 0.8484805770958512, 'beta_1': 0.9906951133958362, 'epsilon': 3.2179125209530288e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:30:31,410] Trial 24 finished with value: 0.5952091691492325 and parameters: {'batch_size': 13, 'learning_rate': 3.766920597577474e-05, 'weight_decay': 3.1546482100123276e-05, 'beta_0': 0.8699148852082167, 'beta_1': 0.988324301178394, 'epsilon': 2.9108397396997687e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 7}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:42:16,144] Trial 25 finished with value: 0.6126842160240732 and parameters: {'batch_size': 10, 'learning_rate': 5.164455157371197e-05, 'weight_decay': 3.491449233187986e-06, 'beta_0': 0.8813927584320641, 'beta_1': 0.9954762034479263, 'epsilon': 4.406305932861403e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 14:52:31,740] Trial 26 finished with value: 0.5929856769593641 and parameters: {'batch_size': 13, 'learning_rate': 3.0438798879235553e-05, 'weight_decay': 2.5166954778970024e-05, 'beta_0': 0.8853845525153685, 'beta_1': 0.9929823328004523, 'epsilon': 3.564179565197159e-06, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:02:17,916] Trial 27 finished with value: 0.5979415627606691 and parameters: {'batch_size': 29, 'learning_rate': 7.244350067924426e-05, 'weight_decay': 7.240540452622141e-05, 'beta_0': 0.8527781969148898, 'beta_1': 0.9939916258949097, 'epsilon': 1.647594306741955e-05, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:14:42,515] Trial 28 finished with value: 0.6118907405433022 and parameters: {'batch_size': 24, 'learning_rate': 2.2484221994046762e-05, 'weight_decay': 6.121225001343649e-06, 'beta_0': 0.8013492282924527, 'beta_1': 0.9913672684137919, 'epsilon': 3.8692941423321504e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 7}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:24:38,605] Trial 29 finished with value: 0.5992453812501509 and parameters: {'batch_size': 19, 'learning_rate': 4.057733256173183e-05, 'weight_decay': 1.8404456643537255e-05, 'beta_0': 0.8393149405859364, 'beta_1': 0.9961628803980297, 'epsilon': 6.236039920397267e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 10}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:37:09,799] Trial 30 finished with value: 0.012894693337741774 and parameters: {'batch_size': 16, 'learning_rate': 7.72663379543118e-05, 'weight_decay': 0.00024161773540486132, 'beta_0': 0.871129427580339, 'beta_1': 0.9833332901425064, 'epsilon': 6.956551918419403e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 9}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 15:49:02,749] Trial 31 finished with value: 0.5830564716098939 and parameters: {'batch_size': 11, 'learning_rate': 1.8233802183005765e-05, 'weight_decay': 1.2036088154000107e-05, 'beta_0': 0.8564341977947233, 'beta_1': 0.9946671163625331, 'epsilon': 1.463109365627238e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:03:24,074] Trial 32 finished with value: 0.5941087067182054 and parameters: {'batch_size': 10, 'learning_rate': 1.4479301250992525e-05, 'weight_decay': 1.4562641470683004e-05, 'beta_0': 0.8617604512221048, 'beta_1': 0.9986271405591625, 'epsilon': 1.7946406154842426e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:14:07,969] Trial 33 finished with value: 0.6137022077575889 and parameters: {'batch_size': 9, 'learning_rate': 2.1036860936784676e-05, 'weight_decay': 4.5989957126030175e-05, 'beta_0': 0.8448825132924089, 'beta_1': 0.9920379346460467, 'epsilon': 2.3418008578014853e-05, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:26:25,133] Trial 34 finished with value: 0.5667644721114506 and parameters: {'batch_size': 14, 'learning_rate': 2.5285494829167627e-05, 'weight_decay': 2.4702467083346133e-06, 'beta_0': 0.8570907353518025, 'beta_1': 0.9868171425060005, 'epsilon': 5.4117393599369125e-05, 'balanced_loss': True, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:37:43,116] Trial 35 finished with value: 0.5621254881475243 and parameters: {'batch_size': 17, 'learning_rate': 1.0033443554580133e-05, 'weight_decay': 8.380528476499667e-06, 'beta_0': 0.888750424899215, 'beta_1': 0.98949744841095, 'epsilon': 8.408091458994466e-06, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:49:14,671] Trial 36 finished with value: 0.611180352507004 and parameters: {'batch_size': 12, 'learning_rate': 1.772049015624258e-05, 'weight_decay': 4.370044826264924e-06, 'beta_0': 0.876182156675694, 'beta_1': 0.9941448688923895, 'epsilon': 2.053847267881777e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 16:57:22,380] Trial 37 finished with value: 0.5644635898316602 and parameters: {'batch_size': 9, 'learning_rate': 5.333842041648355e-05, 'weight_decay': 8.301041598999121e-05, 'beta_0': 0.8222238099975714, 'beta_1': 0.9846372099873028, 'epsilon': 1.1605709644922947e-05, 'balanced_loss': True, 'epochs': 7, 'early_stopping_patience': 2, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:08:39,064] Trial 38 finished with value: 0.6146722213792681 and parameters: {'batch_size': 17, 'learning_rate': 3.190625950132717e-05, 'weight_decay': 3.866685641158117e-05, 'beta_0': 0.8667770141534287, 'beta_1': 0.9909708707312775, 'epsilon': 6.926853457194582e-07, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:21:05,064] Trial 39 finished with value: 0.6127264417053704 and parameters: {'batch_size': 19, 'learning_rate': 2.242388516007605e-05, 'weight_decay': 0.0009185431669744816, 'beta_0': 0.8759952361995414, 'beta_1': 0.9971879808867795, 'epsilon': 4.5104477945935156e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 6}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:36:15,814] Trial 40 finished with value: 0.5978567145668222 and parameters: {'batch_size': 15, 'learning_rate': 1.4987313921372496e-05, 'weight_decay': 2.1288816375577376e-05, 'beta_0': 0.8515675542840162, 'beta_1': 0.9989238040795587, 'epsilon': 3.702262214969506e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 9}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:46:45,187] Trial 41 finished with value: 0.6202814050688691 and parameters: {'batch_size': 10, 'learning_rate': 3.197192704474243e-05, 'weight_decay': 4.1934121653422066e-05, 'beta_0': 0.8669756452167764, 'beta_1': 0.991680485347784, 'epsilon': 9.424345163779942e-07, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 17:57:13,987] Trial 42 finished with value: 0.6017829050865869 and parameters: {'batch_size': 10, 'learning_rate': 2.829924936649153e-05, 'weight_decay': 0.000101157069774824, 'beta_0': 0.8602231740875685, 'beta_1': 0.9936685796914793, 'epsilon': 5.174309209801442e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:07:25,525] Trial 43 finished with value: 0.6084801533906571 and parameters: {'batch_size': 12, 'learning_rate': 3.418211073218282e-05, 'weight_decay': 1.1872302387616416e-05, 'beta_0': 0.8668050088359512, 'beta_1': 0.9919298420606586, 'epsilon': 1.3179287349873441e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:18:22,436] Trial 44 finished with value: 0.6084893942788944 and parameters: {'batch_size': 8, 'learning_rate': 2.0686760963367827e-05, 'weight_decay': 3.1039918610246496e-05, 'beta_0': 0.8381814519767522, 'beta_1': 0.9948588648784958, 'epsilon': 2.4308156633898476e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:23:50,896] Trial 45 finished with value: 0.5468849252974618 and parameters: {'batch_size': 9, 'learning_rate': 2.4807351035584378e-05, 'weight_decay': 5.442612726327373e-05, 'beta_0': 0.874381379903249, 'beta_1': 0.996268895618079, 'epsilon': 1.0132223692905025e-06, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:32:47,535] Trial 46 finished with value: 0.5906162100433722 and parameters: {'batch_size': 12, 'learning_rate': 3.948896939373244e-05, 'weight_decay': 0.0004478247625271731, 'beta_0': 0.8472277622594189, 'beta_1': 0.988811905706879, 'epsilon': 1.9087129074951056e-07, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:43:55,480] Trial 47 finished with value: 0.5972381664531695 and parameters: {'batch_size': 14, 'learning_rate': 2.8299026768515838e-05, 'weight_decay': 2.2507992361892614e-05, 'beta_0': 0.8558995117744914, 'beta_1': 0.9927558159177661, 'epsilon': 8.315643127388813e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 6}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 18:52:38,054] Trial 48 finished with value: 0.5856028818494682 and parameters: {'batch_size': 30, 'learning_rate': 3.2571301629919606e-05, 'weight_decay': 9.705239096701394e-06, 'beta_0': 0.8641502551614961, 'beta_1': 0.9905552025583593, 'epsilon': 6.565816792223688e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 2, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:03:52,825] Trial 49 finished with value: 0.6031546016913466 and parameters: {'batch_size': 25, 'learning_rate': 1.9881841224689684e-05, 'weight_decay': 5.242254211735114e-06, 'beta_0': 0.8058366419535519, 'beta_1': 0.9870014148476732, 'epsilon': 2.4048392939004348e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:13:46,542] Trial 50 finished with value: 0.5955728423929934 and parameters: {'batch_size': 21, 'learning_rate': 2.3122351863162775e-05, 'weight_decay': 1.526864498377942e-05, 'beta_0': 0.8124993359466968, 'beta_1': 0.9951884154915283, 'epsilon': 2.376132310411146e-05, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:25:18,608] Trial 51 finished with value: 0.5970779478095861 and parameters: {'batch_size': 16, 'learning_rate': 3.203364687236155e-05, 'weight_decay': 3.0476645278322167e-05, 'beta_0': 0.8671097556744771, 'beta_1': 0.9912802630691914, 'epsilon': 6.59812547573654e-07, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:35:28,954] Trial 52 finished with value: 0.619563703980762 and parameters: {'batch_size': 18, 'learning_rate': 4.418265874054356e-05, 'weight_decay': 4.675112977899225e-05, 'beta_0': 0.8701310048333467, 'beta_1': 0.9904315083200743, 'epsilon': 3.5975903758560744e-07, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:46:31,447] Trial 53 finished with value: 0.6239863973220565 and parameters: {'batch_size': 27, 'learning_rate': 4.77794731885723e-05, 'weight_decay': 4.000219922573808e-05, 'beta_0': 0.8796439579754572, 'beta_1': 0.9888759146793852, 'epsilon': 4.3048803523313794e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 19:56:22,549] Trial 54 finished with value: 0.6049763459321819 and parameters: {'batch_size': 29, 'learning_rate': 5.3468899517406284e-05, 'weight_decay': 0.00012048439739144044, 'beta_0': 0.8791972611527421, 'beta_1': 0.985294566022779, 'epsilon': 2.8871118657276205e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:06:11,805] Trial 55 finished with value: 0.61497405521406 and parameters: {'batch_size': 26, 'learning_rate': 4.663274015157391e-05, 'weight_decay': 3.6957131870036326e-05, 'beta_0': 0.887791770842164, 'beta_1': 0.9873694919497074, 'epsilon': 9.76293995922059e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:19:37,493] Trial 56 finished with value: 0.5974568608412908 and parameters: {'batch_size': 27, 'learning_rate': 1.6380066493184455e-05, 'weight_decay': 2.5082143539689e-05, 'beta_0': 0.8931647557841517, 'beta_1': 0.9932138412347321, 'epsilon': 4.638673698964808e-06, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 21 with value: 0.6295804277406244.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:29:23,091] Trial 57 finished with value: 0.6496777958109826 and parameters: {'batch_size': 32, 'learning_rate': 3.668609126919097e-05, 'weight_decay': 1.8399610479384876e-05, 'beta_0': 0.8734598951226153, 'beta_1': 0.9891135336638085, 'epsilon': 1.1136372668466935e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:39:09,060] Trial 58 finished with value: 0.6015139049554358 and parameters: {'batch_size': 32, 'learning_rate': 3.718268623671705e-05, 'weight_decay': 7.471676302943446e-06, 'beta_0': 0.8216250214369071, 'beta_1': 0.9888908916120662, 'epsilon': 1.0894544432146897e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:48:58,289] Trial 59 finished with value: 0.5886126758876431 and parameters: {'batch_size': 31, 'learning_rate': 4.9725528720116844e-05, 'weight_decay': 1.8062282947052057e-05, 'beta_0': 0.8970594990185977, 'beta_1': 0.9880477956113156, 'epsilon': 3.532813727082028e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 20:58:41,913] Trial 60 finished with value: 0.5917017803122082 and parameters: {'batch_size': 31, 'learning_rate': 5.698785548057931e-05, 'weight_decay': 1.0817935130641995e-05, 'beta_0': 0.884744641208265, 'beta_1': 0.9898802269458181, 'epsilon': 1.115319730217125e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:08:32,650] Trial 61 finished with value: 0.5810111098146268 and parameters: {'batch_size': 28, 'learning_rate': 4.14442182510727e-05, 'weight_decay': 1.591829110594817e-05, 'beta_0': 0.8735828897366001, 'beta_1': 0.9893002150150181, 'epsilon': 1.198850432672108e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:19:41,800] Trial 62 finished with value: 0.6025930847674126 and parameters: {'batch_size': 30, 'learning_rate': 2.6740573474511542e-05, 'weight_decay': 2.84008426944814e-05, 'beta_0': 0.8798138792444317, 'beta_1': 0.9922638208466076, 'epsilon': 8.636093040056193e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:28:29,189] Trial 63 finished with value: 0.5973007188426893 and parameters: {'batch_size': 22, 'learning_rate': 3.6599571398538314e-05, 'weight_decay': 5.819204662366195e-05, 'beta_0': 0.8607488172733553, 'beta_1': 0.9885228800595797, 'epsilon': 5.00805877799912e-07, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:37:47,746] Trial 64 finished with value: 0.6165041306313606 and parameters: {'batch_size': 11, 'learning_rate': 2.856815837371229e-05, 'weight_decay': 2.0768184085950962e-05, 'beta_0': 0.8715131607523446, 'beta_1': 0.9860829256577122, 'epsilon': 2.7740279555670554e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:48:48,892] Trial 65 finished with value: 0.6212885505019147 and parameters: {'batch_size': 31, 'learning_rate': 3.4047425064179236e-05, 'weight_decay': 7.670488426742498e-05, 'beta_0': 0.8784483322799869, 'beta_1': 0.9941757383875927, 'epsilon': 1.7826735163213222e-05, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 21:58:36,211] Trial 66 finished with value: 0.6029275306047441 and parameters: {'batch_size': 32, 'learning_rate': 6.702192797380429e-05, 'weight_decay': 8.528524380975287e-05, 'beta_0': 0.8823069348252685, 'beta_1': 0.9943070661000858, 'epsilon': 1.1701019847160936e-05, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:07:09,622] Trial 67 finished with value: 0.5870093606133153 and parameters: {'batch_size': 30, 'learning_rate': 4.663626314634057e-05, 'weight_decay': 0.00018136809769746573, 'beta_0': 0.8894995545893374, 'beta_1': 0.9970989733271559, 'epsilon': 1.7256220153945325e-05, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 6}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:19:26,424] Trial 68 finished with value: 0.6025925348982732 and parameters: {'batch_size': 31, 'learning_rate': 2.9721648125611874e-05, 'weight_decay': 1.4219967741298601e-05, 'beta_0': 0.8865018781340719, 'beta_1': 0.9800013814926112, 'epsilon': 7.68173631277018e-05, 'balanced_loss': True, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:30:26,276] Trial 69 finished with value: 0.5891129003801874 and parameters: {'batch_size': 29, 'learning_rate': 3.415001379245395e-05, 'weight_decay': 9.835267660933627e-06, 'beta_0': 0.8541266481567796, 'beta_1': 0.9959337174582035, 'epsilon': 2.9961203933355883e-05, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:42:38,928] Trial 70 finished with value: 0.6130698435106506 and parameters: {'batch_size': 28, 'learning_rate': 3.933302187680622e-05, 'weight_decay': 7.124202206986083e-06, 'beta_0': 0.8772654868521261, 'beta_1': 0.9936146547987504, 'epsilon': 4.423574931381821e-05, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 5, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 22:54:53,691] Trial 71 finished with value: 0.6073248398029542 and parameters: {'batch_size': 32, 'learning_rate': 2.427190092722716e-05, 'weight_decay': 3.723629788861916e-05, 'beta_0': 0.8680061837752265, 'beta_1': 0.991494202395416, 'epsilon': 1.6137127822248505e-06, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:05:20,201] Trial 72 finished with value: 0.597669204101243 and parameters: {'batch_size': 10, 'learning_rate': 3.567713484496221e-05, 'weight_decay': 6.835359702481937e-05, 'beta_0': 0.8640676334551735, 'beta_1': 0.9952811987348394, 'epsilon': 6.3398892451837485e-06, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:16:07,214] Trial 73 finished with value: 0.6023486416123232 and parameters: {'batch_size': 9, 'learning_rate': 2.6279824701938732e-05, 'weight_decay': 5.003944084313021e-05, 'beta_0': 0.872390002817753, 'beta_1': 0.9924641276288868, 'epsilon': 1.66983470484748e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:27:04,387] Trial 74 finished with value: 0.5944965374710529 and parameters: {'batch_size': 8, 'learning_rate': 3.082639830561119e-05, 'weight_decay': 2.576904007957301e-05, 'beta_0': 0.8593372259923129, 'beta_1': 0.9944131388400587, 'epsilon': 5.4749424332067176e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:36:54,005] Trial 75 finished with value: 0.6134164054434876 and parameters: {'batch_size': 32, 'learning_rate': 4.2803158947051664e-05, 'weight_decay': 1.848649191769747e-05, 'beta_0': 0.877598339440353, 'beta_1': 0.9933349510372321, 'epsilon': 2.0259814173973995e-05, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:47:58,974] Trial 76 finished with value: 0.5961184105873154 and parameters: {'batch_size': 29, 'learning_rate': 3.887908156228996e-05, 'weight_decay': 3.735117221628368e-05, 'beta_0': 0.8687560194209272, 'beta_1': 0.9877891053169163, 'epsilon': 1.0104747811469227e-05, 'balanced_loss': False, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 7}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-14 23:59:50,578] Trial 77 finished with value: 0.5902669341394299 and parameters: {'batch_size': 11, 'learning_rate': 1.9515009575595605e-05, 'weight_decay': 1.2780678899519518e-05, 'beta_0': 0.8743608787104873, 'beta_1': 0.9965112660617044, 'epsilon': 3.4377445564922814e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:09:34,852] Trial 78 finished with value: 0.588200697104651 and parameters: {'batch_size': 31, 'learning_rate': 3.402720196898983e-05, 'weight_decay': 8.590294245183621e-05, 'beta_0': 0.881769454465077, 'beta_1': 0.9917636474349084, 'epsilon': 4.4357493441926255e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 3, 'plateau_patience': 5, 'plateau_divider': 8}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:23:53,319] Trial 79 finished with value: 0.6133213154278789 and parameters: {'batch_size': 10, 'learning_rate': 2.311861508370172e-05, 'weight_decay': 2.0847759529666654e-05, 'beta_0': 0.8638478316636445, 'beta_1': 0.9940164681981544, 'epsilon': 1.3392089069626083e-05, 'balanced_loss': True, 'epochs': 15, 'early_stopping_patience': 4, 'plateau_patience': 4, 'plateau_divider': 5}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:33:44,812] Trial 80 finished with value: 0.6198415770033087 and parameters: {'batch_size': 27, 'learning_rate': 5.766983349089272e-05, 'weight_decay': 0.00010447920361739881, 'beta_0': 0.8280332175683166, 'beta_1': 0.9893945794059112, 'epsilon': 2.0600175545447874e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:43:37,918] Trial 81 finished with value: 0.6337434457508451 and parameters: {'batch_size': 25, 'learning_rate': 6.0914579074582644e-05, 'weight_decay': 6.555811737621927e-05, 'beta_0': 0.8165347938132159, 'beta_1': 0.9902388152621519, 'epsilon': 1.8185851051935966e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 00:54:44,189] Trial 82 finished with value: 0.5962210941400058 and parameters: {'batch_size': 24, 'learning_rate': 7.790037981050509e-05, 'weight_decay': 0.00014710824654909273, 'beta_0': 0.8091106195666857, 'beta_1': 0.9902094886650052, 'epsilon': 1.310560275892617e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:04:38,456] Trial 83 finished with value: 0.6243218508615851 and parameters: {'batch_size': 30, 'learning_rate': 6.1000731633833025e-05, 'weight_decay': 6.641808761892999e-05, 'beta_0': 0.8160783176430393, 'beta_1': 0.9908604968407377, 'epsilon': 1.5665892069155838e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:14:26,762] Trial 84 finished with value: 0.6120970169946096 and parameters: {'batch_size': 30, 'learning_rate': 6.443404533958575e-05, 'weight_decay': 6.722737390670937e-05, 'beta_0': 0.8184595993190826, 'beta_1': 0.987352142631516, 'epsilon': 1.504706562874064e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:25:32,341] Trial 85 finished with value: 0.6384219704507148 and parameters: {'batch_size': 25, 'learning_rate': 6.856471588314857e-05, 'weight_decay': 7.737071677367614e-05, 'beta_0': 0.8044084744535549, 'beta_1': 0.9908785886178577, 'epsilon': 6.253157031122558e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:35:29,935] Trial 86 finished with value: 0.6215256991542423 and parameters: {'batch_size': 25, 'learning_rate': 7.250251719411717e-05, 'weight_decay': 0.00020178797367140922, 'beta_0': 0.8043910268805394, 'beta_1': 0.9910272790920885, 'epsilon': 2.772463504627249e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:46:29,543] Trial 87 finished with value: 0.609813629476288 and parameters: {'batch_size': 26, 'learning_rate': 9.491574017298361e-05, 'weight_decay': 5.172196671936056e-05, 'beta_0': 0.8083730967148506, 'beta_1': 0.9900586153577138, 'epsilon': 8.315738518009285e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 01:55:13,543] Trial 88 finished with value: 0.6205300764645567 and parameters: {'batch_size': 23, 'learning_rate': 7.90020056406129e-05, 'weight_decay': 5.854948547846841e-05, 'beta_0': 0.8140781119023927, 'beta_1': 0.9888992276768668, 'epsilon': 3.7726483355457834e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:06:22,278] Trial 89 finished with value: 0.5997943659401618 and parameters: {'batch_size': 20, 'learning_rate': 8.524574893324175e-05, 'weight_decay': 8.769957596994547e-06, 'beta_0': 0.801103480286136, 'beta_1': 0.99060988086783, 'epsilon': 5.2866698100793436e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:16:07,101] Trial 90 finished with value: 0.6084736270329396 and parameters: {'batch_size': 28, 'learning_rate': 6.284828809532302e-05, 'weight_decay': 9.534506563556498e-05, 'beta_0': 0.8060687739027715, 'beta_1': 0.9866483015955833, 'epsilon': 9.456971006858771e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:25:58,151] Trial 91 finished with value: 0.627816126799612 and parameters: {'batch_size': 25, 'learning_rate': 6.928879327019037e-05, 'weight_decay': 0.00044679466802709613, 'beta_0': 0.8042548655814316, 'beta_1': 0.9909627375334737, 'epsilon': 3.1034794536018576e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:37:11,641] Trial 92 finished with value: 0.6203402898370102 and parameters: {'batch_size': 25, 'learning_rate': 6.799819055232764e-05, 'weight_decay': 0.00043804426435592903, 'beta_0': 0.8074501484842705, 'beta_1': 0.9891830197949989, 'epsilon': 1.572095396088973e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:47:03,059] Trial 93 finished with value: 0.6315942031076806 and parameters: {'batch_size': 24, 'learning_rate': 5.790322284093474e-05, 'weight_decay': 0.0005071524974550646, 'beta_0': 0.8115301010230799, 'beta_1': 0.9898139766401691, 'epsilon': 3.725234610332298e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 02:57:01,622] Trial 94 finished with value: 0.6259793352876389 and parameters: {'batch_size': 23, 'learning_rate': 5.906172308965128e-05, 'weight_decay': 0.0005905558311470639, 'beta_0': 0.8110031927427659, 'beta_1': 0.9883911506666845, 'epsilon': 3.165459239341589e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:08:02,643] Trial 95 finished with value: 0.6090375103991663 and parameters: {'batch_size': 24, 'learning_rate': 5.776786269121117e-05, 'weight_decay': 0.0007085102028970965, 'beta_0': 0.8107070143187232, 'beta_1': 0.9885097175327435, 'epsilon': 3.737604335001234e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:17:59,950] Trial 96 finished with value: 0.6113972357308203 and parameters: {'batch_size': 23, 'learning_rate': 5.489729067574854e-05, 'weight_decay': 0.0003856235821118488, 'beta_0': 0.8164713636991683, 'beta_1': 0.989694161633401, 'epsilon': 2.432829781528442e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:29:05,411] Trial 97 finished with value: 0.600920775627445 and parameters: {'batch_size': 22, 'learning_rate': 6.085383110969693e-05, 'weight_decay': 0.000969756875287186, 'beta_0': 0.8118925287637629, 'beta_1': 0.9897756011325711, 'epsilon': 4.951372542206021e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:38:52,810] Trial 98 finished with value: 0.5883024418903807 and parameters: {'batch_size': 24, 'learning_rate': 7.128484848860783e-05, 'weight_decay': 0.0005843876088281413, 'beta_0': 0.8186587977755274, 'beta_1': 0.9911237644091834, 'epsilon': 1.8019417048009686e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:48:47,000] Trial 99 finished with value: 0.6044416360366454 and parameters: {'batch_size': 25, 'learning_rate': 5.177086777585913e-05, 'weight_decay': 0.00031137378470912563, 'beta_0': 0.8224785843701826, 'beta_1': 0.9906186344674132, 'epsilon': 3.090413549818894e-07, 'balanced_loss': True, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 03:58:39,253] Trial 100 finished with value: 0.6435553309440804 and parameters: {'batch_size': 27, 'learning_rate': 4.8668768239089215e-05, 'weight_decay': 0.0007024297497358707, 'beta_0': 0.8034799307754306, 'beta_1': 0.9902401442885614, 'epsilon': 5.975452332164946e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:09:37,930] Trial 101 finished with value: 0.6236145912517072 and parameters: {'batch_size': 27, 'learning_rate': 4.958298920415888e-05, 'weight_decay': 0.0008054589432229263, 'beta_0': 0.8022951182239564, 'beta_1': 0.9902318383855652, 'epsilon': 6.643060436987016e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:19:28,986] Trial 102 finished with value: 0.5907050710627255 and parameters: {'batch_size': 22, 'learning_rate': 6.0294218489992655e-05, 'weight_decay': 0.0006444074498495039, 'beta_0': 0.8042336603529696, 'beta_1': 0.9891922405730793, 'epsilon': 4.294083131605458e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:30:30,064] Trial 103 finished with value: 0.6214744752080952 and parameters: {'batch_size': 26, 'learning_rate': 4.63913870550903e-05, 'weight_decay': 0.00048798124774948896, 'beta_0': 0.8063882070167316, 'beta_1': 0.9908751802637469, 'epsilon': 2.3079278323375293e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:40:16,822] Trial 104 finished with value: 0.6124842144497974 and parameters: {'batch_size': 24, 'learning_rate': 6.939887145474509e-05, 'weight_decay': 0.0005341173337422254, 'beta_0': 0.8000746197836933, 'beta_1': 0.9883648713754067, 'epsilon': 3.4481834499492836e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 04:50:07,153] Trial 105 finished with value: 0.6080060893258097 and parameters: {'batch_size': 26, 'learning_rate': 8.299986657401825e-05, 'weight_decay': 0.0003125361119601041, 'beta_0': 0.8148770280078323, 'beta_1': 0.992869745836815, 'epsilon': 8.553736873812641e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:01:05,631] Trial 106 finished with value: 0.6181993831507873 and parameters: {'batch_size': 27, 'learning_rate': 5.5172837273284714e-05, 'weight_decay': 0.0007376137096137559, 'beta_0': 0.8122868172722408, 'beta_1': 0.9919830911427139, 'epsilon': 5.477632146256719e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:12:20,803] Trial 107 finished with value: 0.5833742789070883 and parameters: {'batch_size': 23, 'learning_rate': 7.456738547902443e-05, 'weight_decay': 0.00039322522428746894, 'beta_0': 0.8104591536133783, 'beta_1': 0.9914682235765281, 'epsilon': 4.326856024196313e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:23:24,810] Trial 108 finished with value: 0.6194322028520622 and parameters: {'batch_size': 25, 'learning_rate': 6.566028996124106e-05, 'weight_decay': 0.0008183532894141186, 'beta_0': 0.8254040477066013, 'beta_1': 0.9902674147392406, 'epsilon': 7.033421621419138e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 5, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:35:37,709] Trial 109 finished with value: 0.6281598358455277 and parameters: {'batch_size': 26, 'learning_rate': 5.208417424332061e-05, 'weight_decay': 0.0003528769344157607, 'beta_0': 0.8036298152706018, 'beta_1': 0.9896912694197086, 'epsilon': 1.157141612481868e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:47:51,396] Trial 110 finished with value: 0.6230367841641418 and parameters: {'batch_size': 26, 'learning_rate': 5.10549497164214e-05, 'weight_decay': 0.0005828121592997207, 'beta_0': 0.8036867109888014, 'beta_1': 0.9896809040228098, 'epsilon': 1.3443215288071965e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 05:57:44,751] Trial 111 finished with value: 0.6229776515305674 and parameters: {'batch_size': 25, 'learning_rate': 5.999152973617796e-05, 'weight_decay': 0.00025143191688517736, 'beta_0': 0.8087956145537938, 'beta_1': 0.9888812817083898, 'epsilon': 1.2295272586518997e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:08:42,698] Trial 112 finished with value: 0.5982082831069789 and parameters: {'batch_size': 27, 'learning_rate': 4.895884065336649e-05, 'weight_decay': 0.00037114533302249903, 'beta_0': 0.806216981346348, 'beta_1': 0.9880432441304047, 'epsilon': 1.0398396634686436e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:19:43,188] Trial 113 finished with value: 0.6100783436599692 and parameters: {'batch_size': 24, 'learning_rate': 4.4754249661442756e-05, 'weight_decay': 0.0004923220222997941, 'beta_0': 0.8138208143375852, 'beta_1': 0.9900047079725605, 'epsilon': 2.580647205673841e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:29:30,431] Trial 114 finished with value: 0.5928988915134367 and parameters: {'batch_size': 26, 'learning_rate': 6.264750044176001e-05, 'weight_decay': 0.00027323209130665623, 'beta_0': 0.8201022187951107, 'beta_1': 0.9893674403418753, 'epsilon': 2.805582291040772e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:40:29,283] Trial 115 finished with value: 0.5986004675229127 and parameters: {'batch_size': 28, 'learning_rate': 5.4245786507556836e-05, 'weight_decay': 0.00012621067949899898, 'beta_0': 0.801485101607063, 'beta_1': 0.9876556556011504, 'epsilon': 1.8853477934987753e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 06:51:41,813] Trial 116 finished with value: 0.6091831980250138 and parameters: {'batch_size': 25, 'learning_rate': 4.7206090256022025e-05, 'weight_decay': 0.0006758214513090326, 'beta_0': 0.8040809793750885, 'beta_1': 0.990789885219639, 'epsilon': 5.987637544821084e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:01:38,669] Trial 117 finished with value: 0.6042009427762242 and parameters: {'batch_size': 23, 'learning_rate': 5.7612907954657256e-05, 'weight_decay': 0.0003459955314174999, 'beta_0': 0.8323765938181892, 'beta_1': 0.988801170859588, 'epsilon': 2.954896143200249e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:09:04,691] Trial 118 finished with value: 0.5862959480203228 and parameters: {'batch_size': 29, 'learning_rate': 4.167758766454227e-05, 'weight_decay': 0.0004376119228615611, 'beta_0': 0.8157901872747723, 'beta_1': 0.9914207481351538, 'epsilon': 7.334449943062616e-07, 'balanced_loss': True, 'epochs': 6, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 10}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:18:56,566] Trial 119 finished with value: 0.6226593231670813 and parameters: {'batch_size': 21, 'learning_rate': 5.2066439299626094e-05, 'weight_decay': 0.0005920486672390726, 'beta_0': 0.8099992840545355, 'beta_1': 0.9905123279172287, 'epsilon': 2.1519522217607997e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:29:52,478] Trial 120 finished with value: 0.6048474680007858 and parameters: {'batch_size': 27, 'learning_rate': 6.49634849355374e-05, 'weight_decay': 2.650037527889089e-05, 'beta_0': 0.883495692889942, 'beta_1': 0.9896702345641593, 'epsilon': 4.20415880256787e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:40:52,200] Trial 121 finished with value: 0.592250057937894 and parameters: {'batch_size': 27, 'learning_rate': 4.903190530706894e-05, 'weight_decay': 0.0007680546419521863, 'beta_0': 0.8023023704690267, 'beta_1': 0.9902565837356628, 'epsilon': 3.269083079866551e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 07:50:42,097] Trial 122 finished with value: 0.605520552035992 and parameters: {'batch_size': 26, 'learning_rate': 5.040062592866716e-05, 'weight_decay': 0.0009952412819893594, 'beta_0': 0.8025623605587924, 'beta_1': 0.9922702230000211, 'epsilon': 5.948648189482804e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:01:46,403] Trial 123 finished with value: 0.6101944974917238 and parameters: {'batch_size': 28, 'learning_rate': 4.356057812085037e-05, 'weight_decay': 0.0007824642426069462, 'beta_0': 0.8000081972190561, 'beta_1': 0.9891164711945977, 'epsilon': 1.139051284602558e-06, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:11:38,908] Trial 124 finished with value: 0.6147271569100057 and parameters: {'batch_size': 25, 'learning_rate': 5.5882946367051224e-05, 'weight_decay': 0.0008184586634289105, 'beta_0': 0.8075342892905277, 'beta_1': 0.9884632299162943, 'epsilon': 7.804434860181464e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:22:45,617] Trial 125 finished with value: 0.6219081384990013 and parameters: {'batch_size': 24, 'learning_rate': 5.9190872521223526e-05, 'weight_decay': 0.00020118976969178356, 'beta_0': 0.8050831116267162, 'beta_1': 0.9911469511952259, 'epsilon': 1.757410358774758e-07, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:33:50,347] Trial 126 finished with value: 0.6221427310163105 and parameters: {'batch_size': 26, 'learning_rate': 4.8211907824407814e-05, 'weight_decay': 0.0005172134772148414, 'beta_0': 0.8124725072776113, 'beta_1': 0.9948710106002979, 'epsilon': 8.058180132269569e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:43:29,523] Trial 127 finished with value: 0.595374096467087 and parameters: {'batch_size': 28, 'learning_rate': 6.199256789108387e-05, 'weight_decay': 1.095199317960289e-05, 'beta_0': 0.8429528481632609, 'beta_1': 0.9901759232654183, 'epsilon': 1.4352213736483657e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 08:54:31,916] Trial 128 finished with value: 0.611282115357704 and parameters: {'batch_size': 27, 'learning_rate': 5.2753966901365676e-05, 'weight_decay': 1.098636902690187e-06, 'beta_0': 0.8082738807829118, 'beta_1': 0.9895459696483717, 'epsilon': 6.893010178614381e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:05:34,721] Trial 129 finished with value: 0.6025765696830595 and parameters: {'batch_size': 20, 'learning_rate': 6.911339395951878e-05, 'weight_decay': 1.6636382560573316e-05, 'beta_0': 0.8020798898771929, 'beta_1': 0.9916345482996021, 'epsilon': 1.987674944633366e-06, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 3}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:16:40,750] Trial 130 finished with value: 0.5860536899353159 and parameters: {'batch_size': 30, 'learning_rate': 7.465436170046372e-05, 'weight_decay': 4.5156805149582015e-05, 'beta_0': 0.8917229934128477, 'beta_1': 0.9821530624917321, 'epsilon': 4.670439127271174e-08, 'balanced_loss': False, 'epochs': 14, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 4}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:27:45,281] Trial 131 finished with value: 0.6390705205417858 and parameters: {'batch_size': 26, 'learning_rate': 5.0575853303634934e-05, 'weight_decay': 0.0006011773528954036, 'beta_0': 0.8036540474297389, 'beta_1': 0.9898268288847859, 'epsilon': 1.2146264551422099e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:38:45,713] Trial 132 finished with value: 0.6344856689645855 and parameters: {'batch_size': 26, 'learning_rate': 4.475548164136603e-05, 'weight_decay': 0.0008707859070339664, 'beta_0': 0.8063504448461901, 'beta_1': 0.9908420237803913, 'epsilon': 5.011774153276421e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 09:49:54,730] Trial 133 finished with value: 0.6237407606548476 and parameters: {'batch_size': 25, 'learning_rate': 4.614965941974219e-05, 'weight_decay': 0.0006158496689859388, 'beta_0': 0.8059329022940505, 'beta_1': 0.9886913605476575, 'epsilon': 1.1746341259736102e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:00:48,710] Trial 134 finished with value: 0.6143288035866687 and parameters: {'batch_size': 26, 'learning_rate': 4.3432296297438625e-05, 'weight_decay': 0.0008977017748550355, 'beta_0': 0.8075082912725229, 'beta_1': 0.9909549185916713, 'epsilon': 9.703994755022389e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:10:40,479] Trial 135 finished with value: 0.6108405623514639 and parameters: {'batch_size': 24, 'learning_rate': 5.5862516217966093e-05, 'weight_decay': 0.00044360012229154174, 'beta_0': 0.8106523017418007, 'beta_1': 0.9898573007354928, 'epsilon': 4.789924327716961e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:21:53,944] Trial 136 finished with value: 0.6140202101420175 and parameters: {'batch_size': 25, 'learning_rate': 4.155392190225242e-05, 'weight_decay': 1.3569428405023394e-05, 'beta_0': 0.804826211105929, 'beta_1': 0.9905897872697381, 'epsilon': 2.52053546685078e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:31:42,125] Trial 137 finished with value: 0.6130101893237125 and parameters: {'batch_size': 26, 'learning_rate': 3.958347431237136e-05, 'weight_decay': 0.0005243435337287152, 'beta_0': 0.817367790514847, 'beta_1': 0.9881070167492587, 'epsilon': 3.536222771424417e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:42:50,841] Trial 138 finished with value: 0.5969130543621697 and parameters: {'batch_size': 24, 'learning_rate': 5.3458831363804285e-05, 'weight_decay': 3.436486237225531e-05, 'beta_0': 0.8799466319572204, 'beta_1': 0.9894649778678825, 'epsilon': 1.645172470236961e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:52:48,557] Trial 139 finished with value: 0.5973635719939424 and parameters: {'batch_size': 22, 'learning_rate': 6.510205504728153e-05, 'weight_decay': 6.0875431272482464e-05, 'beta_0': 0.8093315369317574, 'beta_1': 0.9890996255178197, 'epsilon': 8.918435185968563e-05, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 10:57:44,716] Trial 140 finished with value: 0.5494594967093245 and parameters: {'batch_size': 27, 'learning_rate': 3.789309850484234e-05, 'weight_decay': 7.222335403431393e-05, 'beta_0': 0.8132411647781569, 'beta_1': 0.989893101194559, 'epsilon': 6.49777875971785e-08, 'balanced_loss': False, 'epochs': 4, 'early_stopping_patience': 3, 'plateau_patience': 3, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:08:53,674] Trial 141 finished with value: 0.6163254020182416 and parameters: {'batch_size': 25, 'learning_rate': 4.555303107631746e-05, 'weight_decay': 0.0005868663168475638, 'beta_0': 0.8036903995537927, 'beta_1': 0.987247563787443, 'epsilon': 1.3505222977683752e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:18:51,099] Trial 142 finished with value: 0.6406931711146744 and parameters: {'batch_size': 25, 'learning_rate': 5.138763915586737e-05, 'weight_decay': 0.0006589722738085919, 'beta_0': 0.8064848455417359, 'beta_1': 0.98844335817853, 'epsilon': 1.198641080441556e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:29:56,352] Trial 143 finished with value: 0.6203518827508246 and parameters: {'batch_size': 26, 'learning_rate': 5.912310315404133e-05, 'weight_decay': 0.00038650839031744723, 'beta_0': 0.8071807657539816, 'beta_1': 0.9877576744542742, 'epsilon': 2.299520174407478e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 57 with value: 0.6496777958109826.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:39:51,871] Trial 144 finished with value: 0.6609549168012957 and parameters: {'batch_size': 25, 'learning_rate': 5.105268084197251e-05, 'weight_decay': 0.0005369628950111307, 'beta_0': 0.8111402739409961, 'beta_1': 0.988478390509104, 'epsilon': 8.74164880436343e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 11:50:51,596] Trial 145 finished with value: 0.6229249748389014 and parameters: {'batch_size': 24, 'learning_rate': 5.153927231617205e-05, 'weight_decay': 0.0004609391019430342, 'beta_0': 0.8114899684913898, 'beta_1': 0.9883324217705317, 'epsilon': 1.0068233323839855e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:00:46,018] Trial 146 finished with value: 0.59321321344544 and parameters: {'batch_size': 25, 'learning_rate': 5.739624446971885e-05, 'weight_decay': 0.0006617154476016659, 'beta_0': 0.8054848802596041, 'beta_1': 0.9925318719450282, 'epsilon': 8.344555006754642e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:10:38,491] Trial 147 finished with value: 0.6176138368684871 and parameters: {'batch_size': 23, 'learning_rate': 5.3837391609310145e-05, 'weight_decay': 0.0006945277739343972, 'beta_0': 0.814208190846268, 'beta_1': 0.9904850930278759, 'epsilon': 6.007165457959582e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:20:35,233] Trial 148 finished with value: 0.6347932282092803 and parameters: {'batch_size': 24, 'learning_rate': 6.202225255418103e-05, 'weight_decay': 0.00033443192635208927, 'beta_0': 0.8089682626158375, 'beta_1': 0.991244557235773, 'epsilon': 1.9283707416384714e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:29:13,625] Trial 149 finished with value: 0.594191213552995 and parameters: {'batch_size': 24, 'learning_rate': 6.847652382546266e-05, 'weight_decay': 0.0003225782026958187, 'beta_0': 0.8093273985096981, 'beta_1': 0.9918109247131006, 'epsilon': 1.1499387303289006e-07, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:39:06,301] Trial 150 finished with value: 0.5934426790624717 and parameters: {'batch_size': 23, 'learning_rate': 6.348801163100245e-05, 'weight_decay': 0.0005470502806173459, 'beta_0': 0.8032527901882266, 'beta_1': 0.9912382685721595, 'epsilon': 2.0310586839506532e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:49:00,672] Trial 151 finished with value: 0.6249894606140063 and parameters: {'batch_size': 25, 'learning_rate': 6.142190048051988e-05, 'weight_decay': 0.00041369807390347844, 'beta_0': 0.8158272576989019, 'beta_1': 0.9900748752197484, 'epsilon': 1.5707746938685943e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 12:58:51,428] Trial 152 finished with value: 0.6148111305210022 and parameters: {'batch_size': 25, 'learning_rate': 6.110067552155525e-05, 'weight_decay': 0.00040645052522682155, 'beta_0': 0.8159887254016757, 'beta_1': 0.9902131853853557, 'epsilon': 1.5658474362771332e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:08:37,362] Trial 153 finished with value: 0.6047080453255268 and parameters: {'batch_size': 26, 'learning_rate': 6.668531256390587e-05, 'weight_decay': 0.0008840762672051645, 'beta_0': 0.8194527376232275, 'beta_1': 0.990841995507634, 'epsilon': 8.538134062864372e-08, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:18:31,152] Trial 154 finished with value: 0.6214000370589268 and parameters: {'batch_size': 25, 'learning_rate': 7.096113167609195e-05, 'weight_decay': 0.00024532333031700957, 'beta_0': 0.8112606975927625, 'beta_1': 0.9893947020943226, 'epsilon': 1.1555916797379919e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:28:32,193] Trial 155 finished with value: 0.6172268611363391 and parameters: {'batch_size': 23, 'learning_rate': 5.709676647915678e-05, 'weight_decay': 0.0003409253078964082, 'beta_0': 0.8078410061360819, 'beta_1': 0.9899892119383398, 'epsilon': 2.7555880466540237e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:32:16,271] Trial 156 finished with value: 0.47878360552540117 and parameters: {'batch_size': 24, 'learning_rate': 6.201511944523796e-05, 'weight_decay': 0.0006822730839248059, 'beta_0': 0.8387444115295464, 'beta_1': 0.9908951268040912, 'epsilon': 7.118527193114978e-08, 'balanced_loss': False, 'epochs': 3, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:40:53,220] Trial 157 finished with value: 0.5976804260815991 and parameters: {'batch_size': 25, 'learning_rate': 5.501356780928982e-05, 'weight_decay': 0.00028520065477457657, 'beta_0': 0.8133205415759773, 'beta_1': 0.9891221247717237, 'epsilon': 1.9537126826471895e-07, 'balanced_loss': False, 'epochs': 7, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 13:54:18,307] Trial 158 finished with value: 0.6152471403740664 and parameters: {'batch_size': 26, 'learning_rate': 6.017804840768233e-05, 'weight_decay': 0.0005115602668929664, 'beta_0': 0.8094154264242178, 'beta_1': 0.9905317601603754, 'epsilon': 1.5131323963619972e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:04:07,057] Trial 159 finished with value: 0.624882345319346 and parameters: {'batch_size': 24, 'learning_rate': 5.0081085992342035e-05, 'weight_decay': 0.0004346723864280794, 'beta_0': 0.8004894057941834, 'beta_1': 0.9896220929776182, 'epsilon': 1.3534865578716516e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:15:12,971] Trial 160 finished with value: 0.608573181788611 and parameters: {'batch_size': 24, 'learning_rate': 5.053625439462033e-05, 'weight_decay': 0.0004414573488613216, 'beta_0': 0.8050719544943131, 'beta_1': 0.9896717211919663, 'epsilon': 3.0689513278298476e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:25:10,411] Trial 161 finished with value: 0.6469074538346228 and parameters: {'batch_size': 25, 'learning_rate': 4.819484621789199e-05, 'weight_decay': 0.00036534719650931514, 'beta_0': 0.802452478607373, 'beta_1': 0.9912439114685488, 'epsilon': 1.1937777917409385e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:35:09,334] Trial 162 finished with value: 0.6225096658281438 and parameters: {'batch_size': 25, 'learning_rate': 4.7786682934861975e-05, 'weight_decay': 0.0003642083791028824, 'beta_0': 0.8003034895369845, 'beta_1': 0.9885430538497545, 'epsilon': 9.643026174410099e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:43:50,365] Trial 163 finished with value: 0.5963925006326682 and parameters: {'batch_size': 25, 'learning_rate': 4.98340101776952e-05, 'weight_decay': 0.0004915081111088285, 'beta_0': 0.802253144806583, 'beta_1': 0.991335055232329, 'epsilon': 1.1995397215031727e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 2, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 14:54:50,669] Trial 164 finished with value: 0.5939602915303659 and parameters: {'batch_size': 24, 'learning_rate': 5.1639993703385424e-05, 'weight_decay': 0.0002900841984123411, 'beta_0': 0.8066494873030782, 'beta_1': 0.9899827110142483, 'epsilon': 1.2900112592940188e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:05:49,214] Trial 165 finished with value: 0.6245315171173289 and parameters: {'batch_size': 26, 'learning_rate': 4.4571779247222654e-05, 'weight_decay': 0.0006347071929403343, 'beta_0': 0.8029511254779844, 'beta_1': 0.9888839438561197, 'epsilon': 4.5648802998786997e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:15:44,197] Trial 166 finished with value: 0.6213148915760134 and parameters: {'batch_size': 24, 'learning_rate': 5.534133323013876e-05, 'weight_decay': 0.00042577265426655336, 'beta_0': 0.8045459204706223, 'beta_1': 0.9921097832155594, 'epsilon': 1.7545493598579265e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:25:43,408] Trial 167 finished with value: 0.600070344742345 and parameters: {'batch_size': 23, 'learning_rate': 4.7669786663932604e-05, 'weight_decay': 0.0005365722565451055, 'beta_0': 0.8011343539297128, 'beta_1': 0.9893161097446783, 'epsilon': 2.3641059578007886e-07, 'balanced_loss': False, 'epochs': 8, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:37:08,594] Trial 168 finished with value: 0.6010376500164041 and parameters: {'batch_size': 18, 'learning_rate': 5.258961641607006e-05, 'weight_decay': 0.00017667632618708186, 'beta_0': 0.8000247004356142, 'beta_1': 0.9903938422914501, 'epsilon': 3.73881310381714e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:46:59,478] Trial 169 finished with value: 0.6512066682379547 and parameters: {'batch_size': 26, 'learning_rate': 5.791749892200395e-05, 'weight_decay': 0.0003458634476451751, 'beta_0': 0.8066621248687512, 'beta_1': 0.9881597563426756, 'epsilon': 9.311692420018065e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 15:56:46,249] Trial 170 finished with value: 0.610506181972404 and parameters: {'batch_size': 26, 'learning_rate': 5.817145611611633e-05, 'weight_decay': 0.00022548643499185994, 'beta_0': 0.8069995219761195, 'beta_1': 0.9879954728404599, 'epsilon': 5.966178899688989e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:06:34,205] Trial 171 finished with value: 0.6145419403335213 and parameters: {'batch_size': 25, 'learning_rate': 5.327743342515985e-05, 'weight_decay': 0.000331991356797277, 'beta_0': 0.8044915938853748, 'beta_1': 0.9875308809292649, 'epsilon': 7.560406002977478e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:16:28,421] Trial 172 finished with value: 0.6187929385176643 and parameters: {'batch_size': 26, 'learning_rate': 6.441789919626307e-05, 'weight_decay': 0.0003821032078407212, 'beta_0': 0.8087518373987986, 'beta_1': 0.9882732670819273, 'epsilon': 8.763684582484555e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:26:23,863] Trial 173 finished with value: 0.6188959851712809 and parameters: {'batch_size': 25, 'learning_rate': 4.9496149531081116e-05, 'weight_decay': 0.0005819317064094176, 'beta_0': 0.8059118362430158, 'beta_1': 0.986898111298821, 'epsilon': 1.0501839427340136e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:36:18,705] Trial 174 finished with value: 0.6293453686758576 and parameters: {'batch_size': 24, 'learning_rate': 5.64015826738099e-05, 'weight_decay': 0.0007478467936008244, 'beta_0': 0.8101108232700118, 'beta_1': 0.9887440585643291, 'epsilon': 1.267095289589639e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 16:51:07,067] Trial 175 finished with value: 0.6130981098581357 and parameters: {'batch_size': 26, 'learning_rate': 1.3152245461376384e-05, 'weight_decay': 0.0008140521028780604, 'beta_0': 0.8119812327093852, 'beta_1': 0.9887154692448155, 'epsilon': 1.7574941576160834e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:01:59,822] Trial 176 finished with value: 0.6077231426780102 and parameters: {'batch_size': 27, 'learning_rate': 5.7627372777698035e-05, 'weight_decay': 0.0007399813630937394, 'beta_0': 0.8104941082668151, 'beta_1': 0.9879156800107152, 'epsilon': 1.0490382679514754e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:11:50,105] Trial 177 finished with value: 0.6151338671795861 and parameters: {'batch_size': 25, 'learning_rate': 7.350856708291203e-05, 'weight_decay': 0.0009224992453419458, 'beta_0': 0.807463200800101, 'beta_1': 0.9889987838654387, 'epsilon': 2.0954672930414593e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:22:45,434] Trial 178 finished with value: 0.6277975617604497 and parameters: {'batch_size': 24, 'learning_rate': 6.70026752035098e-05, 'weight_decay': 0.000650785156122303, 'beta_0': 0.8497543555069557, 'beta_1': 0.9886461138030372, 'epsilon': 4.995409320073471e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:32:41,882] Trial 179 finished with value: 0.6339741748932884 and parameters: {'batch_size': 22, 'learning_rate': 6.569399549450996e-05, 'weight_decay': 0.0006464540931676302, 'beta_0': 0.8511571214801559, 'beta_1': 0.9883788578224579, 'epsilon': 4.789296487978451e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:43:55,678] Trial 180 finished with value: 0.624525785972858 and parameters: {'batch_size': 22, 'learning_rate': 8.032536311989975e-05, 'weight_decay': 0.0007190629645035393, 'beta_0': 0.8484428755795497, 'beta_1': 0.9872060506166122, 'epsilon': 5.466960855038141e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 17:55:02,805] Trial 181 finished with value: 0.5961668370762067 and parameters: {'batch_size': 23, 'learning_rate': 6.650187568465171e-05, 'weight_decay': 0.0006247021403311102, 'beta_0': 0.8512353809168839, 'beta_1': 0.9883498418736398, 'epsilon': 4.471210707550188e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:04:50,253] Trial 182 finished with value: 0.610849740884948 and parameters: {'batch_size': 24, 'learning_rate': 6.856317041390238e-05, 'weight_decay': 0.0004959949572813886, 'beta_0': 0.8473792727586397, 'beta_1': 0.988606439014282, 'epsilon': 4.0768371105018006e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:14:51,744] Trial 183 finished with value: 0.6149008776480773 and parameters: {'batch_size': 22, 'learning_rate': 5.594885212010176e-05, 'weight_decay': 0.0009854597485905907, 'beta_0': 0.8354431534717084, 'beta_1': 0.9878996270306654, 'epsilon': 5.338621510652565e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:26:02,302] Trial 184 finished with value: 0.6310947380062218 and parameters: {'batch_size': 23, 'learning_rate': 6.370407929091656e-05, 'weight_decay': 0.0006784533129006943, 'beta_0': 0.8536261699029487, 'beta_1': 0.989268835056991, 'epsilon': 8.978766505769895e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:35:56,228] Trial 185 finished with value: 0.6007850513292902 and parameters: {'batch_size': 20, 'learning_rate': 6.507997409345327e-05, 'weight_decay': 0.0008104523790265418, 'beta_0': 0.8553761546465096, 'beta_1': 0.9892713966964766, 'epsilon': 9.482625038183666e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:47:12,825] Trial 186 finished with value: 0.6127700254884869 and parameters: {'batch_size': 21, 'learning_rate': 7.193753278921657e-05, 'weight_decay': 0.0006696851403321859, 'beta_0': 0.8415073306067724, 'beta_1': 0.9888218156936229, 'epsilon': 6.183475179504524e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 18:58:12,150] Trial 187 finished with value: 0.6335596193372615 and parameters: {'batch_size': 24, 'learning_rate': 6.339651048775068e-05, 'weight_decay': 0.0007319696440786528, 'beta_0': 0.8502741081833541, 'beta_1': 0.9895234157759822, 'epsilon': 8.190044316649066e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:09:20,138] Trial 188 finished with value: 0.6170998801277764 and parameters: {'batch_size': 24, 'learning_rate': 7.634015722977526e-05, 'weight_decay': 0.000746744682888126, 'beta_0': 0.8468965538229332, 'beta_1': 0.9895733884639275, 'epsilon': 7.848649107239508e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:15:35,799] Trial 189 finished with value: 0.546665072912597 and parameters: {'batch_size': 14, 'learning_rate': 5.975546461847433e-05, 'weight_decay': 1.8277021877612074e-06, 'beta_0': 0.8521434892463834, 'beta_1': 0.990633937354014, 'epsilon': 1.381091238453976e-06, 'balanced_loss': False, 'epochs': 5, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:27:06,281] Trial 190 finished with value: 0.6005697608467531 and parameters: {'batch_size': 23, 'learning_rate': 6.312253576027162e-05, 'weight_decay': 0.0005442644542257431, 'beta_0': 0.8547626787434623, 'beta_1': 0.9916053541263866, 'epsilon': 9.164116617498146e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:38:03,103] Trial 191 finished with value: 0.6058952229637299 and parameters: {'batch_size': 24, 'learning_rate': 6.917442557747307e-05, 'weight_decay': 0.0008445251112888867, 'beta_0': 0.8459614723145737, 'beta_1': 0.9893300731920442, 'epsilon': 7.292977277687681e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:47:58,008] Trial 192 finished with value: 0.6494136346453316 and parameters: {'batch_size': 25, 'learning_rate': 6.591125936560769e-05, 'weight_decay': 0.0006257213451539072, 'beta_0': 0.8581147704270673, 'beta_1': 0.9898345763342287, 'epsilon': 4.6628856816607687e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 19:57:53,007] Trial 193 finished with value: 0.6136378249315783 and parameters: {'batch_size': 25, 'learning_rate': 6.309562196356183e-05, 'weight_decay': 0.0007344590178252227, 'beta_0': 0.857240783710531, 'beta_1': 0.9899335145134573, 'epsilon': 1.0862036134679725e-06, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:08:51,382] Trial 194 finished with value: 0.6157813602692018 and parameters: {'batch_size': 26, 'learning_rate': 5.5371969713544295e-05, 'weight_decay': 0.0005740906121931474, 'beta_0': 0.8532922464804367, 'beta_1': 0.9902488360981169, 'epsilon': 1.5754277974617354e-06, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:19:54,852] Trial 195 finished with value: 0.6221055079766274 and parameters: {'batch_size': 25, 'learning_rate': 5.335068668102127e-05, 'weight_decay': 0.0004652673753971756, 'beta_0': 0.8504507589329459, 'beta_1': 0.9911195233882998, 'epsilon': 6.806883791515875e-08, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:29:46,329] Trial 196 finished with value: 0.6118958884546533 and parameters: {'batch_size': 25, 'learning_rate': 7.089495879121091e-05, 'weight_decay': 0.0008889281556538647, 'beta_0': 0.8618970599454073, 'beta_1': 0.9897589585011115, 'epsilon': 6.240274960667973e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 4}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:40:23,049] Trial 197 finished with value: 0.6138120678160178 and parameters: {'batch_size': 23, 'learning_rate': 5.915858376429357e-05, 'weight_decay': 0.0006341475458954862, 'beta_0': 0.8572280846916371, 'beta_1': 0.9889857205413072, 'epsilon': 8.827738453345745e-08, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 20:52:36,622] Trial 198 finished with value: 0.6088835469729423 and parameters: {'batch_size': 24, 'learning_rate': 6.504736824131934e-05, 'weight_decay': 0.00014574487630755317, 'beta_0': 0.8587439525308438, 'beta_1': 0.9907421931556765, 'epsilon': 1.2900178206782016e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:02:23,617] Trial 199 finished with value: 0.6052206905981903 and parameters: {'batch_size': 26, 'learning_rate': 4.64348267804111e-05, 'weight_decay': 0.000507004765201658, 'beta_0': 0.803140152878941, 'beta_1': 0.9895282200756343, 'epsilon': 3.637474744309456e-07, 'balanced_loss': False, 'epochs': 13, 'early_stopping_patience': 4, 'plateau_patience': 5, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:12:18,304] Trial 200 finished with value: 0.6183758427437779 and parameters: {'batch_size': 25, 'learning_rate': 5.7862430846358417e-05, 'weight_decay': 0.0007144099700622017, 'beta_0': 0.8059378737133528, 'beta_1': 0.9903715777359314, 'epsilon': 5.2372433010618296e-08, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:23:24,920] Trial 201 finished with value: 0.583610855349453 and parameters: {'batch_size': 24, 'learning_rate': 6.718725663641982e-05, 'weight_decay': 0.0006456481789208626, 'beta_0': 0.8496433890297613, 'beta_1': 0.9887267342431731, 'epsilon': 5.065865288224465e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:33:14,137] Trial 202 finished with value: 0.6249419266063562 and parameters: {'batch_size': 24, 'learning_rate': 6.244265242079675e-05, 'weight_decay': 0.0005636914579335089, 'beta_0': 0.8494148642138954, 'beta_1': 0.9883175382279286, 'epsilon': 4.9456985898778e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:44:12,042] Trial 203 finished with value: 0.6128039589115124 and parameters: {'batch_size': 26, 'learning_rate': 6.7575721378119e-05, 'weight_decay': 0.0006370645024598955, 'beta_0': 0.8544320203158354, 'beta_1': 0.9890037622800605, 'epsilon': 7.401044908045945e-07, 'balanced_loss': False, 'epochs': 12, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 21:54:13,866] Trial 204 finished with value: 0.5956915175101034 and parameters: {'batch_size': 23, 'learning_rate': 7.384987443676264e-05, 'weight_decay': 0.0007858520622063561, 'beta_0': 0.8660153968055099, 'beta_1': 0.9899084544599747, 'epsilon': 4.2120335220624005e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 3}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:06:24,514] Trial 205 finished with value: 0.6404824723351339 and parameters: {'batch_size': 25, 'learning_rate': 6.369128372093498e-05, 'weight_decay': 0.0003312698371944992, 'beta_0': 0.8526179903670955, 'beta_1': 0.9892086800904946, 'epsilon': 6.089896216095664e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:17:05,970] Trial 206 finished with value: 0.6188223060129397 and parameters: {'batch_size': 25, 'learning_rate': 6.32677008062203e-05, 'weight_decay': 0.0002631869951536523, 'beta_0': 0.8040657333965825, 'beta_1': 0.9892827667951589, 'epsilon': 8.442231313622027e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:26:54,053] Trial 207 finished with value: 0.6326188130366058 and parameters: {'batch_size': 26, 'learning_rate': 5.185827968996606e-05, 'weight_decay': 0.0002987964960833151, 'beta_0': 0.8093020906858862, 'beta_1': 0.9896046430851819, 'epsilon': 5.898922773133327e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:37:54,008] Trial 208 finished with value: 0.6288049195558595 and parameters: {'batch_size': 27, 'learning_rate': 5.295296117622699e-05, 'weight_decay': 0.00033801415349292354, 'beta_0': 0.8528111571313683, 'beta_1': 0.9896225966940533, 'epsilon': 6.303894529129088e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:48:48,756] Trial 209 finished with value: 0.6136766316826734 and parameters: {'batch_size': 27, 'learning_rate': 5.463709471041841e-05, 'weight_decay': 0.0002804836985214606, 'beta_0': 0.8559494782121218, 'beta_1': 0.9895891472019841, 'epsilon': 5.96524505905022e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 22:59:44,232] Trial 210 finished with value: 0.620636716117059 and parameters: {'batch_size': 28, 'learning_rate': 4.787813216055819e-05, 'weight_decay': 0.00035467527229170377, 'beta_0': 0.8532503150146085, 'beta_1': 0.9891586841010238, 'epsilon': 7.013799301739443e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:10:46,283] Trial 211 finished with value: 0.6257671547900329 and parameters: {'batch_size': 27, 'learning_rate': 4.9951090464238995e-05, 'weight_decay': 0.00031592013601523653, 'beta_0': 0.8516980883920182, 'beta_1': 0.9899678446776157, 'epsilon': 1.08781181156549e-06, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:20:36,417] Trial 212 finished with value: 0.601125797911216 and parameters: {'batch_size': 26, 'learning_rate': 5.196484092884024e-05, 'weight_decay': 0.00035585555149341845, 'beta_0': 0.8091452041422965, 'beta_1': 0.9895100589019181, 'epsilon': 6.532486408920207e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:32:26,713] Trial 213 finished with value: 0.6263437560005476 and parameters: {'batch_size': 26, 'learning_rate': 5.137380981969705e-05, 'weight_decay': 0.00031071881577564433, 'beta_0': 0.8527412095449495, 'beta_1': 0.9902616551918205, 'epsilon': 5.886339561320062e-07, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:43:28,989] Trial 214 finished with value: 0.600718057447948 and parameters: {'batch_size': 27, 'learning_rate': 5.6589326933365454e-05, 'weight_decay': 0.00021832322821189502, 'beta_0': 0.8081030575790997, 'beta_1': 0.989132733516137, 'epsilon': 8.813924156052789e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-15 23:54:21,016] Trial 215 finished with value: 0.598347097978666 and parameters: {'batch_size': 26, 'learning_rate': 5.9690929628163646e-05, 'weight_decay': 0.00039272175273786646, 'beta_0': 0.863404283254769, 'beta_1': 0.989698405678575, 'epsilon': 1.1207566078362355e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 9}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:05:30,653] Trial 216 finished with value: 0.6246246291260519 and parameters: {'batch_size': 22, 'learning_rate': 5.3653837147026135e-05, 'weight_decay': 1.8684599751710737e-05, 'beta_0': 0.8693380947715076, 'beta_1': 0.9901564783260091, 'epsilon': 9.057825605630451e-08, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:15:28,838] Trial 217 finished with value: 0.5950840048154046 and parameters: {'batch_size': 25, 'learning_rate': 4.477400941077231e-05, 'weight_decay': 0.0002628001068080851, 'beta_0': 0.8061699305804128, 'beta_1': 0.9882160719227057, 'epsilon': 7.746299588378505e-08, 'balanced_loss': False, 'epochs': 9, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:26:32,028] Trial 218 finished with value: 0.6528733548106905 and parameters: {'batch_size': 27, 'learning_rate': 4.923656405108972e-05, 'weight_decay': 0.0004812714295639758, 'beta_0': 0.8449529552164731, 'beta_1': 0.9887378087756206, 'epsilon': 4.7645502244850486e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:36:22,527] Trial 219 finished with value: 0.6258018655577016 and parameters: {'batch_size': 27, 'learning_rate': 4.816636493540875e-05, 'weight_decay': 0.00048328714264372756, 'beta_0': 0.8446685415337573, 'beta_1': 0.9885297849969203, 'epsilon': 4.846103635077011e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:47:16,936] Trial 220 finished with value: 0.6196513497959852 and parameters: {'batch_size': 27, 'learning_rate': 6.0484604898110163e-05, 'weight_decay': 0.0005416720840813573, 'beta_0': 0.851156485295962, 'beta_1': 0.9875426044272687, 'epsilon': 7.865901337731489e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 00:58:16,793] Trial 221 finished with value: 0.6120084305736055 and parameters: {'batch_size': 26, 'learning_rate': 5.1470311847830586e-05, 'weight_decay': 0.00039949758185960673, 'beta_0': 0.8729849191937451, 'beta_1': 0.9891556974488033, 'epsilon': 5.779815997173182e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:08:14,508] Trial 222 finished with value: 0.6195952372013501 and parameters: {'batch_size': 25, 'learning_rate': 4.937908786900399e-05, 'weight_decay': 0.00034542780146018054, 'beta_0': 0.8101411280927948, 'beta_1': 0.9887720011776118, 'epsilon': 3.922313082595374e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:19:06,773] Trial 223 finished with value: 0.6324737519537712 and parameters: {'batch_size': 28, 'learning_rate': 5.5545649127038164e-05, 'weight_decay': 0.0004447926881365702, 'beta_0': 0.8590248733322183, 'beta_1': 0.988085797410109, 'epsilon': 4.410304286250364e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:29:55,828] Trial 224 finished with value: 0.6294234135553876 and parameters: {'batch_size': 28, 'learning_rate': 5.619944709535451e-05, 'weight_decay': 0.00045905880038623416, 'beta_0': 0.8587325399228289, 'beta_1': 0.9879942775820835, 'epsilon': 4.277666983866725e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:39:50,806] Trial 225 finished with value: 0.616602516205454 and parameters: {'batch_size': 29, 'learning_rate': 5.8128626198104075e-05, 'weight_decay': 0.00047321108479553986, 'beta_0': 0.8601205584573929, 'beta_1': 0.9881460879569908, 'epsilon': 4.3565208701703623e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 01:50:46,628] Trial 226 finished with value: 0.6109406334342358 and parameters: {'batch_size': 28, 'learning_rate': 5.5316685793552436e-05, 'weight_decay': 0.0005823606758904325, 'beta_0': 0.8621582302571059, 'beta_1': 0.9882350648873273, 'epsilon': 4.6582385085077203e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:01:42,695] Trial 227 finished with value: 0.6145560299047301 and parameters: {'batch_size': 28, 'learning_rate': 6.120093700879283e-05, 'weight_decay': 0.0009996935324839051, 'beta_0': 0.855153482391069, 'beta_1': 0.9878364431728813, 'epsilon': 3.1921086687826435e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:12:48,080] Trial 228 finished with value: 0.6287866922742634 and parameters: {'batch_size': 25, 'learning_rate': 5.679688287173687e-05, 'weight_decay': 0.0007216629148905125, 'beta_0': 0.8649424392918333, 'beta_1': 0.9875978152528169, 'epsilon': 3.56498367551338e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:23:54,301] Trial 229 finished with value: 0.6383153909344572 and parameters: {'batch_size': 24, 'learning_rate': 4.626728000620705e-05, 'weight_decay': 0.0004785541304193405, 'beta_0': 0.8586955762140125, 'beta_1': 0.988485521010973, 'epsilon': 1.4177596951282462e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:33:45,516] Trial 230 finished with value: 0.6267231073063376 and parameters: {'batch_size': 29, 'learning_rate': 4.5572201177685035e-05, 'weight_decay': 0.0004454482668768796, 'beta_0': 0.8580397444904367, 'beta_1': 0.9878865491684062, 'epsilon': 5.339536094511879e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:44:44,887] Trial 231 finished with value: 0.6062602468969214 and parameters: {'batch_size': 24, 'learning_rate': 4.2508861595421846e-05, 'weight_decay': 0.000517774707328881, 'beta_0': 0.8599840537765493, 'beta_1': 0.9885638290274755, 'epsilon': 1.38551607916769e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 02:54:33,372] Trial 232 finished with value: 0.6294682425191855 and parameters: {'batch_size': 24, 'learning_rate': 6.46580293436207e-05, 'weight_decay': 0.0004200363304174704, 'beta_0': 0.8569004562327215, 'beta_1': 0.988552770048287, 'epsilon': 1.1432371212750919e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:07:57,650] Trial 233 finished with value: 0.614796206388951 and parameters: {'batch_size': 28, 'learning_rate': 1.6763552546082275e-05, 'weight_decay': 0.00040851274123050713, 'beta_0': 0.8570546165286231, 'beta_1': 0.9880955113027676, 'epsilon': 1.004357881876455e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:17:51,605] Trial 234 finished with value: 0.6327738167481796 and parameters: {'batch_size': 25, 'learning_rate': 6.42137714295576e-05, 'weight_decay': 0.0004575798243410801, 'beta_0': 0.8604236317224704, 'beta_1': 0.9885815169532688, 'epsilon': 1.518838267827073e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:28:53,665] Trial 235 finished with value: 0.6431393482753098 and parameters: {'batch_size': 25, 'learning_rate': 6.537280448063243e-05, 'weight_decay': 0.000411400429998782, 'beta_0': 0.8617056483476296, 'beta_1': 0.9889551137378697, 'epsilon': 1.4626329541097607e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:39:55,724] Trial 236 finished with value: 0.6119120716580835 and parameters: {'batch_size': 25, 'learning_rate': 6.558130338611809e-05, 'weight_decay': 0.0005678558318131258, 'beta_0': 0.8614986662670217, 'beta_1': 0.9889070423531428, 'epsilon': 1.4920090416852608e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 03:50:54,648] Trial 237 finished with value: 0.6284315690556068 and parameters: {'batch_size': 25, 'learning_rate': 6.357354714791901e-05, 'weight_decay': 0.00039195083487162584, 'beta_0': 0.8630620431653736, 'beta_1': 0.9890822667935822, 'epsilon': 1.6963862905586278e-07, 'balanced_loss': True, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:01:59,116] Trial 238 finished with value: 0.6139340701321289 and parameters: {'batch_size': 26, 'learning_rate': 4.62087646732691e-05, 'weight_decay': 0.0004981892488870747, 'beta_0': 0.8605731059302683, 'beta_1': 0.989285433879991, 'epsilon': 1.3679953835962928e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:11:54,512] Trial 239 finished with value: 0.576267660807205 and parameters: {'batch_size': 25, 'learning_rate': 7.08015305588502e-05, 'weight_decay': 0.0005831040827069111, 'beta_0': 0.8584592915593809, 'beta_1': 0.9905136872769476, 'epsilon': 1.0906595598388785e-07, 'balanced_loss': False, 'epochs': 10, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:22:59,859] Trial 240 finished with value: 0.6353423038811313 and parameters: {'batch_size': 26, 'learning_rate': 6.256704094762059e-05, 'weight_decay': 0.00010983662937983667, 'beta_0': 0.8656528450160027, 'beta_1': 0.9883655013659618, 'epsilon': 1.9451104577965036e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:34:03,699] Trial 241 finished with value: 0.6387584327141245 and parameters: {'batch_size': 26, 'learning_rate': 6.16958455545469e-05, 'weight_decay': 0.0006495864940631932, 'beta_0': 0.8672645306811033, 'beta_1': 0.9883917514549995, 'epsilon': 2.076094474546603e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:45:07,922] Trial 242 finished with value: 0.6445359129122231 and parameters: {'batch_size': 26, 'learning_rate': 6.208291870100257e-05, 'weight_decay': 0.0001137254988575097, 'beta_0': 0.8643128608108771, 'beta_1': 0.9883979128990552, 'epsilon': 2.0590699098007446e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 04:54:58,409] Trial 243 finished with value: 0.6112312736656967 and parameters: {'batch_size': 26, 'learning_rate': 6.0736336895969044e-05, 'weight_decay': 0.00048011544228527436, 'beta_0': 0.8667361067897932, 'beta_1': 0.9883052811731396, 'epsilon': 2.1647528944032923e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:06:03,168] Trial 244 finished with value: 0.6266151105449274 and parameters: {'batch_size': 26, 'learning_rate': 6.67522003731971e-05, 'weight_decay': 0.00016911351100907436, 'beta_0': 0.8679568118780234, 'beta_1': 0.9883349585067003, 'epsilon': 1.9736178731025301e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:17:04,196] Trial 245 finished with value: 0.6346635616132902 and parameters: {'batch_size': 26, 'learning_rate': 6.21982704173535e-05, 'weight_decay': 9.0959160497104e-05, 'beta_0': 0.8647979832177255, 'beta_1': 0.9875701226669149, 'epsilon': 1.8767158464271885e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:26:54,385] Trial 246 finished with value: 0.6282399865920801 and parameters: {'batch_size': 26, 'learning_rate': 6.297121902649138e-05, 'weight_decay': 8.484656619907313e-05, 'beta_0': 0.8661541497598849, 'beta_1': 0.9885279268229277, 'epsilon': 1.6660715552178446e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:40:24,396] Trial 247 finished with value: 0.5595330736236263 and parameters: {'batch_size': 27, 'learning_rate': 1.0068005752251513e-05, 'weight_decay': 9.262143929821368e-05, 'beta_0': 0.864009806023508, 'beta_1': 0.9872406160204602, 'epsilon': 1.9127920860139047e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 05:50:17,156] Trial 248 finished with value: 0.615575327861643 and parameters: {'batch_size': 26, 'learning_rate': 6.059393869879605e-05, 'weight_decay': 0.00011117854162674169, 'beta_0': 0.8636491723335219, 'beta_1': 0.9876113651968881, 'epsilon': 2.184026886599216e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-01-16 06:00:06,342] Trial 249 finished with value: 0.6125852116986428 and parameters: {'batch_size': 27, 'learning_rate': 6.914492701373411e-05, 'weight_decay': 0.00010397280418106659, 'beta_0': 0.8701925812499492, 'beta_1': 0.9878912198159205, 'epsilon': 1.501404893269231e-07, 'balanced_loss': False, 'epochs': 11, 'early_stopping_patience': 4, 'plateau_patience': 2, 'plateau_divider': 2}. Best is trial 144 with value: 0.6609549168012957.

[TRIAL] 144 [VALIDATION PERFORMANCE] 0.6609549168012957 [TRAINING LOSS] 0.08511339334977998 [VALIDATION LOSS] 1.3203402592076197 

number                                 144
value                             0.660955
params_balanced_loss                 False
params_batch_size                       25
params_early_stopping_patience           4
params_epochs                            9
params_learning_rate              0.000051
params_plateau_divider                   2
params_plateau_patience                  2
params_weight_decay               0.000537
params_beta_0                      0.81114
params_beta_1                     0.988478
params_epsilon                         0.0
user_attrs_epoch                         7
user_attrs_training_loss          0.085113
user_attrs_validation_loss         1.32034
Name: 144, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.6168512847951594 Test: 0.633933120377742
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.6256181764272801 Test: 0.618330233644091
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.6145297543173606 Test: 0.6452194285926122
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.6236840499438866 Test: 0.6358199291120935
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.6056230614906079 Test: 0.6336375730589772
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.6609549168012957 Test: 0.6281984689917437
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.6171638519499687 Test: 0.6317360809603956
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.6221868960227264 Test: 0.6064544385828846
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.6057566296604475 Test: 0.6337865864938368
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.6098902783508836 Test: 0.632826362645811
Validation performance: 60.56 & 62.02  1.59 & 66.1
Testing performance: 60.65 & 63.0  1.06 & 64.52

[TRIAL] 218 [VALIDATION PERFORMANCE] 0.6528733548106905 [TRAINING LOSS] 0.04219265003688633 [VALIDATION LOSS] 1.3786129879951476 

number                                 218
value                             0.652873
params_balanced_loss                 False
params_batch_size                       27
params_early_stopping_patience           4
params_epochs                           11
params_learning_rate              0.000049
params_plateau_divider                   2
params_plateau_patience                  2
params_weight_decay               0.000481
params_beta_0                     0.844953
params_beta_1                     0.988738
params_epsilon                         0.0
user_attrs_epoch                         9
user_attrs_training_loss          0.042193
user_attrs_validation_loss        1.378613
Name: 218, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.6285854111965703 Test: 0.6381576496239557
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.625097323515299 Test: 0.6238153911711481
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.6273975394942461 Test: 0.6429653244774676
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.6172756438430389 Test: 0.6411021450799557
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
41 Val: 0.6049533640846714 Test: 0.6319270164421418
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
42 Val: 0.6528733548106905 Test: 0.6352137287834217
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
43 Val: 0.6082537014900133 Test: 0.6226906959990824
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
44 Val: 0.6008191200089061 Test: 0.6157722125710412
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
45 Val: 0.5930731613008283 Test: 0.6143241375087737
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
46 Val: 0.617578667726233 Test: 0.6097655002659222
Validation performance: 59.31 & 61.76  1.72 & 65.29
Testing performance: 60.98 & 62.76  1.19 & 64.3

[TRIAL] 169 [VALIDATION PERFORMANCE] 0.6512066682379547 [TRAINING LOSS] 0.09617702177582452 [VALIDATION LOSS] 1.3803988786844106 

number                                 169
value                             0.651207
params_balanced_loss                 False
params_batch_size                       26
params_early_stopping_patience           4
params_epochs                           11
params_learning_rate              0.000058
params_plateau_divider                   2
params_plateau_patience                  2
params_weight_decay               0.000346
params_beta_0                     0.806662
params_beta_1                      0.98816
params_epsilon                         0.0
user_attrs_epoch                         7
user_attrs_training_loss          0.096177
user_attrs_validation_loss        1.380399
Name: 169, dtype: object
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
37 Val: 0.6258856319949941 Test: 0.6332417123714217
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
38 Val: 0.6358629706291935 Test: 0.6221375401872657
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
39 Val: 0.6222072175586763 Test: 0.6374139407249025
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
40 Val: 0.6040399160944471 Test: 0.632205379472509
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
slurmstepd: error: *** JOB 14454008 ON gpu035 CANCELLED AT 2025-01-16T10:31:11 DUE TO TIME LIMIT ***
