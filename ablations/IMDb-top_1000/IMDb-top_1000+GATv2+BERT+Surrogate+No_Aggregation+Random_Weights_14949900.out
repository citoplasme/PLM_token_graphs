[I 2025-02-20 10:40:43,282] Using an existing study with name 'IMDb-top_1000-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-Random_Weights-0.0-0.0' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (1147 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-20 10:53:00,456] Trial 240 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9526906434332147, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.345544966957914, 'global_pooling': 'mean', 'learning_rate': 0.0003286951709129132, 'weight_decay': 8.493584590616983e-06, 'beta_0': 0.8776847319892023, 'beta_1': 0.9983424179243315, 'epsilon': 7.52543112238206e-06, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 14, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 71 with value: 0.9212121212121213.
[I 2025-02-20 11:04:54,263] Trial 241 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.946475770455868, 'batch_size': 19, 'attention_heads': 9, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30663619453410046, 'global_pooling': 'mean', 'learning_rate': 0.0005582590547057709, 'weight_decay': 1.0217174982110231e-05, 'beta_0': 0.8811910287884063, 'beta_1': 0.998609218814509, 'epsilon': 3.317450726332648e-06, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 10}. Best is trial 71 with value: 0.9212121212121213.
[I 2025-02-20 11:16:46,974] Trial 242 finished with value: 0.9272727272727272 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9448895898826953, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 38, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31750173307787577, 'global_pooling': 'mean', 'learning_rate': 0.00027920151369446373, 'weight_decay': 7.28566393783587e-06, 'beta_0': 0.8821702231159224, 'beta_1': 0.9984107107193174, 'epsilon': 4.813658675376246e-06, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 11:28:00,447] Trial 243 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9441899962381228, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32079678579240506, 'global_pooling': 'mean', 'learning_rate': 0.001593227460500878, 'weight_decay': 7.423444374001738e-06, 'beta_0': 0.8851525313529622, 'beta_1': 0.9974533678756511, 'epsilon': 4.398838008027175e-06, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 11:40:02,807] Trial 244 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9418065509690858, 'batch_size': 16, 'attention_heads': 10, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3144949083097021, 'global_pooling': 'mean', 'learning_rate': 0.00027371728348109036, 'weight_decay': 6.516721479351129e-06, 'beta_0': 0.8793665928262685, 'beta_1': 0.998004803144466, 'epsilon': 1.5614228071471595e-07, 'balanced_loss': True, 'epochs': 145, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 11:52:22,046] Trial 245 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9465290123586643, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 37, 'number_of_hidden_layers': 0, 'dropout_rate': 0.334956177633, 'global_pooling': 'mean', 'learning_rate': 0.00019729549634005283, 'weight_decay': 8.972312338124707e-06, 'beta_0': 0.8830110064595408, 'beta_1': 0.9989918394485786, 'epsilon': 6.230186115254593e-06, 'balanced_loss': True, 'epochs': 147, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 12:03:35,387] Trial 246 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9686778821470139, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3081686890854359, 'global_pooling': 'mean', 'learning_rate': 0.00023709682301207758, 'weight_decay': 7.976899584266231e-06, 'beta_0': 0.8885329070228745, 'beta_1': 0.9982613408240667, 'epsilon': 5.046483893631615e-06, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 12:16:05,749] Trial 247 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9434228760598512, 'batch_size': 18, 'attention_heads': 9, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3272756612937325, 'global_pooling': 'mean', 'learning_rate': 0.00032374431138568806, 'weight_decay': 1.0539375385179786e-05, 'beta_0': 0.8735117424763151, 'beta_1': 0.9976729930944325, 'epsilon': 3.85273051775908e-06, 'balanced_loss': True, 'epochs': 138, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 4}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 604.69 MiB is free. Including non-PyTorch memory, this process has 43.96 GiB memory in use. Of the allocated memory 38.11 GiB is allocated by PyTorch, and 4.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 12:31:28,008] Trial 248 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9409639480229782, 'batch_size': 20, 'attention_heads': 9, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32636401797741654, 'global_pooling': 'mean', 'learning_rate': 0.0003917930126723387, 'weight_decay': 9.617450115127662e-06, 'beta_0': 0.8746728665602691, 'beta_1': 0.9972131297668131, 'epsilon': 3.80538581796415e-06, 'balanced_loss': True, 'epochs': 138, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 12:43:56,421] Trial 249 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9394047439681157, 'batch_size': 19, 'attention_heads': 9, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3415105527106261, 'global_pooling': 'mean', 'learning_rate': 0.00032480823202640666, 'weight_decay': 1.0768198989707181e-05, 'beta_0': 0.8955687774698368, 'beta_1': 0.9976125828532034, 'epsilon': 2.885630060486688e-06, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 3}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 12:56:01,137] Trial 250 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9572338245005041, 'batch_size': 18, 'attention_heads': 9, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33031432927868054, 'global_pooling': 'mean', 'learning_rate': 0.00040176272056228, 'weight_decay': 5.246258409413441e-06, 'beta_0': 0.8729882575711237, 'beta_1': 0.996900321083508, 'epsilon': 4.15426960276279e-06, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 5}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 13:08:49,603] Trial 251 finished with value: 0.8727272727272727 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9429701358848802, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3234644362451979, 'global_pooling': 'max', 'learning_rate': 0.0001508018242249146, 'weight_decay': 6.396012619263964e-06, 'beta_0': 0.86921033857303, 'beta_1': 0.9977270371132057, 'epsilon': 5.846957680662587e-06, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 13:20:28,798] Trial 252 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9440031783654593, 'batch_size': 19, 'attention_heads': 9, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3360592463864197, 'global_pooling': 'mean', 'learning_rate': 0.01006190641752299, 'weight_decay': 8.919176908992823e-06, 'beta_0': 0.8769428061084263, 'beta_1': 0.9911447220553284, 'epsilon': 5.0330038993619674e-06, 'balanced_loss': True, 'epochs': 197, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 7}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 13:34:52,761] Trial 253 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9594708005340199, 'batch_size': 18, 'attention_heads': 9, 'hidden_dimension': 80, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31913919737495516, 'global_pooling': 'mean', 'learning_rate': 0.0004641973283075761, 'weight_decay': 7.23281771239695e-06, 'beta_0': 0.8923331803536294, 'beta_1': 0.9973151744986112, 'epsilon': 3.43780656308191e-06, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 20, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 13:46:32,589] Trial 254 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9623665319057534, 'batch_size': 17, 'attention_heads': 10, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35269614634200114, 'global_pooling': 'mean', 'learning_rate': 0.00031698750853086837, 'weight_decay': 2.077937976735749e-06, 'beta_0': 0.8867856517508398, 'beta_1': 0.9978946476924997, 'epsilon': 4.7363452027629137e-07, 'balanced_loss': True, 'epochs': 140, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 144.69 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 41.06 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 13:56:17,878] Trial 255 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9365976487183106, 'batch_size': 21, 'attention_heads': 10, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33126538026429775, 'global_pooling': 'mean', 'learning_rate': 0.00019006709482619413, 'weight_decay': 1.1206232281526903e-05, 'beta_0': 0.8789487195641621, 'beta_1': 0.996417470131876, 'epsilon': 6.979137844417643e-06, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.01 GiB. GPU 0 has a total capacity of 44.56 GiB of which 430.69 MiB is free. Including non-PyTorch memory, this process has 44.13 GiB memory in use. Of the allocated memory 40.01 GiB is allocated by PyTorch, and 2.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 14:07:57,542] Trial 256 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9414334609553697, 'batch_size': 19, 'attention_heads': 9, 'hidden_dimension': 90, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3147167320418406, 'global_pooling': 'mean', 'learning_rate': 0.00011559341055495977, 'weight_decay': 1.476624792687571e-06, 'beta_0': 0.8900411848770287, 'beta_1': 0.9985673850902038, 'epsilon': 4.527620064596682e-06, 'balanced_loss': True, 'epochs': 138, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 14:20:23,088] Trial 257 finished with value: 0.8545454545454545 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9547288327807499, 'batch_size': 16, 'attention_heads': 8, 'hidden_dimension': 54, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3248837873674202, 'global_pooling': 'sum', 'learning_rate': 0.0002713122566202065, 'weight_decay': 9.679868092841748e-06, 'beta_0': 0.8851031316829624, 'beta_1': 0.9969649799563787, 'epsilon': 2.853072664079495e-06, 'balanced_loss': True, 'epochs': 106, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 14:32:37,533] Trial 258 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9672673735616097, 'batch_size': 17, 'attention_heads': 10, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33953858059735226, 'global_pooling': 'mean', 'learning_rate': 0.0006312602383747955, 'weight_decay': 8.069344739050732e-06, 'beta_0': 0.8248680998544478, 'beta_1': 0.9980044060385098, 'epsilon': 6.726863690314919e-06, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 14:44:07,855] Trial 259 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9641676662177265, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 59, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31937696844672947, 'global_pooling': 'mean', 'learning_rate': 0.00031398925310745457, 'weight_decay': 6.0059108931059155e-06, 'beta_0': 0.8764220219009012, 'beta_1': 0.9975576351108301, 'epsilon': 5.986717304477571e-06, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 14:55:54,804] Trial 260 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9449554825629527, 'batch_size': 17, 'attention_heads': 10, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3117261421271078, 'global_pooling': 'mean', 'learning_rate': 0.00021748262030765565, 'weight_decay': 8.639964859928033e-06, 'beta_0': 0.8819940907968242, 'beta_1': 0.9986039059706951, 'epsilon': 3.978977003142593e-06, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 40.37 GiB is allocated by PyTorch, and 2.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 15:05:06,831] Trial 261 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9380894058314968, 'batch_size': 29, 'attention_heads': 8, 'hidden_dimension': 51, 'number_of_hidden_layers': 4, 'dropout_rate': 0.32921786524157676, 'global_pooling': 'mean', 'learning_rate': 0.0001664787045950091, 'weight_decay': 4.715625624364697e-06, 'beta_0': 0.8875495221198131, 'beta_1': 0.9972876941518551, 'epsilon': 1.061727697287243e-05, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 15:16:08,971] Trial 262 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9507364606307024, 'batch_size': 16, 'attention_heads': 9, 'hidden_dimension': 32, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30585068146278194, 'global_pooling': 'mean', 'learning_rate': 0.0003651838544645179, 'weight_decay': 7.0637458377997846e-06, 'beta_0': 0.870827759530777, 'beta_1': 0.9980360097213943, 'epsilon': 5.433372421999005e-06, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 15:28:03,221] Trial 263 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.940451409252807, 'batch_size': 20, 'attention_heads': 10, 'hidden_dimension': 45, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3246006636246709, 'global_pooling': 'mean', 'learning_rate': 0.00043930628768438227, 'weight_decay': 1.1633258142870226e-05, 'beta_0': 0.8736076514913902, 'beta_1': 0.9960707474951861, 'epsilon': 8.16250393788336e-06, 'balanced_loss': True, 'epochs': 131, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 5}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.09 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 42.82 GiB memory in use. Of the allocated memory 38.79 GiB is allocated by PyTorch, and 2.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 15:37:55,004] Trial 264 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9430798836157908, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31718241265710534, 'global_pooling': 'mean', 'learning_rate': 0.00022720676424669873, 'weight_decay': 9.983366276219621e-06, 'beta_0': 0.8839723791905887, 'beta_1': 0.9922288214085544, 'epsilon': 4.836759265597897e-06, 'balanced_loss': True, 'epochs': 156, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 15:51:50,098] Trial 265 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9616592986887024, 'batch_size': 19, 'attention_heads': 11, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3469728321325821, 'global_pooling': 'mean', 'learning_rate': 0.0001444280043773574, 'weight_decay': 8.06352993602812e-06, 'beta_0': 0.8802382728219617, 'beta_1': 0.9989985405453946, 'epsilon': 7.298328875705199e-06, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 16:02:19,734] Trial 266 finished with value: 0.8909090909090909 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9699858371961136, 'batch_size': 18, 'attention_heads': 9, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33530997479536245, 'global_pooling': 'mean', 'learning_rate': 0.0005003912479225579, 'weight_decay': 1.3089898767902593e-05, 'beta_0': 0.894138758886396, 'beta_1': 0.9966173213048068, 'epsilon': 1.6388877267007874e-05, 'balanced_loss': True, 'epochs': 144, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 16:15:07,114] Trial 267 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9567015151687147, 'batch_size': 16, 'attention_heads': 11, 'hidden_dimension': 78, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3088486555417026, 'global_pooling': 'mean', 'learning_rate': 0.00026108026208655804, 'weight_decay': 1.8693007461104476e-06, 'beta_0': 0.8166456350614111, 'beta_1': 0.9983043564765407, 'epsilon': 3.584473136780088e-06, 'balanced_loss': False, 'epochs': 134, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 4}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 16:26:12,079] Trial 268 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.96596423894216, 'batch_size': 30, 'attention_heads': 10, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31314200085477983, 'global_pooling': 'mean', 'learning_rate': 0.0027740279555670475, 'weight_decay': 9.16251444497546e-06, 'beta_0': 0.8904655337937434, 'beta_1': 0.9976338037554249, 'epsilon': 1.7504614760036243e-06, 'balanced_loss': True, 'epochs': 123, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 16:38:25,073] Trial 269 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.953519656026048, 'batch_size': 21, 'attention_heads': 9, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32178006574011825, 'global_pooling': 'max', 'learning_rate': 0.0001856955034101028, 'weight_decay': 5.784325944043493e-06, 'beta_0': 0.8779119637690382, 'beta_1': 0.996883184926535, 'epsilon': 8.61438989473973e-06, 'balanced_loss': True, 'epochs': 141, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 16:49:47,344] Trial 270 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9594606117569661, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3304578524213087, 'global_pooling': 'mean', 'learning_rate': 0.0003515749740027615, 'weight_decay': 6.9857427902822385e-06, 'beta_0': 0.8822278743621922, 'beta_1': 0.9954318872288241, 'epsilon': 5.737427540133189e-06, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 19, 'plateau_patience': 22, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 17:08:27,971] Trial 271 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.946336561606541, 'batch_size': 27, 'attention_heads': 8, 'hidden_dimension': 39, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34246955527052075, 'global_pooling': 'mean', 'learning_rate': 0.0002838190906484211, 'weight_decay': 1.0961706920737867e-05, 'beta_0': 0.8859632504377039, 'beta_1': 0.9978504726466049, 'epsilon': 6.972712298850672e-06, 'balanced_loss': True, 'epochs': 182, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 7}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 17:20:03,606] Trial 272 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9482090905471123, 'batch_size': 17, 'attention_heads': 6, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30521182763584753, 'global_pooling': 'mean', 'learning_rate': 0.000212734354074303, 'weight_decay': 7.989498997852547e-06, 'beta_0': 0.8753796064704725, 'beta_1': 0.9984759839254284, 'epsilon': 2.2056002035882094e-06, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 17:32:49,123] Trial 273 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9418680463240248, 'batch_size': 16, 'attention_heads': 9, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31683181054402443, 'global_pooling': 'mean', 'learning_rate': 0.00011917727226116647, 'weight_decay': 9.791905567023337e-06, 'beta_0': 0.8923260665145136, 'beta_1': 0.9972185795765053, 'epsilon': 9.891898820616558e-06, 'balanced_loss': True, 'epochs': 150, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.23 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 43.10 GiB memory in use. Of the allocated memory 38.70 GiB is allocated by PyTorch, and 3.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 17:42:33,582] Trial 274 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9395314411231275, 'batch_size': 19, 'attention_heads': 10, 'hidden_dimension': 70, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3254488117098072, 'global_pooling': 'mean', 'learning_rate': 0.00016052352862835797, 'weight_decay': 1.5137984386076739e-05, 'beta_0': 0.8892530347669956, 'beta_1': 0.9981084404468398, 'epsilon': 4.496111202296997e-06, 'balanced_loss': True, 'epochs': 128, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 17:53:49,785] Trial 275 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9444294919920461, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3586416110579487, 'global_pooling': 'mean', 'learning_rate': 0.0007463692581994834, 'weight_decay': 1.1619218136655849e-05, 'beta_0': 0.8972951825488686, 'beta_1': 0.9986844576566297, 'epsilon': 5.438154489822286e-06, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 18:04:45,559] Trial 276 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9637804448551314, 'batch_size': 17, 'attention_heads': 10, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3788756712891915, 'global_pooling': 'mean', 'learning_rate': 0.000553524388240243, 'weight_decay': 6.56761293872673e-06, 'beta_0': 0.810580546308805, 'beta_1': 0.997533434173138, 'epsilon': 7.620249880697555e-06, 'balanced_loss': True, 'epochs': 132, 'early_stopping_patience': 10, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 4.16 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 43.31 GiB memory in use. Of the allocated memory 40.05 GiB is allocated by PyTorch, and 2.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 18:13:31,545] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.913664803275008, 'batch_size': 36, 'attention_heads': 10, 'hidden_dimension': 56, 'number_of_hidden_layers': 0, 'dropout_rate': 0.31117690512584956, 'global_pooling': 'mean', 'learning_rate': 0.0004300336378455238, 'weight_decay': 8.7185035171748e-06, 'beta_0': 0.8808456271621351, 'beta_1': 0.992775580797295, 'epsilon': 6.400463433054418e-06, 'balanced_loss': True, 'epochs': 146, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 18:26:15,779] Trial 278 finished with value: 0.896969696969697 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9684284696487222, 'batch_size': 19, 'attention_heads': 14, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3363254027731813, 'global_pooling': 'mean', 'learning_rate': 0.0003171360251611327, 'weight_decay': 1.0216120835596654e-06, 'beta_0': 0.884003837662253, 'beta_1': 0.9891339435604477, 'epsilon': 3.376021668659692e-05, 'balanced_loss': True, 'epochs': 190, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 18:38:53,461] Trial 279 finished with value: 0.8666666666666667 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9514149532312552, 'batch_size': 20, 'attention_heads': 9, 'hidden_dimension': 48, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3193651487241384, 'global_pooling': 'sum', 'learning_rate': 0.00024346773637676288, 'weight_decay': 7.2611683263638215e-06, 'beta_0': 0.8785298003435951, 'beta_1': 0.9970421872657105, 'epsilon': 1.0744753372413094e-05, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 18:52:30,066] Trial 280 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9576941029850341, 'batch_size': 16, 'attention_heads': 10, 'hidden_dimension': 75, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30815543502798226, 'global_pooling': 'mean', 'learning_rate': 0.00036245675405823847, 'weight_decay': 1.0161875598821355e-05, 'beta_0': 0.8879443725606104, 'beta_1': 0.9982230105621804, 'epsilon': 2.571108925749037e-06, 'balanced_loss': True, 'epochs': 171, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 19:05:13,224] Trial 281 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9614318322805053, 'batch_size': 18, 'attention_heads': 7, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32716107004919476, 'global_pooling': 'mean', 'learning_rate': 0.00013494346054268488, 'weight_decay': 4.9672343145026414e-06, 'beta_0': 0.8858634613403387, 'beta_1': 0.9976862662724689, 'epsilon': 3.607369868905039e-06, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.24 GiB is free. Including non-PyTorch memory, this process has 43.32 GiB memory in use. Of the allocated memory 38.00 GiB is allocated by PyTorch, and 4.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 19:16:50,101] Trial 282 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9491724739716733, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3147076449167304, 'global_pooling': 'mean', 'learning_rate': 0.00018275631722002616, 'weight_decay': 1.400644462138488e-05, 'beta_0': 0.882392943854584, 'beta_1': 0.9966668495405828, 'epsilon': 4.58158250178778e-06, 'balanced_loss': True, 'epochs': 142, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.98 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 43.32 GiB memory in use. Of the allocated memory 40.51 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 19:26:29,925] Trial 283 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9374173283680259, 'batch_size': 18, 'attention_heads': 11, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.30613265422749814, 'global_pooling': 'mean', 'learning_rate': 0.00027856309879849603, 'weight_decay': 1.2025095721509054e-05, 'beta_0': 0.8767427125515578, 'beta_1': 0.9987487857469608, 'epsilon': 8.67167542529134e-06, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 5}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 19:41:03,860] Trial 284 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9553983573079673, 'batch_size': 16, 'attention_heads': 10, 'hidden_dimension': 81, 'number_of_hidden_layers': 0, 'dropout_rate': 0.332110116608148, 'global_pooling': 'mean', 'learning_rate': 0.0001053014780537945, 'weight_decay': 8.083667103510208e-06, 'beta_0': 0.879731102717844, 'beta_1': 0.9973184960739888, 'epsilon': 1.3391638055992162e-05, 'balanced_loss': True, 'epochs': 133, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 19:56:41,616] Trial 285 finished with value: 0.8848484848484849 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9547273697878883, 'batch_size': 16, 'attention_heads': 13, 'hidden_dimension': 88, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33394801955492387, 'global_pooling': 'mean', 'learning_rate': 0.00010080356550280446, 'weight_decay': 1.701799997745872e-05, 'beta_0': 0.8726748094647481, 'beta_1': 0.9905840386188667, 'epsilon': 1.1507382025447289e-05, 'balanced_loss': True, 'epochs': 133, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 20:11:48,528] Trial 286 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9561806011750129, 'batch_size': 16, 'attention_heads': 9, 'hidden_dimension': 95, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34042938829768526, 'global_pooling': 'mean', 'learning_rate': 0.00013013937916912097, 'weight_decay': 1.6369726789103293e-06, 'beta_0': 0.8786983728273593, 'beta_1': 0.9978434388345891, 'epsilon': 1.3040574539254848e-05, 'balanced_loss': True, 'epochs': 177, 'early_stopping_patience': 25, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 20:26:31,880] Trial 287 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9594986252376889, 'batch_size': 17, 'attention_heads': 10, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3675194326994908, 'global_pooling': 'mean', 'learning_rate': 9.697565213486649e-05, 'weight_decay': 5.713261375135983e-06, 'beta_0': 0.8918480808417494, 'beta_1': 0.9973431130614075, 'epsilon': 1.650857327634552e-05, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 20:38:33,644] Trial 288 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9717102018078398, 'batch_size': 16, 'attention_heads': 9, 'hidden_dimension': 72, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3308998759461301, 'global_pooling': 'mean', 'learning_rate': 0.00016224269284766262, 'weight_decay': 7.8471870795378e-06, 'beta_0': 0.8750119982918251, 'beta_1': 0.9856196818608283, 'epsilon': 1.3310308251739334e-05, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 20:50:46,706] Trial 289 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9568363280425741, 'batch_size': 17, 'attention_heads': 8, 'hidden_dimension': 79, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34941546004723223, 'global_pooling': 'mean', 'learning_rate': 0.00021339109204720358, 'weight_decay': 9.141408313254214e-06, 'beta_0': 0.880096367143393, 'beta_1': 0.9983518545286266, 'epsilon': 9.450121253384802e-06, 'balanced_loss': True, 'epochs': 126, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 21:03:46,777] Trial 290 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9533414360937744, 'batch_size': 16, 'attention_heads': 10, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3901381168761294, 'global_pooling': 'mean', 'learning_rate': 0.00025692980692583814, 'weight_decay': 1.0306457552315852e-05, 'beta_0': 0.8893784587950286, 'beta_1': 0.9978543095170058, 'epsilon': 7.429579216247308e-06, 'balanced_loss': True, 'epochs': 133, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.15 GiB. GPU 0 has a total capacity of 44.56 GiB of which 324.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 40.18 GiB is allocated by PyTorch, and 2.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 21:13:25,396] Trial 291 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9402554384415069, 'batch_size': 18, 'attention_heads': 11, 'hidden_dimension': 66, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32345484731837737, 'global_pooling': 'mean', 'learning_rate': 1.0942290680543605e-05, 'weight_decay': 6.446050658314483e-06, 'beta_0': 0.8948004953487187, 'beta_1': 0.9982041489687563, 'epsilon': 9.814161786896866e-06, 'balanced_loss': True, 'epochs': 121, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 21:26:09,023] Trial 292 finished with value: 0.8666666666666667 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9426555720635164, 'batch_size': 18, 'attention_heads': 10, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3315634144897859, 'global_pooling': 'max', 'learning_rate': 0.00012229712310882272, 'weight_decay': 7.606526119533153e-06, 'beta_0': 0.8996226227176392, 'beta_1': 0.9973636790762784, 'epsilon': 6.5113298671846005e-06, 'balanced_loss': True, 'epochs': 130, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 21:43:56,569] Trial 293 finished with value: 0.9030303030303031 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9648106705023395, 'batch_size': 26, 'attention_heads': 9, 'hidden_dimension': 52, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3380103615084347, 'global_pooling': 'mean', 'learning_rate': 0.00018627088812223642, 'weight_decay': 1.2947241154541677e-05, 'beta_0': 0.8763975745105589, 'beta_1': 0.9989678447677978, 'epsilon': 1.4247522991635421e-06, 'balanced_loss': True, 'epochs': 136, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 21:56:28,845] Trial 294 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9622249660882898, 'batch_size': 16, 'attention_heads': 10, 'hidden_dimension': 88, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32265907757021944, 'global_pooling': 'mean', 'learning_rate': 0.0003338361387117077, 'weight_decay': 9.009068809126485e-06, 'beta_0': 0.838439841029056, 'beta_1': 0.9985451594013134, 'epsilon': 2.0217809984308283e-05, 'balanced_loss': True, 'epochs': 113, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.32 GiB is free. Including non-PyTorch memory, this process has 42.23 GiB memory in use. Of the allocated memory 34.96 GiB is allocated by PyTorch, and 6.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 22:06:17,422] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9578939457066474, 'batch_size': 56, 'attention_heads': 8, 'hidden_dimension': 46, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3455746282551503, 'global_pooling': 'mean', 'learning_rate': 0.00022150289942231407, 'weight_decay': 4.362520323498244e-06, 'beta_0': 0.8853239328526852, 'beta_1': 0.9949131280996824, 'epsilon': 5.030479765762245e-06, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 22:21:01,018] Trial 296 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9665180805346607, 'batch_size': 45, 'attention_heads': 10, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3190370965258555, 'global_pooling': 'mean', 'learning_rate': 0.0001450742028827102, 'weight_decay': 1.0933008620631261e-05, 'beta_0': 0.8796555661369619, 'beta_1': 0.997516495834486, 'epsilon': 7.95158585660771e-06, 'balanced_loss': True, 'epochs': 97, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 22:33:42,770] Trial 297 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.967196440646464, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3262809871130529, 'global_pooling': 'mean', 'learning_rate': 0.00011427766589401414, 'weight_decay': 5.9732339695299494e-05, 'beta_0': 0.8810819510634763, 'beta_1': 0.9979121550553173, 'epsilon': 8.497931694027789e-06, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 22:46:41,703] Trial 298 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9672302000398174, 'batch_size': 51, 'attention_heads': 6, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.46757235287104193, 'global_pooling': 'mean', 'learning_rate': 0.00011089157996083788, 'weight_decay': 7.437925060411432e-05, 'beta_0': 0.8813009271094614, 'beta_1': 0.998075342949972, 'epsilon': 1.0929126713113397e-05, 'balanced_loss': True, 'epochs': 95, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 23:00:42,921] Trial 299 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9703392315713483, 'batch_size': 46, 'attention_heads': 9, 'hidden_dimension': 61, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3263197605723228, 'global_pooling': 'mean', 'learning_rate': 0.00014820830511248577, 'weight_decay': 1.1252482846587898e-05, 'beta_0': 0.8837007907597512, 'beta_1': 0.9978310443994333, 'epsilon': 8.447243459965587e-06, 'balanced_loss': True, 'epochs': 98, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.32 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.94 GiB is free. Including non-PyTorch memory, this process has 41.61 GiB memory in use. Of the allocated memory 36.50 GiB is allocated by PyTorch, and 3.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 23:10:28,718] Trial 300 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9653574245815533, 'batch_size': 49, 'attention_heads': 9, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33164313298709364, 'global_pooling': 'mean', 'learning_rate': 0.00011157579496176913, 'weight_decay': 5.579301061819919e-05, 'beta_0': 0.887488633164193, 'beta_1': 0.9873358993239917, 'epsilon': 1.2305183325768664e-05, 'balanced_loss': True, 'epochs': 93, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 422.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 388.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 41.32 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-20 23:19:03,488] Trial 301 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9010554075031962, 'batch_size': 28, 'attention_heads': 9, 'hidden_dimension': 57, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3395636967241584, 'global_pooling': 'mean', 'learning_rate': 9.952789909024382e-05, 'weight_decay': 1.4636234455960963e-05, 'beta_0': 0.8805260209137599, 'beta_1': 0.998648981019298, 'epsilon': 8.435444209977156e-06, 'balanced_loss': True, 'epochs': 89, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 23:31:37,910] Trial 302 finished with value: 0.806060606060606 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9683295000883281, 'batch_size': 48, 'attention_heads': 9, 'hidden_dimension': 63, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5975021424468195, 'global_pooling': 'sum', 'learning_rate': 7.929399463238076e-05, 'weight_decay': 2.9063486960091916e-05, 'beta_0': 0.8838293963679111, 'beta_1': 0.9980692908519706, 'epsilon': 8.105226748689347e-06, 'balanced_loss': True, 'epochs': 100, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 23:43:34,731] Trial 303 finished with value: 0.8787878787878788 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9726748515980861, 'batch_size': 17, 'attention_heads': 9, 'hidden_dimension': 58, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3266051584525113, 'global_pooling': 'mean', 'learning_rate': 0.00018820835995354653, 'weight_decay': 5.004007234264942e-05, 'beta_0': 0.893115305079069, 'beta_1': 0.9976370026768705, 'epsilon': 1.6118161244647558e-05, 'balanced_loss': True, 'epochs': 86, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-20 23:54:32,906] Trial 304 finished with value: 0.8787878787878788 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9660018677953506, 'batch_size': 20, 'attention_heads': 7, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.319342616213153, 'global_pooling': 'mean', 'learning_rate': 0.00013075349675167452, 'weight_decay': 5.842214654113788e-05, 'beta_0': 0.8794010177183101, 'beta_1': 0.9983673981501981, 'epsilon': 1.0649711342211544e-05, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 00:10:53,141] Trial 305 finished with value: 0.9030303030303031 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9637973073226243, 'batch_size': 17, 'attention_heads': 16, 'hidden_dimension': 68, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3338145412675655, 'global_pooling': 'mean', 'learning_rate': 0.00015244318054832243, 'weight_decay': 1.2457673279588475e-05, 'beta_0': 0.8861811246215191, 'beta_1': 0.9973974179755735, 'epsilon': 4.219559790712674e-05, 'balanced_loss': True, 'epochs': 100, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 00:24:46,326] Trial 306 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9688996039081286, 'batch_size': 42, 'attention_heads': 12, 'hidden_dimension': 64, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3209075864517756, 'global_pooling': 'mean', 'learning_rate': 0.00015988863939934736, 'weight_decay': 3.670276041282327e-05, 'beta_0': 0.8901468380819407, 'beta_1': 0.9986189339351887, 'epsilon': 6.0812100857093e-06, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 00:36:58,635] Trial 307 finished with value: 0.8787878787878788 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9665287064798256, 'batch_size': 44, 'attention_heads': 8, 'hidden_dimension': 55, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3285323547498152, 'global_pooling': 'mean', 'learning_rate': 0.04440540239698851, 'weight_decay': 0.00015530861559519287, 'beta_0': 0.8831047724012079, 'beta_1': 0.9979769477399357, 'epsilon': 7.0355544922827384e-06, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 41.51 GiB memory in use. Of the allocated memory 38.76 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 00:46:38,976] Trial 308 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9548015865715522, 'batch_size': 39, 'attention_heads': 9, 'hidden_dimension': 84, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3174485035276227, 'global_pooling': 'mean', 'learning_rate': 8.390772074020766e-05, 'weight_decay': 1.2032662253550745e-06, 'beta_0': 0.8816773850834846, 'beta_1': 0.9971189011778223, 'epsilon': 9.702213330958004e-06, 'balanced_loss': True, 'epochs': 90, 'early_stopping_patience': 18, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 01:03:50,416] Trial 309 finished with value: 0.8909090909090909 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9606859885403912, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5254546844713344, 'global_pooling': 'mean', 'learning_rate': 0.00013186813252289825, 'weight_decay': 2.1022164461286445e-06, 'beta_0': 0.89601992233334, 'beta_1': 0.9983090481976938, 'epsilon': 2.6840810731870215e-05, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.59 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.21 GiB is free. Including non-PyTorch memory, this process has 42.35 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 2.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 01:13:33,268] Trial 310 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9626184657013382, 'batch_size': 45, 'attention_heads': 10, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3555797574477239, 'global_pooling': 'mean', 'learning_rate': 0.0001811897478527009, 'weight_decay': 2.4515832858962737e-06, 'beta_0': 0.8881579964874154, 'beta_1': 0.9976449190108603, 'epsilon': 1.4318260939568307e-05, 'balanced_loss': True, 'epochs': 107, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 01:26:58,241] Trial 311 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9594167401767704, 'batch_size': 19, 'attention_heads': 11, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3427062861105513, 'global_pooling': 'mean', 'learning_rate': 0.00010982973968460618, 'weight_decay': 9.095981480575658e-05, 'beta_0': 0.8849973143887065, 'beta_1': 0.9987356662744374, 'epsilon': 5.525377286360108e-06, 'balanced_loss': True, 'epochs': 129, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 01:40:35,236] Trial 312 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9591355005003039, 'batch_size': 20, 'attention_heads': 12, 'hidden_dimension': 50, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3430197340641155, 'global_pooling': 'mean', 'learning_rate': 0.0001204571753116931, 'weight_decay': 0.00015023062805696947, 'beta_0': 0.8852414486650567, 'beta_1': 0.9989515830200927, 'epsilon': 5.718695664383382e-06, 'balanced_loss': True, 'epochs': 127, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 01:51:08,990] Trial 313 finished with value: 0.8909090909090909 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9605091559266212, 'batch_size': 19, 'attention_heads': 11, 'hidden_dimension': 53, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3483972888434262, 'global_pooling': 'mean', 'learning_rate': 0.004462726215650464, 'weight_decay': 0.00012194710909383838, 'beta_0': 0.891426939382013, 'beta_1': 0.9989863956069643, 'epsilon': 7.483534067256033e-06, 'balanced_loss': True, 'epochs': 129, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 02:06:47,340] Trial 314 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9576784232287985, 'batch_size': 22, 'attention_heads': 11, 'hidden_dimension': 91, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3380902655914729, 'global_pooling': 'mean', 'learning_rate': 0.0001047763554153727, 'weight_decay': 6.491947309257411e-05, 'beta_0': 0.8876016121426407, 'beta_1': 0.9985724075805689, 'epsilon': 6.62593531721957e-06, 'balanced_loss': True, 'epochs': 125, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.24 GiB is free. Including non-PyTorch memory, this process has 42.31 GiB memory in use. Of the allocated memory 38.57 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 02:16:25,078] Trial 315 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9556998705717101, 'batch_size': 23, 'attention_heads': 11, 'hidden_dimension': 93, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33782352617019706, 'global_pooling': 'mean', 'learning_rate': 9.337559339734779e-05, 'weight_decay': 9.578088926313777e-05, 'beta_0': 0.8875996122493028, 'beta_1': 0.9986062359441077, 'epsilon': 5.254980781381908e-06, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.77 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 42.73 GiB memory in use. Of the allocated memory 35.82 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 02:31:33,896] Trial 316 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9586028239370175, 'batch_size': 20, 'attention_heads': 11, 'hidden_dimension': 102, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34293782601101924, 'global_pooling': 'max', 'learning_rate': 6.94180048340075e-05, 'weight_decay': 9.499174101250185e-05, 'beta_0': 0.8857757926371255, 'beta_1': 0.9984582162636663, 'epsilon': 9.213725942880512e-06, 'balanced_loss': True, 'epochs': 126, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 02:46:51,438] Trial 317 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9621337591247682, 'batch_size': 22, 'attention_heads': 9, 'hidden_dimension': 92, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35389801346792443, 'global_pooling': 'mean', 'learning_rate': 9.611663023888215e-05, 'weight_decay': 8.618561024982123e-05, 'beta_0': 0.8896313741223418, 'beta_1': 0.9986376443679164, 'epsilon': 6.548094775829347e-06, 'balanced_loss': True, 'epochs': 118, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 03:03:59,024] Trial 318 finished with value: 0.9212121212121213 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9620644545469546, 'batch_size': 24, 'attention_heads': 12, 'hidden_dimension': 87, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3492070418618419, 'global_pooling': 'mean', 'learning_rate': 9.483403462436623e-05, 'weight_decay': 6.6124261876935e-05, 'beta_0': 0.8892213537288136, 'beta_1': 0.9986744275494364, 'epsilon': 8.237864004266655e-06, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 03:21:36,875] Trial 319 finished with value: 0.9030303030303031 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9632489055286888, 'batch_size': 22, 'attention_heads': 12, 'hidden_dimension': 95, 'number_of_hidden_layers': 0, 'dropout_rate': 0.36087096558010245, 'global_pooling': 'mean', 'learning_rate': 8.854771635198423e-05, 'weight_decay': 8.302068602443317e-05, 'beta_0': 0.8901284930740829, 'beta_1': 0.9985997320405319, 'epsilon': 1.1399766776257011e-05, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.16 GiB is free. Including non-PyTorch memory, this process has 41.39 GiB memory in use. Of the allocated memory 36.47 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 03:30:58,237] Trial 320 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9607512282634265, 'batch_size': 24, 'attention_heads': 12, 'hidden_dimension': 99, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3541914640925435, 'global_pooling': 'mean', 'learning_rate': 0.00010462006839558883, 'weight_decay': 6.20566512670515e-05, 'beta_0': 0.88868537627122, 'beta_1': 0.9982409211746951, 'epsilon': 1.975476744741773e-05, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 03:46:53,166] Trial 321 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9622417644966521, 'batch_size': 22, 'attention_heads': 8, 'hidden_dimension': 87, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3505714179591849, 'global_pooling': 'mean', 'learning_rate': 6.644363488734375e-05, 'weight_decay': 8.000827372016528e-05, 'beta_0': 0.8929240464336271, 'beta_1': 0.9985137366880901, 'epsilon': 8.422733095882173e-06, 'balanced_loss': True, 'epochs': 111, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 04:02:05,617] Trial 322 finished with value: 0.8787878787878788 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9642566191147289, 'batch_size': 23, 'attention_heads': 12, 'hidden_dimension': 90, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3501623800007123, 'global_pooling': 'mean', 'learning_rate': 8.399521213882824e-05, 'weight_decay': 0.00011063422859409554, 'beta_0': 0.891564441200015, 'beta_1': 0.9986723840396212, 'epsilon': 6.628453704305208e-06, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.47 GiB is free. Including non-PyTorch memory, this process has 42.09 GiB memory in use. Of the allocated memory 37.75 GiB is allocated by PyTorch, and 3.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 04:11:42,850] Trial 323 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9597071889366802, 'batch_size': 26, 'attention_heads': 12, 'hidden_dimension': 91, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34350706619598104, 'global_pooling': 'mean', 'learning_rate': 0.00010262107798959759, 'weight_decay': 6.191888079272241e-05, 'beta_0': 0.8890543246436436, 'beta_1': 0.9982959020258027, 'epsilon': 9.36120471544314e-06, 'balanced_loss': True, 'epochs': 121, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 04:28:29,120] Trial 324 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9646108470367938, 'batch_size': 25, 'attention_heads': 13, 'hidden_dimension': 85, 'number_of_hidden_layers': 0, 'dropout_rate': 0.36542614787029776, 'global_pooling': 'mean', 'learning_rate': 0.00011182579301692836, 'weight_decay': 9.003134165427542e-05, 'beta_0': 0.8875895948789017, 'beta_1': 0.99801622045623, 'epsilon': 8.0257374503814e-06, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.97 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.46 GiB is free. Including non-PyTorch memory, this process has 42.09 GiB memory in use. Of the allocated memory 37.43 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 04:38:09,329] Trial 325 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9574475574706134, 'batch_size': 24, 'attention_heads': 12, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3550121836111902, 'global_pooling': 'mean', 'learning_rate': 7.97393742363321e-05, 'weight_decay': 4.647686915492676e-05, 'beta_0': 0.8898803118182413, 'beta_1': 0.9989727296073345, 'epsilon': 1.2665438815189568e-05, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 42.89 GiB memory in use. Of the allocated memory 39.94 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 04:47:46,417] Trial 326 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9618278719079495, 'batch_size': 25, 'attention_heads': 12, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3377532966313762, 'global_pooling': 'sum', 'learning_rate': 0.0001351782925403142, 'weight_decay': 6.40701288945656e-05, 'beta_0': 0.894071950237886, 'beta_1': 0.9983060362884237, 'epsilon': 5.48006757428179e-05, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.81 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 43.09 GiB memory in use. Of the allocated memory 40.77 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 04:57:24,146] Trial 327 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9673910195499185, 'batch_size': 21, 'attention_heads': 11, 'hidden_dimension': 86, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3469498467325864, 'global_pooling': 'mean', 'learning_rate': 5.389019091398263e-05, 'weight_decay': 9.537343050732798e-05, 'beta_0': 0.8862288598768239, 'beta_1': 0.997795278514578, 'epsilon': 5.932009229895175e-06, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.92 GiB. GPU 0 has a total capacity of 44.56 GiB of which 370.69 MiB is free. Including non-PyTorch memory, this process has 44.19 GiB memory in use. Of the allocated memory 36.43 GiB is allocated by PyTorch, and 6.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 05:07:32,938] Trial 328 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9590663901371654, 'batch_size': 22, 'attention_heads': 12, 'hidden_dimension': 93, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3361497209870298, 'global_pooling': 'mean', 'learning_rate': 9.832175465912181e-05, 'weight_decay': 0.00011041775289068171, 'beta_0': 0.8918727139398118, 'beta_1': 0.998989407705668, 'epsilon': 4.388417553894139e-06, 'balanced_loss': True, 'epochs': 57, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.46 GiB is free. Including non-PyTorch memory, this process has 42.09 GiB memory in use. Of the allocated memory 39.30 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 05:16:29,940] Trial 329 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9656812721117889, 'batch_size': 23, 'attention_heads': 13, 'hidden_dimension': 169, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3472264381925335, 'global_pooling': 'mean', 'learning_rate': 0.00014227602347046384, 'weight_decay': 8.083407869088141e-05, 'beta_0': 0.8872908510147105, 'beta_1': 0.998142813148068, 'epsilon': 7.366201199979493e-06, 'balanced_loss': True, 'epochs': 114, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 05:32:24,408] Trial 330 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9623867332925248, 'batch_size': 21, 'attention_heads': 9, 'hidden_dimension': 89, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3393594391870786, 'global_pooling': 'mean', 'learning_rate': 0.00011300462891124794, 'weight_decay': 6.845877230276128e-05, 'beta_0': 0.8903509920136826, 'beta_1': 0.998609418932483, 'epsilon': 1.0338048476009006e-05, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 05:47:31,926] Trial 331 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9604801947839244, 'batch_size': 25, 'attention_heads': 11, 'hidden_dimension': 80, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3325792406196826, 'global_pooling': 'mean', 'learning_rate': 0.00015336837673769616, 'weight_decay': 9.378777896008693e-05, 'beta_0': 0.8965989803705392, 'beta_1': 0.9932790633063655, 'epsilon': 6.236382611019949e-06, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 05:59:43,095] Trial 332 finished with value: 0.8848484848484849 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9686057255156031, 'batch_size': 21, 'attention_heads': 9, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3570676515925351, 'global_pooling': 'mean', 'learning_rate': 0.0001247579802890458, 'weight_decay': 4.656986679510216e-05, 'beta_0': 0.8846780824145869, 'beta_1': 0.9975469753994254, 'epsilon': 1.4808263811657206e-05, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 06:15:21,924] Trial 333 finished with value: 0.896969696969697 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9572966842591294, 'batch_size': 23, 'attention_heads': 8, 'hidden_dimension': 78, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34275680911760975, 'global_pooling': 'mean', 'learning_rate': 8.875213495423654e-05, 'weight_decay': 6.538934334718028e-05, 'beta_0': 0.8882384118254191, 'beta_1': 0.99772509208903, 'epsilon': 8.545719409975695e-06, 'balanced_loss': True, 'epochs': 185, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 06:28:26,928] Trial 334 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9715189776094237, 'batch_size': 26, 'attention_heads': 9, 'hidden_dimension': 82, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33403526655496646, 'global_pooling': 'mean', 'learning_rate': 0.00016715066805068047, 'weight_decay': 0.0005911156417314022, 'beta_0': 0.8841902661574512, 'beta_1': 0.9941916890021831, 'epsilon': 5.296016122257361e-06, 'balanced_loss': True, 'epochs': 127, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 688.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 40.04 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 06:39:15,033] Trial 335 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9639708156054577, 'batch_size': 19, 'attention_heads': 9, 'hidden_dimension': 71, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3285068057507412, 'global_pooling': 'mean', 'learning_rate': 6.986536235364992e-05, 'weight_decay': 7.734270382165773e-05, 'beta_0': 0.8933741962331374, 'beta_1': 0.997113257845298, 'epsilon': 7.4376723338810535e-06, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 06:53:54,186] Trial 336 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9662109237793124, 'batch_size': 21, 'attention_heads': 11, 'hidden_dimension': 86, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3420657843718844, 'global_pooling': 'mean', 'learning_rate': 0.0001221769691197758, 'weight_decay': 1.5035775966690899e-06, 'beta_0': 0.8863290108957652, 'beta_1': 0.9982618621136874, 'epsilon': 1.1414545737310886e-05, 'balanced_loss': True, 'epochs': 104, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.27 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 41.30 GiB memory in use. Of the allocated memory 38.27 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 07:03:35,265] Trial 337 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9589135735906132, 'batch_size': 22, 'attention_heads': 9, 'hidden_dimension': 143, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32501316664577046, 'global_pooling': 'mean', 'learning_rate': 0.00014279846127592576, 'weight_decay': 6.956119741467026e-05, 'beta_0': 0.8899242204162847, 'beta_1': 0.9978894591968992, 'epsilon': 4.339062069321402e-06, 'balanced_loss': True, 'epochs': 125, 'early_stopping_patience': 18, 'plateau_patience': 21, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 122.69 MiB is free. Including non-PyTorch memory, this process has 44.43 GiB memory in use. Of the allocated memory 37.53 GiB is allocated by PyTorch, and 5.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 07:15:15,374] Trial 338 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9611070750647596, 'batch_size': 40, 'attention_heads': 7, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35237269761901, 'global_pooling': 'mean', 'learning_rate': 0.0001965260737103914, 'weight_decay': 5.194755113621467e-05, 'beta_0': 0.8841906602860187, 'beta_1': 0.9985371542511641, 'epsilon': 6.643887014464784e-06, 'balanced_loss': True, 'epochs': 193, 'early_stopping_patience': 11, 'plateau_patience': 11, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 07:28:53,431] Trial 339 finished with value: 0.8727272727272727 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9700018840344877, 'batch_size': 24, 'attention_heads': 9, 'hidden_dimension': 91, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33248867967285767, 'global_pooling': 'max', 'learning_rate': 0.00010333631399547533, 'weight_decay': 7.047647739778261e-05, 'beta_0': 0.8882859093232429, 'beta_1': 0.9973238004609547, 'epsilon': 9.539469676823156e-06, 'balanced_loss': True, 'epochs': 199, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 3}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 07:48:56,048] Trial 340 finished with value: 0.8909090909090909 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9567661369042976, 'batch_size': 28, 'attention_heads': 9, 'hidden_dimension': 77, 'number_of_hidden_layers': 0, 'dropout_rate': 0.33824151182894313, 'global_pooling': 'mean', 'learning_rate': 0.00016183548102748866, 'weight_decay': 1.8007357008287164e-06, 'beta_0': 0.8861421212095132, 'beta_1': 0.9986546635218229, 'epsilon': 4.995031167248308e-06, 'balanced_loss': True, 'epochs': 130, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.72 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.46 GiB is free. Including non-PyTorch memory, this process has 42.10 GiB memory in use. Of the allocated memory 39.00 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 07:58:49,628] Trial 341 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9630987178193368, 'batch_size': 27, 'attention_heads': 12, 'hidden_dimension': 95, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3624919134126687, 'global_pooling': 'mean', 'learning_rate': 0.00021147051129403297, 'weight_decay': 5.447271355173697e-06, 'beta_0': 0.8914796681335416, 'beta_1': 0.996747219506967, 'epsilon': 2.3534900753012026e-05, 'balanced_loss': True, 'epochs': 123, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 08:13:07,739] Trial 342 finished with value: 0.9151515151515152 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9668116011981813, 'batch_size': 19, 'attention_heads': 8, 'hidden_dimension': 102, 'number_of_hidden_layers': 0, 'dropout_rate': 0.32488193371624524, 'global_pooling': 'mean', 'learning_rate': 9.12060400928824e-05, 'weight_decay': 0.00010774128240861911, 'beta_0': 0.8300223572618869, 'beta_1': 0.9980800692345286, 'epsilon': 6.071564622930838e-06, 'balanced_loss': True, 'epochs': 182, 'early_stopping_patience': 24, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 08:26:27,309] Trial 343 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9552431510823991, 'batch_size': 20, 'attention_heads': 9, 'hidden_dimension': 72, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34565458272990285, 'global_pooling': 'mean', 'learning_rate': 0.00013855257124851524, 'weight_decay': 8.012000475953594e-05, 'beta_0': 0.8832232730515994, 'beta_1': 0.9974901402542015, 'epsilon': 7.801759972638504e-06, 'balanced_loss': True, 'epochs': 108, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 08:41:14,527] Trial 344 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9644208084275466, 'batch_size': 18, 'attention_heads': 11, 'hidden_dimension': 83, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3313520356255826, 'global_pooling': 'mean', 'learning_rate': 0.00011727093132889746, 'weight_decay': 5.9237905064122886e-05, 'beta_0': 0.8937778298395016, 'beta_1': 0.9982988084551382, 'epsilon': 1.2953426531794396e-05, 'balanced_loss': True, 'epochs': 116, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 08:51:59,633] Trial 345 finished with value: 0.8727272727272727 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9948654442050255, 'batch_size': 27, 'attention_heads': 9, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3353714427465083, 'global_pooling': 'mean', 'learning_rate': 0.0001724871033345779, 'weight_decay': 2.0863802852380243e-05, 'beta_0': 0.8890778792400863, 'beta_1': 0.9883086915760766, 'epsilon': 8.690862576023691e-06, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 09:05:14,115] Trial 346 finished with value: 0.8545454545454545 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9609913995575178, 'batch_size': 53, 'attention_heads': 9, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3222606334245516, 'global_pooling': 'mean', 'learning_rate': 7.670734159050855e-05, 'weight_decay': 0.00012661115844741778, 'beta_0': 0.8954437126444115, 'beta_1': 0.9958530207191791, 'epsilon': 3.911918695742527e-06, 'balanced_loss': True, 'epochs': 128, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 09:18:15,491] Trial 347 finished with value: 0.8909090909090909 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9674675992556596, 'batch_size': 19, 'attention_heads': 7, 'hidden_dimension': 157, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3722114768017012, 'global_pooling': 'mean', 'learning_rate': 0.00021404352400258362, 'weight_decay': 6.215088234091896e-06, 'beta_0': 0.8863486523774419, 'beta_1': 0.9989513580107425, 'epsilon': 6.732613283459565e-06, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 09:30:48,183] Trial 348 finished with value: 0.8727272727272727 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9592781701791242, 'batch_size': 21, 'attention_heads': 12, 'hidden_dimension': 88, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35052983534406823, 'global_pooling': 'sum', 'learning_rate': 0.00015384934658438135, 'weight_decay': 1.3339704455846002e-06, 'beta_0': 0.8657016564571014, 'beta_1': 0.9979037214267008, 'epsilon': 1.0363531899720509e-05, 'balanced_loss': True, 'epochs': 112, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 09:43:43,178] Trial 349 finished with value: 0.8484848484848485 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9574428047645944, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.49681472766244855, 'global_pooling': 'mean', 'learning_rate': 5.942023718245397e-05, 'weight_decay': 8.671547558344925e-05, 'beta_0': 0.8824186651913951, 'beta_1': 0.9968673161724181, 'epsilon': 5.347994353515815e-06, 'balanced_loss': True, 'epochs': 81, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 242 with value: 0.9272727272727272.
CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process has 41.06 GiB memory in use. Of the allocated memory 37.98 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-21 09:57:26,358] Trial 350 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9626466883246978, 'batch_size': 24, 'attention_heads': 11, 'hidden_dimension': 109, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3265172151614313, 'global_pooling': 'mean', 'learning_rate': 0.00018343022469050567, 'weight_decay': 1.7411021024704184e-05, 'beta_0': 0.8919795487717783, 'beta_1': 0.9976310061724228, 'epsilon': 5.633382459470894e-08, 'balanced_loss': True, 'epochs': 66, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 10:10:04,006] Trial 351 finished with value: 0.896969696969697 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9739042534090189, 'batch_size': 18, 'attention_heads': 9, 'hidden_dimension': 75, 'number_of_hidden_layers': 0, 'dropout_rate': 0.34035965807599855, 'global_pooling': 'mean', 'learning_rate': 0.00011348209643911929, 'weight_decay': 3.9942884096121403e-05, 'beta_0': 0.8848053488417713, 'beta_1': 0.9986179317959375, 'epsilon': 8.139437857818475e-06, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 4}. Best is trial 242 with value: 0.9272727272727272.
[I 2025-02-21 10:23:16,363] Trial 352 finished with value: 0.9090909090909091 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9692700859548928, 'batch_size': 20, 'attention_heads': 13, 'hidden_dimension': 80, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3192236034282453, 'global_pooling': 'mean', 'learning_rate': 0.0002355743501094919, 'weight_decay': 5.006612205854086e-06, 'beta_0': 0.8880153026756664, 'beta_1': 0.9971553269433989, 'epsilon': 6.0607720595816415e-06, 'balanced_loss': True, 'epochs': 189, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 242 with value: 0.9272727272727272.

[TRIAL] 242 [VALIDATION PERFORMANCE] 0.9272727272727272 [TRAINING LOSS] 0.07624504708995422 [VALIDATION LOSS] 0.23899813517928123 

number                                     242
value                                 0.927273
params_threshold                       0.94489
params_attention_heads                       9
params_balanced_loss                      True
params_embedding_pooling_operation         min
params_attention_pooling_operation         min
params_batch_size                           17
params_dropout_rate                   0.317502
params_early_stopping_patience              18
params_epochs                              145
params_global_pooling                     mean
params_hidden_dimension                     38
params_learning_rate                  0.000279
params_number_of_hidden_layers               0
params_plateau_divider                      10
params_plateau_patience                     10
params_weight_decay                   0.000007
params_beta_0                          0.88217
params_beta_1                         0.998411
params_epsilon                        0.000005
user_attrs_epoch                          23.0
user_attrs_training_loss              0.076245
user_attrs_validation_loss            0.238998
params_left_stride                          64
params_right_stride                        128
Name: 242, dtype: object
37 Val: 0.896969696969697 Test: 0.9194029850746268
38 Val: 0.9030303030303031 Test: 0.8865671641791045
39 Val: 0.9151515151515152 Test: 0.9104477611940298
40 Val: 0.9151515151515152 Test: 0.9164179104477612
41 Val: 0.9030303030303031 Test: 0.9074626865671642
42 Val: 0.9272727272727272 Test: 0.9014925373134328
43 Val: 0.9212121212121213 Test: 0.9134328358208955
44 Val: 0.9030303030303031 Test: 0.9194029850746268
45 Val: 0.9090909090909091 Test: 0.9074626865671642
46 Val: 0.9030303030303031 Test: 0.8985074626865671
Validation performance: 89.7 & 90.97  0.97 & 92.73
Testing performance: 88.66 & 90.81  1.03 & 91.94

[TRIAL] 223 [VALIDATION PERFORMANCE] 0.9212121212121213 [TRAINING LOSS] 0.09805011126445606 [VALIDATION LOSS] 0.220412669534033 

number                                     223
value                                 0.921212
params_threshold                      0.942947
params_attention_heads                      10
params_balanced_loss                      True
params_embedding_pooling_operation        mean
params_attention_pooling_operation         min
params_batch_size                           16
params_dropout_rate                   0.310745
params_early_stopping_patience              16
params_epochs                              127
params_global_pooling                     mean
params_hidden_dimension                     44
params_learning_rate                  0.000202
params_number_of_hidden_layers               0
params_plateau_divider                       2
params_plateau_patience                     10
params_weight_decay                    0.00001
params_beta_0                         0.888145
params_beta_1                         0.997747
params_epsilon                        0.000009
user_attrs_epoch                          23.0
user_attrs_training_loss               0.09805
user_attrs_validation_loss            0.220413
params_left_stride                          64
params_right_stride                        128
Name: 223, dtype: object
37 Val: 0.9151515151515152 Test: 0.9223880597014925
38 Val: 0.9090909090909091 Test: 0.9044776119402985
39 Val: 0.9151515151515152 Test: 0.9104477611940298
40 Val: 0.9151515151515152 Test: 0.9074626865671642
41 Val: 0.9090909090909091 Test: 0.9014925373134328
42 Val: 0.9212121212121213 Test: 0.8985074626865671
43 Val: 0.9090909090909091 Test: 0.9074626865671642
44 Val: 0.9151515151515152 Test: 0.9014925373134328
45 Val: 0.896969696969697 Test: 0.8865671641791045
46 Val: 0.9212121212121213 Test: 0.9194029850746268
Validation performance: 89.7 & 91.27  0.71 & 92.12
Testing performance: 88.66 & 90.6  1.03 & 92.24

[TRIAL] 112 [VALIDATION PERFORMANCE] 0.9212121212121213 [TRAINING LOSS] 0.07471569124609231 [VALIDATION LOSS] 0.223251716366836 

number                                     112
value                                 0.921212
params_threshold                       0.96468
params_attention_heads                       7
params_balanced_loss                      True
params_embedding_pooling_operation        mean
params_attention_pooling_operation         min
params_batch_size                           26
params_dropout_rate                   0.427354
params_early_stopping_patience              16
params_epochs                              115
params_global_pooling                     mean
params_hidden_dimension                     66
params_learning_rate                  0.000159
params_number_of_hidden_layers               0
params_plateau_divider                      10
params_plateau_patience                     11
params_weight_decay                   0.000006
params_beta_0                         0.850961
params_beta_1                         0.998031
params_epsilon                        0.000005
user_attrs_epoch                          49.0
user_attrs_training_loss              0.074716
user_attrs_validation_loss            0.223252
params_left_stride                          64
params_right_stride                        128
Name: 112, dtype: object
37 Val: 0.9151515151515152 Test: 0.9164179104477612
38 Val: 0.9090909090909091 Test: 0.9104477611940298
39 Val: 0.896969696969697 Test: 0.9223880597014925
40 Val: 0.9090909090909091 Test: 0.9253731343283582
41 Val: 0.9030303030303031 Test: 0.9134328358208955
42 Val: 0.9212121212121213 Test: 0.9134328358208955
43 Val: 0.9090909090909091 Test: 0.9074626865671642
44 Val: 0.9090909090909091 Test: 0.9104477611940298
45 Val: 0.9030303030303031 Test: 0.9134328358208955
46 Val: 0.9151515151515152 Test: 0.8955223880597015
Validation performance: 89.7 & 90.91  0.7 & 92.12
Testing performance: 89.55 & 91.28  0.82 & 92.54

[TRIAL] 138 [VALIDATION PERFORMANCE] 0.9212121212121213 [TRAINING LOSS] 0.07813750831410289 [VALIDATION LOSS] 0.22530812174081802 

number                                     138
value                                 0.921212
params_threshold                      0.958583
params_attention_heads                       8
params_balanced_loss                      True
params_embedding_pooling_operation        mean
params_attention_pooling_operation         min
params_batch_size                           17
params_dropout_rate                   0.328887
params_early_stopping_patience              16
params_epochs                              120
params_global_pooling                     mean
params_hidden_dimension                     55
params_learning_rate                  0.000128
params_number_of_hidden_layers               0
params_plateau_divider                      10
params_plateau_patience                     11
params_weight_decay                   0.000002
params_beta_0                         0.881255
params_beta_1                         0.996919
params_epsilon                        0.000008
user_attrs_epoch                          42.0
user_attrs_training_loss              0.078138
user_attrs_validation_loss            0.225308
params_left_stride                          64
params_right_stride                        128
Name: 138, dtype: object
37 Val: 0.9151515151515152 Test: 0.9164179104477612
38 Val: 0.9030303030303031 Test: 0.9014925373134328
39 Val: 0.9212121212121213 Test: 0.9074626865671642
40 Val: 0.9151515151515152 Test: 0.9044776119402985
41 Val: 0.9090909090909091 Test: 0.9074626865671642
42 Val: 0.9212121212121213 Test: 0.8985074626865671
43 Val: 0.9090909090909091 Test: 0.9014925373134328
44 Val: 0.9151515151515152 Test: 0.8925373134328358
45 Val: 0.9090909090909091 Test: 0.9014925373134328
46 Val: 0.9151515151515152 Test: 0.9104477611940298
Validation performance: 90.3 & 91.33  0.57 & 92.12
Testing performance: 89.25 & 90.42  0.67 & 91.64

[TRIAL] 71 [VALIDATION PERFORMANCE] 0.9212121212121213 [TRAINING LOSS] 0.12757302917264127 [VALIDATION LOSS] 0.22537559436427224 

number                                      71
value                                 0.921212
params_threshold                      0.953025
params_attention_heads                       9
params_balanced_loss                      True
params_embedding_pooling_operation        mean
params_attention_pooling_operation         min
params_batch_size                           19
params_dropout_rate                   0.325818
params_early_stopping_patience              17
params_epochs                              130
params_global_pooling                     mean
params_hidden_dimension                     60
params_learning_rate                  0.000276
params_number_of_hidden_layers               0
params_plateau_divider                      10
params_plateau_patience                     10
params_weight_decay                   0.000006
params_beta_0                         0.885154
params_beta_1                         0.998921
params_epsilon                        0.000008
user_attrs_epoch                          19.0
user_attrs_training_loss              0.127573
user_attrs_validation_loss            0.225376
params_left_stride                          64
params_right_stride                        128
Name: 71, dtype: object
37 Val: 0.9151515151515152 Test: 0.9134328358208955
38 Val: 0.9212121212121213 Test: 0.9223880597014925
39 Val: 0.9151515151515152 Test: 0.9074626865671642
40 Val: 0.9090909090909091 Test: 0.9134328358208955
41 Val: 0.9212121212121213 Test: 0.9044776119402985
42 Val: 0.9212121212121213 Test: 0.9074626865671642
43 Val: 0.9090909090909091 Test: 0.9014925373134328
44 Val: 0.9090909090909091 Test: 0.9014925373134328
45 Val: 0.9030303030303031 Test: 0.9134328358208955
46 Val: 0.9212121212121213 Test: 0.9134328358208955
Validation performance: 90.3 & 91.45  0.67 & 92.12
Testing performance: 90.15 & 90.99  0.66 & 92.24

[IMDb-top_1000] Elapsed time: 2034.0521391669909 minutes.
