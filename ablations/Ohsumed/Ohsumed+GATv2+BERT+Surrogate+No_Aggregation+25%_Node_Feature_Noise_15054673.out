[I 2025-02-28 11:08:51,372] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.25' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-28 11:26:21,907] Trial 172 finished with value: 0.570527155191386 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9778425762505645, 'batch_size': 75, 'attention_heads': 9, 'hidden_dimension': 188, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3958265008218744, 'global_pooling': 'max', 'learning_rate': 0.0006090313823983624, 'weight_decay': 0.00018172710331533693, 'beta_0': 0.8342807124582852, 'beta_1': 0.9888095665436993, 'epsilon': 2.0282064009596805e-06, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.84 GiB is free. Including non-PyTorch memory, this process has 42.72 GiB memory in use. Of the allocated memory 39.88 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 11:39:08,541] Trial 173 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9685143731887627, 'batch_size': 78, 'attention_heads': 10, 'hidden_dimension': 239, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4031685021944314, 'global_pooling': 'max', 'learning_rate': 0.0010811521839703166, 'weight_decay': 0.00013890794068575218, 'beta_0': 0.835674392304548, 'beta_1': 0.9884852773214658, 'epsilon': 1.3461291915376773e-06, 'balanced_loss': False, 'epochs': 194, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 11:59:24,898] Trial 174 finished with value: 0.6055804587438989 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9744822755205889, 'batch_size': 70, 'attention_heads': 10, 'hidden_dimension': 224, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4182476760883086, 'global_pooling': 'max', 'learning_rate': 0.0008936025084200308, 'weight_decay': 0.00022774900195748644, 'beta_0': 0.8273546708533237, 'beta_1': 0.9867649457719608, 'epsilon': 7.977400259743372e-07, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacity of 44.56 GiB of which 40.69 MiB is free. Including non-PyTorch memory, this process has 44.51 GiB memory in use. Of the allocated memory 40.80 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 12:11:59,463] Trial 175 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.964641647532126, 'batch_size': 81, 'attention_heads': 11, 'hidden_dimension': 218, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3880905849872322, 'global_pooling': 'max', 'learning_rate': 0.0007865026681360268, 'weight_decay': 0.0002305828016424015, 'beta_0': 0.8245173886787867, 'beta_1': 0.9867227057788264, 'epsilon': 9.725841003222064e-07, 'balanced_loss': False, 'epochs': 189, 'early_stopping_patience': 20, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 12:31:08,509] Trial 176 finished with value: 0.4849612090118563 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9732171636599799, 'batch_size': 55, 'attention_heads': 10, 'hidden_dimension': 223, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37017991845837717, 'global_pooling': 'sum', 'learning_rate': 0.0004976987837002289, 'weight_decay': 0.0002607752884891791, 'beta_0': 0.8268913250028483, 'beta_1': 0.9912603114062918, 'epsilon': 7.273250788689227e-07, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 12:47:40,668] Trial 177 finished with value: 0.576611972960105 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9821308227098239, 'batch_size': 70, 'attention_heads': 9, 'hidden_dimension': 170, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4241699971203599, 'global_pooling': 'max', 'learning_rate': 0.0009350688967850402, 'weight_decay': 0.00017158147242371248, 'beta_0': 0.8303413838100488, 'beta_1': 0.9877162325779456, 'epsilon': 1.1543808304164266e-06, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 13:09:02,758] Trial 178 finished with value: 0.5840311545372928 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9748692733733884, 'batch_size': 68, 'attention_heads': 10, 'hidden_dimension': 234, 'number_of_hidden_layers': 1, 'dropout_rate': 0.410791545585169, 'global_pooling': 'max', 'learning_rate': 0.0006672534916990672, 'weight_decay': 0.00012468103239851906, 'beta_0': 0.8318598982083827, 'beta_1': 0.9860513799509689, 'epsilon': 7.866315812061757e-07, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 13:28:15,761] Trial 179 finished with value: 0.5733418090439287 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.985760190958777, 'batch_size': 119, 'attention_heads': 11, 'hidden_dimension': 246, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41825511835028345, 'global_pooling': 'max', 'learning_rate': 0.0010728919320567294, 'weight_decay': 9.77063958433665e-05, 'beta_0': 0.8179446525381456, 'beta_1': 0.9853726315576055, 'epsilon': 5.906189714523931e-07, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 13:46:44,989] Trial 180 finished with value: 0.5792707942860462 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.978541533145498, 'batch_size': 73, 'attention_heads': 9, 'hidden_dimension': 228, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3999124026987952, 'global_pooling': 'max', 'learning_rate': 0.0008538536358874416, 'weight_decay': 0.00015250385645781798, 'beta_0': 0.8284044003589691, 'beta_1': 0.9871317406502121, 'epsilon': 2.618705757742508e-06, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 66.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 42.73 GiB is allocated by PyTorch, and 626.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 13:55:52,950] Trial 181 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8693174619549766, 'batch_size': 71, 'attention_heads': 10, 'hidden_dimension': 240, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4179592348814565, 'global_pooling': 'max', 'learning_rate': 0.00040812797057018595, 'weight_decay': 0.00019956286660021268, 'beta_0': 0.8299498882917825, 'beta_1': 0.9841318269013295, 'epsilon': 5.059170331122247e-07, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 14:16:12,746] Trial 182 finished with value: 0.5393161182660517 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.969664471763974, 'batch_size': 76, 'attention_heads': 9, 'hidden_dimension': 252, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3937249976757961, 'global_pooling': 'max', 'learning_rate': 0.0016789764334585245, 'weight_decay': 0.00011303714150818921, 'beta_0': 0.8538337782572969, 'beta_1': 0.9900043113389408, 'epsilon': 1.6630911869678239e-06, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 14:32:05,342] Trial 183 finished with value: 0.5700433106621614 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9901581106098917, 'batch_size': 124, 'attention_heads': 8, 'hidden_dimension': 201, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4283742850091237, 'global_pooling': 'max', 'learning_rate': 0.0005957630138518534, 'weight_decay': 0.0003163577443353077, 'beta_0': 0.832536253432349, 'beta_1': 0.9893061989737891, 'epsilon': 1.4009710518461755e-06, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 42.52 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 14:44:24,778] Trial 184 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9624982218703656, 'batch_size': 83, 'attention_heads': 10, 'hidden_dimension': 215, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40562961556473254, 'global_pooling': 'max', 'learning_rate': 0.0014257509406818045, 'weight_decay': 7.22825346419271e-05, 'beta_0': 0.8264461005235694, 'beta_1': 0.9879126403294625, 'epsilon': 4.177993316074953e-08, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 15:03:08,718] Trial 185 finished with value: 0.5818398015888048 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9752610997999925, 'batch_size': 78, 'attention_heads': 10, 'hidden_dimension': 225, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3838546668181891, 'global_pooling': 'max', 'learning_rate': 0.0007391688858637734, 'weight_decay': 0.00015687361897004804, 'beta_0': 0.8353653978081461, 'beta_1': 0.9897166318874114, 'epsilon': 1.0386066751873278e-06, 'balanced_loss': False, 'epochs': 182, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 15:18:29,946] Trial 186 finished with value: 0.46761625805758145 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9966717015940596, 'batch_size': 74, 'attention_heads': 9, 'hidden_dimension': 233, 'number_of_hidden_layers': 1, 'dropout_rate': 0.411218902188793, 'global_pooling': 'max', 'learning_rate': 0.0009740674219593175, 'weight_decay': 0.00022875581229512727, 'beta_0': 0.8577470833602547, 'beta_1': 0.9891187647583429, 'epsilon': 2.9346769459963488e-08, 'balanced_loss': False, 'epochs': 194, 'early_stopping_patience': 21, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 15:37:47,021] Trial 187 finished with value: 0.5888309166137768 and parameters: {'left_stride': 128, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9681192491270902, 'batch_size': 86, 'attention_heads': 8, 'hidden_dimension': 232, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3906090781598749, 'global_pooling': 'max', 'learning_rate': 0.0007738267734005896, 'weight_decay': 0.00013576208000538707, 'beta_0': 0.8368365013227964, 'beta_1': 0.9884739239105921, 'epsilon': 1.7794463618837853e-06, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 15:56:44,350] Trial 188 finished with value: 0.5990248241542373 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9686465005762344, 'batch_size': 85, 'attention_heads': 8, 'hidden_dimension': 222, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3891908052706373, 'global_pooling': 'max', 'learning_rate': 0.0005431779855915587, 'weight_decay': 0.00018484173664798294, 'beta_0': 0.828731287866716, 'beta_1': 0.9886734182372271, 'epsilon': 2.136983833544436e-06, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 16:15:52,374] Trial 189 finished with value: 0.5900556680882717 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9699163265780923, 'batch_size': 86, 'attention_heads': 8, 'hidden_dimension': 222, 'number_of_hidden_layers': 1, 'dropout_rate': 0.38994739680075086, 'global_pooling': 'max', 'learning_rate': 0.0004979709410763344, 'weight_decay': 0.0002012320667103518, 'beta_0': 0.8287865686177205, 'beta_1': 0.9883114035428566, 'epsilon': 2.012985388679693e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 16:35:07,455] Trial 190 finished with value: 0.5816043565421853 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9671340434202825, 'batch_size': 86, 'attention_heads': 8, 'hidden_dimension': 221, 'number_of_hidden_layers': 1, 'dropout_rate': 0.39032485695572405, 'global_pooling': 'max', 'learning_rate': 0.0005586159418670807, 'weight_decay': 0.00019004384756282318, 'beta_0': 0.8283006839392624, 'beta_1': 0.9884088517364171, 'epsilon': 2.0256551739212045e-06, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 884.69 MiB is free. Including non-PyTorch memory, this process has 43.69 GiB memory in use. Of the allocated memory 40.33 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 16:47:41,990] Trial 191 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9627520778199035, 'batch_size': 84, 'attention_heads': 8, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3955873049687142, 'global_pooling': 'max', 'learning_rate': 0.0008014728823984125, 'weight_decay': 0.00017884330639860978, 'beta_0': 0.8308961158124826, 'beta_1': 0.9887773877064432, 'epsilon': 2.234193202108542e-06, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 510.69 MiB is free. Including non-PyTorch memory, this process has 44.05 GiB memory in use. Of the allocated memory 40.87 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 17:00:14,598] Trial 192 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9584589507778871, 'batch_size': 88, 'attention_heads': 8, 'hidden_dimension': 209, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3851786928368181, 'global_pooling': 'max', 'learning_rate': 0.000361170880832108, 'weight_decay': 0.00021499478025284753, 'beta_0': 0.823615070437387, 'beta_1': 0.9881640085223126, 'epsilon': 2.821285289542387e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 17:18:49,118] Trial 193 finished with value: 0.5681215360215065 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.970590919486177, 'batch_size': 81, 'attention_heads': 8, 'hidden_dimension': 215, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4001482622145964, 'global_pooling': 'max', 'learning_rate': 0.0005008441660706793, 'weight_decay': 0.0001323877642845451, 'beta_0': 0.8335669250949547, 'beta_1': 0.9875972445836292, 'epsilon': 4.049038623181783e-06, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.27 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 42.73 GiB memory in use. Of the allocated memory 39.44 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 17:31:38,081] Trial 194 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9680677701379474, 'batch_size': 86, 'attention_heads': 9, 'hidden_dimension': 222, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3890124811296287, 'global_pooling': 'max', 'learning_rate': 0.0006738568591952371, 'weight_decay': 0.0002931016007011243, 'beta_0': 0.8292429404297732, 'beta_1': 0.9886214930614159, 'epsilon': 1.7742968771250014e-06, 'balanced_loss': False, 'epochs': 197, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 43.38 GiB memory in use. Of the allocated memory 38.25 GiB is allocated by PyTorch, and 3.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 17:46:23,906] Trial 195 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9721454120777167, 'batch_size': 80, 'attention_heads': 10, 'hidden_dimension': 231, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3765076060840666, 'global_pooling': 'max', 'learning_rate': 0.0004219633611410385, 'weight_decay': 0.00016138568727393666, 'beta_0': 0.8365218559650773, 'beta_1': 0.9891470411535982, 'epsilon': 1.437519695776656e-06, 'balanced_loss': True, 'epochs': 185, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 18:04:23,440] Trial 196 finished with value: 0.5585885147899825 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.975977188727755, 'batch_size': 84, 'attention_heads': 8, 'hidden_dimension': 235, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4032235032657649, 'global_pooling': 'mean', 'learning_rate': 0.0011057574976116545, 'weight_decay': 0.00012217123099627681, 'beta_0': 0.8275828590707651, 'beta_1': 0.986727409492354, 'epsilon': 4.8719043206201657e-05, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 18:21:59,210] Trial 197 finished with value: 0.5559548178238743 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9787738721533116, 'batch_size': 93, 'attention_heads': 8, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3719876661525431, 'global_pooling': 'max', 'learning_rate': 0.0005067619064842548, 'weight_decay': 2.3915730745401985e-05, 'beta_0': 0.8318660438170029, 'beta_1': 0.9894426147815778, 'epsilon': 2.332888644208438e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.16 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 43.09 GiB memory in use. Of the allocated memory 39.81 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:34:37,106] Trial 198 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9644423210440479, 'batch_size': 90, 'attention_heads': 9, 'hidden_dimension': 219, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3812478351052753, 'global_pooling': 'max', 'learning_rate': 0.0006035642358195689, 'weight_decay': 2.7402771143955596e-05, 'beta_0': 0.8253628311618826, 'beta_1': 0.9881713862600572, 'epsilon': 1.1851918407412916e-06, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 20, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.46 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.29 GiB is free. Including non-PyTorch memory, this process has 42.26 GiB memory in use. Of the allocated memory 39.69 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:55:26,215] Trial 199 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9715822645295257, 'batch_size': 88, 'attention_heads': 10, 'hidden_dimension': 228, 'number_of_hidden_layers': 1, 'dropout_rate': 0.36282650723652476, 'global_pooling': 'max', 'learning_rate': 0.0003004886442523113, 'weight_decay': 0.00010316559555963669, 'beta_0': 0.8299360546539982, 'beta_1': 0.9899677028672469, 'epsilon': 1.6367515746714312e-06, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 19:15:37,452] Trial 200 finished with value: 0.5745528896923202 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9807565311628942, 'batch_size': 86, 'attention_heads': 10, 'hidden_dimension': 238, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5884039649668332, 'global_pooling': 'max', 'learning_rate': 0.0008335722070866454, 'weight_decay': 2.1308781678357205e-05, 'beta_0': 0.8336246428402829, 'beta_1': 0.9888320481342409, 'epsilon': 9.555062292124778e-07, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 80.69 MiB is free. Including non-PyTorch memory, this process has 44.47 GiB memory in use. Of the allocated memory 41.51 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 19:28:18,732] Trial 201 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.974329995515747, 'batch_size': 82, 'attention_heads': 8, 'hidden_dimension': 222, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3927939729498829, 'global_pooling': 'max', 'learning_rate': 0.00045690871420070456, 'weight_decay': 0.00014465552937071454, 'beta_0': 0.8282313716708183, 'beta_1': 0.9879117222290608, 'epsilon': 6.830718004666663e-07, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 20, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 19:48:19,108] Trial 202 finished with value: 0.589731677870952 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9686290916679119, 'batch_size': 44, 'attention_heads': 9, 'hidden_dimension': 243, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3987503777078188, 'global_pooling': 'max', 'learning_rate': 0.0007396526017827797, 'weight_decay': 0.0001869871833417657, 'beta_0': 0.8361587329720177, 'beta_1': 0.9884182295207021, 'epsilon': 1.920860634234e-06, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.16 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.53 GiB is free. Including non-PyTorch memory, this process has 43.03 GiB memory in use. Of the allocated memory 39.33 GiB is allocated by PyTorch, and 2.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 20:05:37,899] Trial 203 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9657796823791619, 'batch_size': 77, 'attention_heads': 9, 'hidden_dimension': 230, 'number_of_hidden_layers': 1, 'dropout_rate': 0.39807496894346645, 'global_pooling': 'max', 'learning_rate': 0.0007055804108692419, 'weight_decay': 0.0001986494411944898, 'beta_0': 0.8362413849634796, 'beta_1': 0.9874191528815829, 'epsilon': 1.9343519102031163e-06, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 20:24:50,816] Trial 204 finished with value: 0.6117042615120699 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.970038846545547, 'batch_size': 49, 'attention_heads': 9, 'hidden_dimension': 242, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40754883726747265, 'global_pooling': 'max', 'learning_rate': 0.0008925544851490419, 'weight_decay': 0.00022906694354990113, 'beta_0': 0.8309246061040375, 'beta_1': 0.988612273085885, 'epsilon': 3.0361798683545905e-06, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 42.72 GiB memory in use. Of the allocated memory 39.97 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 20:37:37,367] Trial 205 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9576365934893342, 'batch_size': 46, 'attention_heads': 9, 'hidden_dimension': 243, 'number_of_hidden_layers': 1, 'dropout_rate': 0.39179165000137933, 'global_pooling': 'max', 'learning_rate': 0.0008943042396715918, 'weight_decay': 0.0002597075535034087, 'beta_0': 0.834714544829009, 'beta_1': 0.9884762327486962, 'epsilon': 3.4617176183671998e-06, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 43.27 GiB memory in use. Of the allocated memory 40.04 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 20:52:36,426] Trial 206 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9606871990908414, 'batch_size': 50, 'attention_heads': 9, 'hidden_dimension': 236, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40737053283850627, 'global_pooling': 'max', 'learning_rate': 0.0012147642613137605, 'weight_decay': 0.00017116574336521176, 'beta_0': 0.8321254693466463, 'beta_1': 0.9893492585605969, 'epsilon': 2.9764360841633537e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 7}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 21:11:52,365] Trial 207 finished with value: 0.586215465189914 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9687509062678784, 'batch_size': 46, 'attention_heads': 9, 'hidden_dimension': 242, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41363205896261984, 'global_pooling': 'max', 'learning_rate': 0.0010184773609085865, 'weight_decay': 0.00022662782674678876, 'beta_0': 0.8299460110370029, 'beta_1': 0.9808939971297365, 'epsilon': 2.1497047903995155e-06, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 21:31:13,023] Trial 208 finished with value: 0.5988922705481555 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9719480203405717, 'batch_size': 51, 'attention_heads': 9, 'hidden_dimension': 234, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40124202061211695, 'global_pooling': 'max', 'learning_rate': 0.0007395274318747609, 'weight_decay': 0.00018713112391686401, 'beta_0': 0.8267493070770858, 'beta_1': 0.9888987857644894, 'epsilon': 2.533649492359008e-06, 'balanced_loss': False, 'epochs': 181, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 21:51:10,624] Trial 209 finished with value: 0.5678737443462102 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9767743775778535, 'batch_size': 83, 'attention_heads': 10, 'hidden_dimension': 232, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3983637996761041, 'global_pooling': 'max', 'learning_rate': 0.0007201144939416781, 'weight_decay': 0.000152720977291439, 'beta_0': 0.8320825718280915, 'beta_1': 0.9889308804656243, 'epsilon': 1.3414475475126893e-06, 'balanced_loss': False, 'epochs': 192, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 22:11:24,420] Trial 210 finished with value: 0.5961392517240053 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9725032477348803, 'batch_size': 51, 'attention_heads': 10, 'hidden_dimension': 237, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3862301375251261, 'global_pooling': 'max', 'learning_rate': 0.0008413607327612144, 'weight_decay': 0.00013444390730546168, 'beta_0': 0.8379053731197204, 'beta_1': 0.9902535375640944, 'epsilon': 1.7677991666234124e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.56 GiB is free. Including non-PyTorch memory, this process has 42.99 GiB memory in use. Of the allocated memory 39.91 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 22:24:10,148] Trial 211 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.964455089025644, 'batch_size': 51, 'attention_heads': 11, 'hidden_dimension': 237, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4098242526296499, 'global_pooling': 'max', 'learning_rate': 0.0008265566317181431, 'weight_decay': 0.0001303810757419115, 'beta_0': 0.8380488059988036, 'beta_1': 0.9904287511613735, 'epsilon': 1.7153878879727605e-06, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 22:43:10,189] Trial 212 finished with value: 0.5890740989874241 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9706857426463389, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 246, 'number_of_hidden_layers': 1, 'dropout_rate': 0.39620917349440854, 'global_pooling': 'max', 'learning_rate': 0.00059136739526945, 'weight_decay': 8.466115108315015e-05, 'beta_0': 0.8406739525649776, 'beta_1': 0.9897028491657044, 'epsilon': 2.598926420666169e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 23:01:53,715] Trial 213 finished with value: 0.5805268363934335 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9702388478913381, 'batch_size': 44, 'attention_heads': 9, 'hidden_dimension': 244, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4066402417759187, 'global_pooling': 'max', 'learning_rate': 0.0009862141187343115, 'weight_decay': 8.476183323439545e-05, 'beta_0': 0.841208910870846, 'beta_1': 0.9908878707446948, 'epsilon': 5.159145003156855e-06, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 23:20:52,760] Trial 214 finished with value: 0.5199945836523903 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9734014962028029, 'batch_size': 41, 'attention_heads': 9, 'hidden_dimension': 246, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40110015761916046, 'global_pooling': 'sum', 'learning_rate': 0.0006229998046549595, 'weight_decay': 0.00010146468041486195, 'beta_0': 0.841438048017351, 'beta_1': 0.9896876762706195, 'epsilon': 2.7572320174244787e-06, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-02-28 23:41:26,976] Trial 215 finished with value: 0.5945757238496661 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.966273236575625, 'batch_size': 55, 'attention_heads': 9, 'hidden_dimension': 249, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41636125367333626, 'global_pooling': 'max', 'learning_rate': 0.0012183475135510572, 'weight_decay': 0.00035203011297612355, 'beta_0': 0.8372145377582872, 'beta_1': 0.9901139293836093, 'epsilon': 3.7259002224750774e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 43.36 GiB memory in use. Of the allocated memory 40.45 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 23:56:18,310] Trial 216 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9636244703383561, 'batch_size': 54, 'attention_heads': 9, 'hidden_dimension': 246, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41656260595348205, 'global_pooling': 'max', 'learning_rate': 0.0011023938710583478, 'weight_decay': 0.0003741198354290337, 'beta_0': 0.8405522781126418, 'beta_1': 0.991116814736719, 'epsilon': 4.135576019037395e-06, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 00:15:50,372] Trial 217 finished with value: 0.5913978313897141 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.968379302487506, 'batch_size': 52, 'attention_heads': 9, 'hidden_dimension': 250, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4203679933046549, 'global_pooling': 'max', 'learning_rate': 0.0008011737819807453, 'weight_decay': 0.0004183632707160039, 'beta_0': 0.8382674795154784, 'beta_1': 0.990343243832038, 'epsilon': 3.3287349617588632e-06, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 00:34:29,235] Trial 218 finished with value: 0.5601858312288447 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9716502297582413, 'batch_size': 38, 'attention_heads': 9, 'hidden_dimension': 250, 'number_of_hidden_layers': 1, 'dropout_rate': 0.42576167640411255, 'global_pooling': 'max', 'learning_rate': 0.001273146451747338, 'weight_decay': 0.000516163409017888, 'beta_0': 0.8392809127867764, 'beta_1': 0.9902348086888657, 'epsilon': 3.612591066754381e-06, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 00:54:19,088] Trial 219 finished with value: 0.5658152751182033 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9656265662587309, 'batch_size': 52, 'attention_heads': 9, 'hidden_dimension': 240, 'number_of_hidden_layers': 1, 'dropout_rate': 0.42100377739805245, 'global_pooling': 'max', 'learning_rate': 0.0009143645163738204, 'weight_decay': 0.0004182162389128273, 'beta_0': 0.8381688542954885, 'beta_1': 0.9905599896250281, 'epsilon': 3.260653443508753e-06, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 01:13:08,523] Trial 220 finished with value: 0.6107751502925914 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9716319494558946, 'batch_size': 47, 'attention_heads': 9, 'hidden_dimension': 249, 'number_of_hidden_layers': 1, 'dropout_rate': 0.415237287104894, 'global_pooling': 'max', 'learning_rate': 0.0006587022165286241, 'weight_decay': 0.0006280940002783796, 'beta_0': 0.8439846339399214, 'beta_1': 0.9900901946940847, 'epsilon': 2.6242610529803443e-06, 'balanced_loss': False, 'epochs': 168, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 208.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 12.69 MiB is free. Including non-PyTorch memory, this process has 44.54 GiB memory in use. Of the allocated memory 42.91 GiB is allocated by PyTorch, and 494.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 01:20:10,323] Trial 221 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8256374938534885, 'batch_size': 47, 'attention_heads': 9, 'hidden_dimension': 252, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4153894039945624, 'global_pooling': 'max', 'learning_rate': 0.0007423675754093668, 'weight_decay': 0.0005967876151094823, 'beta_0': 0.8254533132494853, 'beta_1': 0.9901941138768898, 'epsilon': 2.9591906230215154e-06, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 17, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 01:38:47,787] Trial 222 finished with value: 0.5778761577698522 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9730124113305034, 'batch_size': 56, 'attention_heads': 9, 'hidden_dimension': 249, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4319672415488307, 'global_pooling': 'max', 'learning_rate': 0.0009084077632981957, 'weight_decay': 0.0006871811645823844, 'beta_0': 0.8444664840378362, 'beta_1': 0.9906628076365062, 'epsilon': 6.271107765426065e-06, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.13 GiB is free. Including non-PyTorch memory, this process has 42.42 GiB memory in use. Of the allocated memory 39.31 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 01:51:27,658] Trial 223 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9602812668654567, 'batch_size': 58, 'attention_heads': 9, 'hidden_dimension': 241, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41337439099831635, 'global_pooling': 'max', 'learning_rate': 0.0012773800398406475, 'weight_decay': 0.00032809419310268127, 'beta_0': 0.8342003851793858, 'beta_1': 0.9902946259799694, 'epsilon': 4.410080945876734e-06, 'balanced_loss': False, 'epochs': 166, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 02:10:43,764] Trial 224 finished with value: 0.587716586697295 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9669461317800928, 'batch_size': 49, 'attention_heads': 9, 'hidden_dimension': 238, 'number_of_hidden_layers': 1, 'dropout_rate': 0.418314666487639, 'global_pooling': 'max', 'learning_rate': 0.0006929353669456205, 'weight_decay': 0.0004632389318974527, 'beta_0': 0.8471494847282447, 'beta_1': 0.9896094096774583, 'epsilon': 2.2739858134023612e-06, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 02:28:07,386] Trial 225 finished with value: 0.5759154425191321 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9807211653927417, 'batch_size': 49, 'attention_heads': 9, 'hidden_dimension': 252, 'number_of_hidden_layers': 1, 'dropout_rate': 0.409334630969658, 'global_pooling': 'max', 'learning_rate': 0.0011127708395741783, 'weight_decay': 0.0008790262474974358, 'beta_0': 0.8429081745378889, 'beta_1': 0.9891409832713631, 'epsilon': 3.369388026336347e-06, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 17, 'plateau_patience': 14, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 02:48:03,473] Trial 226 finished with value: 0.5780617472977758 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.976624169637289, 'batch_size': 52, 'attention_heads': 10, 'hidden_dimension': 243, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4236261433377577, 'global_pooling': 'max', 'learning_rate': 0.0008357089390064001, 'weight_decay': 0.0002862294069296703, 'beta_0': 0.8282439174105082, 'beta_1': 0.9914213556085785, 'epsilon': 2.5023631699650226e-06, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
[I 2025-03-01 03:08:26,804] Trial 227 finished with value: 0.6087799322058507 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9704712694193229, 'batch_size': 44, 'attention_heads': 9, 'hidden_dimension': 249, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40576098328568994, 'global_pooling': 'max', 'learning_rate': 0.0006069125318366945, 'weight_decay': 0.000600386686433626, 'beta_0': 0.8415574802470275, 'beta_1': 0.9898520840604872, 'epsilon': 2.6717770099780684e-06, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 94 with value: 0.6130243860768243.
slurmstepd: error: *** JOB 15054673 ON gpu054 CANCELLED AT 2025-03-01T03:08:50 DUE TO TIME LIMIT ***
