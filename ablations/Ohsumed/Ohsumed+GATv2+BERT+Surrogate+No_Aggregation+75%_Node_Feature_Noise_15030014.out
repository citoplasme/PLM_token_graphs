[I 2025-02-26 08:29:41,780] A new study created in RDB with name: Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.75
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 74.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 42.49 GiB is allocated by PyTorch, and 864.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 08:41:27,096] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.860845406169478, 'batch_size': 82, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
[I 2025-02-26 08:58:21,152] Trial 1 finished with value: 0.3446432663762865 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9878903933234222, 'batch_size': 118, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 1 with value: 0.3446432663762865.
[I 2025-02-26 09:15:52,868] Trial 2 finished with value: 0.47905374076735147 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9774336763878396, 'batch_size': 77, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 112.69 MiB is free. Including non-PyTorch memory, this process has 44.44 GiB memory in use. Of the allocated memory 42.73 GiB is allocated by PyTorch, and 579.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 09:23:37,884] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.845584753156763, 'batch_size': 73, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 6.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 42.67 GiB is allocated by PyTorch, and 744.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 09:32:57,807] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8735529487125235, 'batch_size': 93, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 6.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 6.37 GiB is free. Including non-PyTorch memory, this process has 38.19 GiB memory in use. Of the allocated memory 36.63 GiB is allocated by PyTorch, and 413.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 09:45:19,352] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9451838762172592, 'batch_size': 119, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 2.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.07 GiB is free. Including non-PyTorch memory, this process has 42.48 GiB memory in use. Of the allocated memory 41.10 GiB is allocated by PyTorch, and 239.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 09:57:56,268] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9153750078864256, 'batch_size': 79, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 2 with value: 0.47905374076735147.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-26 09:57:58,019] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8718946353324388, 'batch_size': 60, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 2.34 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 43.34 GiB memory in use. Of the allocated memory 41.57 GiB is allocated by PyTorch, and 630.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 10:10:30,136] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9452110058311951, 'batch_size': 126, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 102.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 38.69 MiB is free. Including non-PyTorch memory, this process has 44.52 GiB memory in use. Of the allocated memory 42.73 GiB is allocated by PyTorch, and 647.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 10:21:35,465] Trial 9 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8983182588646154, 'batch_size': 77, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 10:39:56,922] Trial 10 finished with value: 0.016502270900653104 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9967100388938873, 'batch_size': 35, 'attention_heads': 4, 'hidden_dimension': 48, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5215216634175063, 'global_pooling': 'sum', 'learning_rate': 0.0002656836893415814, 'weight_decay': 8.178412772916804e-06, 'beta_0': 0.8049297522472849, 'beta_1': 0.9840036011509948, 'epsilon': 1.3261882354835817e-05, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 10, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 11:05:45,023] Trial 11 finished with value: 0.2573022203634239 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.998038342792064, 'batch_size': 106, 'attention_heads': 9, 'hidden_dimension': 254, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3965390541849626, 'global_pooling': 'max', 'learning_rate': 4.0083099308315954e-05, 'weight_decay': 1.9948878206620123e-05, 'beta_0': 0.8229997949552795, 'beta_1': 0.9846023396580135, 'epsilon': 1.2304350108398785e-06, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 1.79 GiB. GPU 0 has a total capacity of 44.56 GiB of which 400.69 MiB is free. Including non-PyTorch memory, this process has 44.16 GiB memory in use. Of the allocated memory 41.37 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 11:21:02,371] Trial 12 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9673488701932526, 'batch_size': 58, 'attention_heads': 11, 'hidden_dimension': 174, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4232817731798428, 'global_pooling': 'mean', 'learning_rate': 0.0003794553385403524, 'weight_decay': 1.0332855569626058e-06, 'beta_0': 0.8160804530670469, 'beta_1': 0.9847976982734662, 'epsilon': 1.051978058963241e-06, 'balanced_loss': False, 'epochs': 170, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 14.69 MiB is free. Including non-PyTorch memory, this process has 44.54 GiB memory in use. Of the allocated memory 42.69 GiB is allocated by PyTorch, and 712.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 11:27:59,443] Trial 13 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8213372070008632, 'batch_size': 104, 'attention_heads': 7, 'hidden_dimension': 256, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48082126313764223, 'global_pooling': 'max', 'learning_rate': 6.567919987784165e-05, 'weight_decay': 3.7947055714200486e-05, 'beta_0': 0.8379605356560551, 'beta_1': 0.9827099355668892, 'epsilon': 4.088261595658418e-06, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 11:48:18,084] Trial 14 finished with value: 0.4347735792941312 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9618700653403338, 'batch_size': 59, 'attention_heads': 4, 'hidden_dimension': 181, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3806622045960868, 'global_pooling': 'sum', 'learning_rate': 0.0007163547440785774, 'weight_decay': 0.0001242481468574075, 'beta_0': 0.8159973543818061, 'beta_1': 0.986691679686623, 'epsilon': 3.3584310308021153e-07, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 12:09:25,151] Trial 15 finished with value: 0.45266496428877684 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9468162923144088, 'batch_size': 46, 'attention_heads': 4, 'hidden_dimension': 176, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4184972264557809, 'global_pooling': 'sum', 'learning_rate': 0.0012907927554706986, 'weight_decay': 9.49048471129446e-05, 'beta_0': 0.8144156676117761, 'beta_1': 0.9946506297224307, 'epsilon': 2.641771339771067e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 828.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 606.69 MiB is free. Including non-PyTorch memory, this process has 43.96 GiB memory in use. Of the allocated memory 42.09 GiB is allocated by PyTorch, and 737.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 12:26:54,225] Trial 16 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9234267643951115, 'batch_size': 34, 'attention_heads': 7, 'hidden_dimension': 152, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42462872333668983, 'global_pooling': 'sum', 'learning_rate': 0.01352654210488433, 'weight_decay': 1.131972459279219e-05, 'beta_0': 0.8130909822351192, 'beta_1': 0.9957286903556802, 'epsilon': 2.514825705048361e-07, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 12:48:00,789] Trial 17 finished with value: 0.455978637704968 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9689375068804992, 'batch_size': 46, 'attention_heads': 4, 'hidden_dimension': 123, 'number_of_hidden_layers': 3, 'dropout_rate': 0.50832887599105, 'global_pooling': 'sum', 'learning_rate': 0.0011261127130204611, 'weight_decay': 4.685575661361269e-05, 'beta_0': 0.8983389870168317, 'beta_1': 0.9934226186935181, 'epsilon': 8.726317095919483e-05, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 13:02:26,510] Trial 18 finished with value: 0.02108317184881658 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9726277490588433, 'batch_size': 50, 'attention_heads': 6, 'hidden_dimension': 67, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5210599776599867, 'global_pooling': 'sum', 'learning_rate': 0.010103385413656988, 'weight_decay': 4.895573929837408e-06, 'beta_0': 0.8929988835025001, 'beta_1': 0.9980302171390404, 'epsilon': 9.706029190454322e-05, 'balanced_loss': True, 'epochs': 174, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 970.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 304.69 MiB is free. Including non-PyTorch memory, this process has 44.26 GiB memory in use. Of the allocated memory 42.24 GiB is allocated by PyTorch, and 881.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 13:20:02,030] Trial 19 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9254976678757352, 'batch_size': 67, 'attention_heads': 5, 'hidden_dimension': 118, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5962052301604162, 'global_pooling': 'mean', 'learning_rate': 0.0008649769134734023, 'weight_decay': 5.5385278284652645e-05, 'beta_0': 0.8515885122702299, 'beta_1': 0.9931013303715305, 'epsilon': 2.563692465356134e-05, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 11, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 20.69 MiB is free. Including non-PyTorch memory, this process has 44.53 GiB memory in use. Of the allocated memory 42.55 GiB is allocated by PyTorch, and 854.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 13:32:19,087] Trial 20 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9057042513852782, 'batch_size': 90, 'attention_heads': 8, 'hidden_dimension': 92, 'number_of_hidden_layers': 3, 'dropout_rate': 0.48073744786255823, 'global_pooling': 'mean', 'learning_rate': 3.185753230911198e-05, 'weight_decay': 1.4432854388595148e-05, 'beta_0': 0.8613941117427366, 'beta_1': 0.9877579457441124, 'epsilon': 3.813505499149013e-06, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 16, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 13:54:28,052] Trial 21 finished with value: 0.4467529198853411 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9470696763799795, 'batch_size': 46, 'attention_heads': 4, 'hidden_dimension': 172, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4561701502859013, 'global_pooling': 'sum', 'learning_rate': 0.0015881760770625015, 'weight_decay': 5.05669820333934e-05, 'beta_0': 0.8263399605199344, 'beta_1': 0.9945619200887814, 'epsilon': 3.8245262717193783e-07, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 14:15:37,014] Trial 22 finished with value: 0.458904623915966 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9808656158783176, 'batch_size': 43, 'attention_heads': 5, 'hidden_dimension': 141, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5152398202256283, 'global_pooling': 'sum', 'learning_rate': 0.0005486339667606862, 'weight_decay': 8.097782461526467e-05, 'beta_0': 0.8087984183433634, 'beta_1': 0.9938751142957803, 'epsilon': 2.434544312084683e-05, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 14, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 14:38:01,858] Trial 23 finished with value: 0.4229188402094398 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9773044735313867, 'batch_size': 42, 'attention_heads': 5, 'hidden_dimension': 138, 'number_of_hidden_layers': 3, 'dropout_rate': 0.511495031848123, 'global_pooling': 'sum', 'learning_rate': 0.0004955618969802039, 'weight_decay': 2.499579536201916e-05, 'beta_0': 0.8974504068559414, 'beta_1': 0.9920417578353995, 'epsilon': 2.284198953483048e-05, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 15:02:58,900] Trial 24 finished with value: 0.3514449710040076 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9613597283720613, 'batch_size': 51, 'attention_heads': 5, 'hidden_dimension': 80, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5498370112437861, 'global_pooling': 'sum', 'learning_rate': 9.764390476687972e-05, 'weight_decay': 8.176270659932437e-05, 'beta_0': 0.8445772015234675, 'beta_1': 0.9938468028763795, 'epsilon': 6.795686827312297e-05, 'balanced_loss': False, 'epochs': 185, 'early_stopping_patience': 12, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 15:24:45,761] Trial 25 finished with value: 0.045208116171311354 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9778083771066666, 'batch_size': 37, 'attention_heads': 7, 'hidden_dimension': 115, 'number_of_hidden_layers': 4, 'dropout_rate': 0.49968841345164805, 'global_pooling': 'sum', 'learning_rate': 0.005866012213460184, 'weight_decay': 0.0001980151297018143, 'beta_0': 0.8083817982608033, 'beta_1': 0.9908111723296639, 'epsilon': 1.0794841385003453e-05, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 15, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 174.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 134.69 MiB is free. Including non-PyTorch memory, this process has 44.42 GiB memory in use. Of the allocated memory 42.33 GiB is allocated by PyTorch, and 968.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 15:33:23,856] Trial 26 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8034732850257874, 'batch_size': 66, 'attention_heads': 8, 'hidden_dimension': 149, 'number_of_hidden_layers': 3, 'dropout_rate': 0.552352377947812, 'global_pooling': 'sum', 'learning_rate': 0.00019585474879423514, 'weight_decay': 2.25389261154177e-06, 'beta_0': 0.8250170032271817, 'beta_1': 0.9964557157287334, 'epsilon': 3.5543832874466336e-05, 'balanced_loss': False, 'epochs': 199, 'early_stopping_patience': 13, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 15:48:54,471] Trial 27 finished with value: 0.45366098153222895 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9852382715584083, 'batch_size': 92, 'attention_heads': 5, 'hidden_dimension': 130, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5347225455964931, 'global_pooling': 'mean', 'learning_rate': 0.0008965257426648974, 'weight_decay': 5.740778128249425e-05, 'beta_0': 0.8761283211447248, 'beta_1': 0.998800192918023, 'epsilon': 1.4641942543873258e-05, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 44.56 GiB of which 418.69 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 42.07 GiB is allocated by PyTorch, and 944.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 16:06:17,482] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9284540260314148, 'batch_size': 54, 'attention_heads': 6, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46289192847247956, 'global_pooling': 'sum', 'learning_rate': 0.0004439160311705392, 'weight_decay': 7.034074343640121e-06, 'beta_0': 0.8563153207343507, 'beta_1': 0.9816831855393958, 'epsilon': 2.9071149548752806e-06, 'balanced_loss': False, 'epochs': 163, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 16:25:32,286] Trial 29 finished with value: 0.240308912658862 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9603763372646716, 'batch_size': 68, 'attention_heads': 8, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49652015081968376, 'global_pooling': 'mean', 'learning_rate': 0.006898540397183448, 'weight_decay': 1.5475844069268917e-05, 'beta_0': 0.8010366142118943, 'beta_1': 0.9916355874130983, 'epsilon': 6.986193621144051e-06, 'balanced_loss': True, 'epochs': 139, 'early_stopping_patience': 11, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 2 with value: 0.47905374076735147.
CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 84.69 MiB is free. Including non-PyTorch memory, this process has 44.47 GiB memory in use. Of the allocated memory 42.65 GiB is allocated by PyTorch, and 682.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 16:36:34,508] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8946864654964983, 'batch_size': 85, 'attention_heads': 9, 'hidden_dimension': 160, 'number_of_hidden_layers': 3, 'dropout_rate': 0.43962491132053927, 'global_pooling': 'sum', 'learning_rate': 0.0026840884267864355, 'weight_decay': 0.00018787904422362705, 'beta_0': 0.8487344054340532, 'beta_1': 0.9908168496987348, 'epsilon': 4.644832215974692e-05, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 15, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 16:51:30,350] Trial 31 finished with value: 0.4246009038710417 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9844193863012187, 'batch_size': 97, 'attention_heads': 5, 'hidden_dimension': 131, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5359373675674287, 'global_pooling': 'mean', 'learning_rate': 0.0009628279120308039, 'weight_decay': 5.512556787944067e-05, 'beta_0': 0.8765420813386572, 'beta_1': 0.9988026277299106, 'epsilon': 1.7374557993782487e-05, 'balanced_loss': True, 'epochs': 187, 'early_stopping_patience': 12, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 17:05:42,356] Trial 32 finished with value: 0.3031774631446158 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9878191287742392, 'batch_size': 85, 'attention_heads': 5, 'hidden_dimension': 116, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5739333123379171, 'global_pooling': 'mean', 'learning_rate': 0.0016228929867682467, 'weight_decay': 6.196424965699768e-05, 'beta_0': 0.88915059360934, 'beta_1': 0.9975541478587144, 'epsilon': 2.166245912169998e-06, 'balanced_loss': True, 'epochs': 163, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 17:19:06,957] Trial 33 finished with value: 0.010882341313652679 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9938760281526378, 'batch_size': 104, 'attention_heads': 4, 'hidden_dimension': 94, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5352403586665998, 'global_pooling': 'mean', 'learning_rate': 0.03371047539909981, 'weight_decay': 3.395521807848767e-05, 'beta_0': 0.89986087646536, 'beta_1': 0.9947985795932472, 'epsilon': 8.235479572198603e-06, 'balanced_loss': True, 'epochs': 200, 'early_stopping_patience': 11, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 2 with value: 0.47905374076735147.
[I 2025-02-26 17:38:02,154] Trial 34 finished with value: 0.48936969679134906 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9553704560908574, 'batch_size': 41, 'attention_heads': 6, 'hidden_dimension': 164, 'number_of_hidden_layers': 2, 'dropout_rate': 0.47073518644965645, 'global_pooling': 'mean', 'learning_rate': 0.00015422871575400718, 'weight_decay': 0.00019729005883551486, 'beta_0': 0.8731730539464349, 'beta_1': 0.9890091050282693, 'epsilon': 9.49298759445173e-05, 'balanced_loss': True, 'epochs': 186, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 34 with value: 0.48936969679134906.
[I 2025-02-26 17:59:41,017] Trial 35 finished with value: 0.5110688929206058 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9544214622750054, 'batch_size': 41, 'attention_heads': 7, 'hidden_dimension': 164, 'number_of_hidden_layers': 2, 'dropout_rate': 0.46746095688495354, 'global_pooling': 'mean', 'learning_rate': 0.000147548365369207, 'weight_decay': 0.0004343222469359283, 'beta_0': 0.8701730223538221, 'beta_1': 0.9892050227799578, 'epsilon': 7.942147444257823e-05, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 138.69 MiB is free. Including non-PyTorch memory, this process has 44.42 GiB memory in use. Of the allocated memory 42.51 GiB is allocated by PyTorch, and 778.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 18:12:52,600] Trial 36 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9379919854411782, 'batch_size': 39, 'attention_heads': 7, 'hidden_dimension': 228, 'number_of_hidden_layers': 1, 'dropout_rate': 0.46776705675445035, 'global_pooling': 'mean', 'learning_rate': 0.0001322307455476495, 'weight_decay': 0.0009567239702960158, 'beta_0': 0.8678153003201362, 'beta_1': 0.9892717677817557, 'epsilon': 3.497492011776084e-05, 'balanced_loss': True, 'epochs': 191, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 18:40:10,312] Trial 37 finished with value: 0.41881802532600587 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.95510223091094, 'batch_size': 42, 'attention_heads': 6, 'hidden_dimension': 201, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4407342719874734, 'global_pooling': 'mean', 'learning_rate': 1.122833595572655e-05, 'weight_decay': 0.0004002105587657408, 'beta_0': 0.8426779829686798, 'beta_1': 0.9866414375926448, 'epsilon': 6.1017141838812765e-05, 'balanced_loss': True, 'epochs': 149, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 1.69 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 43.49 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 307.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 18:53:12,073] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9397553822777766, 'batch_size': 55, 'attention_heads': 10, 'hidden_dimension': 161, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4732558108719916, 'global_pooling': 'mean', 'learning_rate': 0.000174010126682136, 'weight_decay': 0.0002538151375509222, 'beta_0': 0.8730332420725692, 'beta_1': 0.9888274901078977, 'epsilon': 6.969981474186903e-07, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.18 GiB is free. Including non-PyTorch memory, this process has 43.37 GiB memory in use. Of the allocated memory 40.37 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 19:06:13,759] Trial 39 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9553589547012109, 'batch_size': 33, 'attention_heads': 16, 'hidden_dimension': 188, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49119807892656303, 'global_pooling': 'mean', 'learning_rate': 7.524345731739051e-05, 'weight_decay': 0.0005598477589186136, 'beta_0': 0.8604879494822357, 'beta_1': 0.9861889737642362, 'epsilon': 1.213965801256414e-08, 'balanced_loss': True, 'epochs': 153, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 4.59 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.80 GiB is free. Including non-PyTorch memory, this process has 41.75 GiB memory in use. Of the allocated memory 40.00 GiB is allocated by PyTorch, and 614.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 19:19:17,968] Trial 40 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9321089817373744, 'batch_size': 73, 'attention_heads': 13, 'hidden_dimension': 213, 'number_of_hidden_layers': 2, 'dropout_rate': 0.43885224321526695, 'global_pooling': 'max', 'learning_rate': 2.404677083679483e-05, 'weight_decay': 0.000172704241927593, 'beta_0': 0.8830128465535256, 'beta_1': 0.9878297428626158, 'epsilon': 3.330911556390684e-05, 'balanced_loss': True, 'epochs': 193, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 19:37:31,143] Trial 41 finished with value: 0.4339667784099 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.9717468502705553, 'batch_size': 48, 'attention_heads': 6, 'hidden_dimension': 162, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5096756776824775, 'global_pooling': 'mean', 'learning_rate': 0.0003118200145132498, 'weight_decay': 0.0003683540781142934, 'beta_0': 0.8888998471314681, 'beta_1': 0.9901679628509977, 'epsilon': 9.14570441329316e-05, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 13, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-26 19:37:32,975] Trial 42 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9780086844248022, 'batch_size': 44, 'attention_heads': 4, 'hidden_dimension': 161, 'number_of_hidden_layers': 3, 'dropout_rate': 0.45064495911524016, 'global_pooling': 'mean', 'learning_rate': 0.0004862231668652988, 'weight_decay': 0.0008648365572891465, 'beta_0': 0.8064014352872224, 'beta_1': 0.9933020731706313, 'epsilon': 5.493910376897844e-05, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 14, 'plateau_patience': 14, 'plateau_divider': 6}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 794.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 352.69 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 42.48 GiB is allocated by PyTorch, and 591.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 19:50:23,146] Trial 43 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9126020091203201, 'batch_size': 39, 'attention_heads': 5, 'hidden_dimension': 142, 'number_of_hidden_layers': 4, 'dropout_rate': 0.49599851594507094, 'global_pooling': 'max', 'learning_rate': 0.00024228621166344882, 'weight_decay': 0.0002619964983762438, 'beta_0': 0.883208298464214, 'beta_1': 0.9834215770086329, 'epsilon': 7.957168610538354e-05, 'balanced_loss': True, 'epochs': 182, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 20:16:33,942] Trial 44 finished with value: 0.44980434844211287 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9525606649213996, 'batch_size': 32, 'attention_heads': 6, 'hidden_dimension': 201, 'number_of_hidden_layers': 2, 'dropout_rate': 0.47127793653044053, 'global_pooling': 'mean', 'learning_rate': 0.00012294080946249252, 'weight_decay': 0.00014264217339788327, 'beta_0': 0.8698394495136949, 'beta_1': 0.9886277029922705, 'epsilon': 1.8550947959234165e-05, 'balanced_loss': False, 'epochs': 159, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 124.69 MiB is free. Including non-PyTorch memory, this process has 44.43 GiB memory in use. Of the allocated memory 42.39 GiB is allocated by PyTorch, and 911.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 20:26:46,875] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8820703378227763, 'batch_size': 63, 'attention_heads': 7, 'hidden_dimension': 225, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5076444830925307, 'global_pooling': 'sum', 'learning_rate': 0.0006571589661536626, 'weight_decay': 1.0066450208514128e-06, 'beta_0': 0.8302738850718625, 'beta_1': 0.9808939971297365, 'epsilon': 4.4360786177546256e-05, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 14, 'plateau_patience': 23, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 120.69 MiB is free. Including non-PyTorch memory, this process has 44.44 GiB memory in use. Of the allocated memory 42.65 GiB is allocated by PyTorch, and 646.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 20:35:06,884] Trial 46 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8509248251123714, 'batch_size': 53, 'attention_heads': 4, 'hidden_dimension': 105, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3422180507966391, 'global_pooling': 'mean', 'learning_rate': 5.3826768362045636e-05, 'weight_decay': 7.756052749752958e-05, 'beta_0': 0.8210720987358657, 'beta_1': 0.9923917855867479, 'epsilon': 2.665738531048067e-05, 'balanced_loss': True, 'epochs': 194, 'early_stopping_patience': 10, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 2.17 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 43.00 GiB memory in use. Of the allocated memory 39.44 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 20:52:30,316] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9654285962778504, 'batch_size': 114, 'attention_heads': 9, 'hidden_dimension': 169, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4054989655125142, 'global_pooling': 'max', 'learning_rate': 0.0018132961186197916, 'weight_decay': 2.3659920592665464e-06, 'beta_0': 0.853761787902687, 'beta_1': 0.9902176097438691, 'epsilon': 1.093839333958432e-07, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 21:11:18,909] Trial 48 finished with value: 0.27950116384962415 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9933925072478319, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 186, 'number_of_hidden_layers': 2, 'dropout_rate': 0.47955433414680826, 'global_pooling': 'sum', 'learning_rate': 0.004028927859325812, 'weight_decay': 0.00048419631036683027, 'beta_0': 0.8619370186495063, 'beta_1': 0.9859390951859242, 'epsilon': 9.970939954609913e-05, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 15, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 21:26:10,989] Trial 49 finished with value: 0.04931445457062063 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9986937035325059, 'batch_size': 58, 'attention_heads': 7, 'hidden_dimension': 140, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5210645906197756, 'global_pooling': 'mean', 'learning_rate': 0.0001593778269011356, 'weight_decay': 0.00010854093214394259, 'beta_0': 0.8398953452930279, 'beta_1': 0.9957110779689381, 'epsilon': 5.822766264491017e-06, 'balanced_loss': True, 'epochs': 65, 'early_stopping_patience': 17, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 21:47:24,374] Trial 50 finished with value: 0.36851191378358866 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9691633589428483, 'batch_size': 75, 'attention_heads': 8, 'hidden_dimension': 150, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4561077000463652, 'global_pooling': 'sum', 'learning_rate': 0.00035937664748177085, 'weight_decay': 2.315934017606356e-05, 'beta_0': 0.8108566487083182, 'beta_1': 0.9823280254084483, 'epsilon': 2.096250610248489e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 11, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 22:02:04,030] Trial 51 finished with value: 0.39498825009787863 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9848303819049823, 'batch_size': 99, 'attention_heads': 6, 'hidden_dimension': 128, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5394103394865948, 'global_pooling': 'mean', 'learning_rate': 0.0011500865644593276, 'weight_decay': 3.685691037420092e-05, 'beta_0': 0.8777382182667312, 'beta_1': 0.9969036583937941, 'epsilon': 1.6500082588496187e-05, 'balanced_loss': True, 'epochs': 185, 'early_stopping_patience': 12, 'plateau_patience': 12, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 22:18:31,983] Trial 52 finished with value: 0.4409343473135946 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9844428857222107, 'batch_size': 93, 'attention_heads': 5, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5649498917766864, 'global_pooling': 'mean', 'learning_rate': 0.0006378812394565947, 'weight_decay': 4.280076646406505e-05, 'beta_0': 0.8198465625862406, 'beta_1': 0.9955291555742175, 'epsilon': 9.987004921898114e-06, 'balanced_loss': True, 'epochs': 179, 'early_stopping_patience': 24, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 22:33:58,076] Trial 53 finished with value: 0.43969347692353977 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9764200345477476, 'batch_size': 81, 'attention_heads': 4, 'hidden_dimension': 105, 'number_of_hidden_layers': 2, 'dropout_rate': 0.521715328297691, 'global_pooling': 'mean', 'learning_rate': 0.00032687445033354563, 'weight_decay': 7.812965315820146e-05, 'beta_0': 0.8726544591926471, 'beta_1': 0.9851374296023663, 'epsilon': 1.2923415426713325e-05, 'balanced_loss': True, 'epochs': 190, 'early_stopping_patience': 13, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 22:46:45,687] Trial 54 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9504912904087197, 'batch_size': 110, 'attention_heads': 11, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49077776129242606, 'global_pooling': 'mean', 'learning_rate': 0.0007064855543050844, 'weight_decay': 0.00015388494590583843, 'beta_0': 0.8944030657212165, 'beta_1': 0.994006686126832, 'epsilon': 6.222100212692219e-05, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 12, 'plateau_patience': 13, 'plateau_divider': 3}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 23:05:32,872] Trial 55 finished with value: 0.3283030356387318 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9677444671130749, 'batch_size': 47, 'attention_heads': 6, 'hidden_dimension': 179, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5294468204418602, 'global_pooling': 'mean', 'learning_rate': 0.002118648680801378, 'weight_decay': 0.00023916272677298902, 'beta_0': 0.8803066276834642, 'beta_1': 0.9911981762247011, 'epsilon': 1.7896273862039077e-06, 'balanced_loss': True, 'epochs': 197, 'early_stopping_patience': 10, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 23:25:16,765] Trial 56 finished with value: 0.39836600332256983 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9602965551265883, 'batch_size': 36, 'attention_heads': 5, 'hidden_dimension': 150, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5062460083302164, 'global_pooling': 'sum', 'learning_rate': 0.0011728641942758138, 'weight_decay': 0.0001151945441955796, 'beta_0': 0.8652319779388483, 'beta_1': 0.9987086638749, 'epsilon': 4.7068387703820265e-06, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 13, 'plateau_patience': 12, 'plateau_divider': 5}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-26 23:45:29,436] Trial 57 finished with value: 0.3957620764699805 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.991112720822563, 'batch_size': 88, 'attention_heads': 4, 'hidden_dimension': 168, 'number_of_hidden_layers': 3, 'dropout_rate': 0.599613019072074, 'global_pooling': 'mean', 'learning_rate': 0.0002344341485443706, 'weight_decay': 3.255381947619766e-06, 'beta_0': 0.8890074961219545, 'beta_1': 0.9884623999273061, 'epsilon': 4.198597647834885e-05, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 14, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 35 with value: 0.5110688929206058.
CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 44.56 GiB of which 200.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 42.73 GiB is allocated by PyTorch, and 492.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 00:03:19,091] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9416016281362188, 'batch_size': 120, 'attention_heads': 5, 'hidden_dimension': 123, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48365664897764876, 'global_pooling': 'max', 'learning_rate': 0.0005221746703983813, 'weight_decay': 0.0006957246624621372, 'beta_0': 0.8746585795386114, 'beta_1': 0.9928390332492265, 'epsilon': 2.5177180697107565e-05, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 12, 'plateau_patience': 10, 'plateau_divider': 6}. Best is trial 35 with value: 0.5110688929206058.
[I 2025-02-27 00:22:17,972] Trial 59 finished with value: 0.3369895327386348 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9821927593911841, 'batch_size': 44, 'attention_heads': 7, 'hidden_dimension': 240, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5483202985178982, 'global_pooling': 'sum', 'learning_rate': 0.0008325248774494454, 'weight_decay': 6.872886488118124e-05, 'beta_0': 0.8932586443297413, 'beta_1': 0.9872903675586052, 'epsilon': 8.186926456605656e-07, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 19, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 35 with value: 0.5110688929206058.
slurmstepd: error: *** JOB 15030014 ON gpu032 CANCELLED AT 2025-02-27T00:29:31 DUE TO TIME LIMIT ***
