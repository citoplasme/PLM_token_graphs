[I 2025-02-26 20:02:53,120] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.5' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-26 20:26:42,265] Trial 42 finished with value: 0.49351122656199403 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9461067998953967, 'batch_size': 49, 'attention_heads': 4, 'hidden_dimension': 193, 'number_of_hidden_layers': 1, 'dropout_rate': 0.41388127302606703, 'global_pooling': 'sum', 'learning_rate': 0.002843462730679623, 'weight_decay': 8.816904586159344e-05, 'beta_0': 0.8258499154883183, 'beta_1': 0.9951204856983663, 'epsilon': 4.434879775206662e-07, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 2 with value: 0.5059441274536349.
CUDA out of memory. Tried to allocate 1004.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 568.69 MiB is free. Including non-PyTorch memory, this process has 44.00 GiB memory in use. Of the allocated memory 42.36 GiB is allocated by PyTorch, and 503.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 20:44:54,561] Trial 43 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9250184998322462, 'batch_size': 45, 'attention_heads': 5, 'hidden_dimension': 187, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3869273861240436, 'global_pooling': 'sum', 'learning_rate': 0.0029968948789610267, 'weight_decay': 0.00010079014427079243, 'beta_0': 0.8237488185850358, 'beta_1': 0.9921675420520165, 'epsilon': 1.8823477194464376e-07, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 2 with value: 0.5059441274536349.
[I 2025-02-26 21:05:31,091] Trial 44 finished with value: 0.40767199694363376 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9924602433494817, 'batch_size': 62, 'attention_heads': 4, 'hidden_dimension': 217, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4438496180294093, 'global_pooling': 'sum', 'learning_rate': 0.0009600189141793192, 'weight_decay': 0.00020363510456836763, 'beta_0': 0.8058774410698107, 'beta_1': 0.9959266183235351, 'epsilon': 1.3897654696399844e-06, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 23, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 2 with value: 0.5059441274536349.
CUDA out of memory. Tried to allocate 170.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 104.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 42.26 GiB is allocated by PyTorch, and 1.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 21:17:07,496] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8536277128075006, 'batch_size': 54, 'attention_heads': 5, 'hidden_dimension': 178, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4219083654776428, 'global_pooling': 'sum', 'learning_rate': 0.005329078957678115, 'weight_decay': 8.081317490572928e-05, 'beta_0': 0.814829523204393, 'beta_1': 0.9977154409606331, 'epsilon': 4.481273150641924e-07, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 7}. Best is trial 2 with value: 0.5059441274536349.
[I 2025-02-26 21:37:06,403] Trial 46 finished with value: 0.4395180205066405 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9555133725354927, 'batch_size': 56, 'attention_heads': 10, 'hidden_dimension': 203, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4679245956870072, 'global_pooling': 'sum', 'learning_rate': 0.0003192988394946105, 'weight_decay': 3.212020244352793e-05, 'beta_0': 0.8341325254098868, 'beta_1': 0.9834164142130433, 'epsilon': 2.8353459605003708e-08, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 8}. Best is trial 2 with value: 0.5059441274536349.
CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 998.69 MiB is free. Including non-PyTorch memory, this process has 43.58 GiB memory in use. Of the allocated memory 40.68 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 21:50:22,870] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9437190945501548, 'batch_size': 82, 'attention_heads': 4, 'hidden_dimension': 242, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3673800724671034, 'global_pooling': 'mean', 'learning_rate': 0.003274926316440593, 'weight_decay': 6.220545480315375e-05, 'beta_0': 0.8266536789900617, 'beta_1': 0.9940804083136254, 'epsilon': 1.1013889628209193e-06, 'balanced_loss': True, 'epochs': 99, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 2 with value: 0.5059441274536349.
[I 2025-02-26 22:13:55,392] Trial 48 finished with value: 0.5081531127424952 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9709130437679077, 'batch_size': 96, 'attention_heads': 11, 'hidden_dimension': 164, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4076190572335312, 'global_pooling': 'max', 'learning_rate': 0.0021624114674499723, 'weight_decay': 0.00033715257698818776, 'beta_0': 0.8194660349354783, 'beta_1': 0.9806704160059783, 'epsilon': 2.1434521705982345e-07, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 10}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.28 GiB is free. Including non-PyTorch memory, this process has 42.28 GiB memory in use. Of the allocated memory 38.98 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 22:32:24,055] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9733891580405561, 'batch_size': 119, 'attention_heads': 12, 'hidden_dimension': 166, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40040882971233144, 'global_pooling': 'max', 'learning_rate': 0.00225365219601838, 'weight_decay': 0.0003765235038335932, 'beta_0': 0.8407378754887367, 'beta_1': 0.981044023527584, 'epsilon': 8.030497036869146e-08, 'balanced_loss': False, 'epochs': 124, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 10}. Best is trial 48 with value: 0.5081531127424952.
[I 2025-02-26 22:49:29,737] Trial 50 finished with value: 0.4856644723569044 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.985944935914422, 'batch_size': 98, 'attention_heads': 11, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3108821916599564, 'global_pooling': 'max', 'learning_rate': 0.00044784822715334104, 'weight_decay': 0.00028114734673106913, 'beta_0': 0.8199590592308037, 'beta_1': 0.9805079834282716, 'epsilon': 3.943203508652823e-06, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 13, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 2.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.99 GiB is free. Including non-PyTorch memory, this process has 42.56 GiB memory in use. Of the allocated memory 39.22 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 23:02:52,014] Trial 51 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9678552692070815, 'batch_size': 108, 'attention_heads': 12, 'hidden_dimension': 149, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48660054351427257, 'global_pooling': 'max', 'learning_rate': 0.00023599861845207754, 'weight_decay': 0.0005447932032471737, 'beta_0': 0.8043412749687684, 'beta_1': 0.9817872127587747, 'epsilon': 1.3796120732645842e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 48 with value: 0.5081531127424952.
[I 2025-02-26 23:21:12,634] Trial 52 finished with value: 0.06709389203211495 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9994526314905706, 'batch_size': 110, 'attention_heads': 11, 'hidden_dimension': 184, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4248754711272515, 'global_pooling': 'max', 'learning_rate': 0.0012686539951909947, 'weight_decay': 0.0001150009077446738, 'beta_0': 0.8219507269062813, 'beta_1': 0.9855953594358717, 'epsilon': 3.02800128181091e-07, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 9}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 3.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 40.89 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 23:39:41,446] Trial 53 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9402854819946833, 'batch_size': 86, 'attention_heads': 9, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.38935647749277064, 'global_pooling': 'max', 'learning_rate': 0.0007029293962356584, 'weight_decay': 0.0001328018479127532, 'beta_0': 0.8163817156164943, 'beta_1': 0.980020590312411, 'epsilon': 1.3947894076138034e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 11, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 48 with value: 0.5081531127424952.
[I 2025-02-27 00:01:03,335] Trial 54 finished with value: 0.47233093600122567 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9633646076819915, 'batch_size': 98, 'attention_heads': 4, 'hidden_dimension': 180, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4168513060298568, 'global_pooling': 'sum', 'learning_rate': 0.002921040327699566, 'weight_decay': 4.3111276186186044e-05, 'beta_0': 0.8091995965935181, 'beta_1': 0.9834858201809098, 'epsilon': 2.2956010297358584e-07, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 15, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 48 with value: 0.5081531127424952.
[I 2025-02-27 00:21:35,788] Trial 55 finished with value: 0.4521474687658853 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.952695786587072, 'batch_size': 46, 'attention_heads': 10, 'hidden_dimension': 169, 'number_of_hidden_layers': 0, 'dropout_rate': 0.40513116797749643, 'global_pooling': 'mean', 'learning_rate': 0.007700757357968214, 'weight_decay': 0.00018640196024307905, 'beta_0': 0.8129206654356351, 'beta_1': 0.9927989011984076, 'epsilon': 6.345462384454807e-07, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 12, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 1.70 GiB. GPU 0 has a total capacity of 44.56 GiB of which 240.69 MiB is free. Including non-PyTorch memory, this process has 44.32 GiB memory in use. Of the allocated memory 42.46 GiB is allocated by PyTorch, and 722.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 00:39:56,669] Trial 56 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9202010001626516, 'batch_size': 71, 'attention_heads': 6, 'hidden_dimension': 154, 'number_of_hidden_layers': 2, 'dropout_rate': 0.43654962014718285, 'global_pooling': 'sum', 'learning_rate': 0.000936314722544962, 'weight_decay': 6.68365721477805e-05, 'beta_0': 0.8329477228224018, 'beta_1': 0.9844938173983308, 'epsilon': 3.4634111629749797e-07, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 10}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 1.37 GiB. GPU 0 has a total capacity of 44.56 GiB of which 164.69 MiB is free. Including non-PyTorch memory, this process has 44.39 GiB memory in use. Of the allocated memory 42.43 GiB is allocated by PyTorch, and 834.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 00:58:16,355] Trial 57 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9334960039120562, 'batch_size': 94, 'attention_heads': 5, 'hidden_dimension': 136, 'number_of_hidden_layers': 1, 'dropout_rate': 0.37174883672751147, 'global_pooling': 'sum', 'learning_rate': 0.0020492163922829682, 'weight_decay': 0.00045057087535833184, 'beta_0': 0.8269685740179337, 'beta_1': 0.995897598222743, 'epsilon': 2.1551685588290258e-07, 'balanced_loss': True, 'epochs': 153, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 1.30 GiB. GPU 0 has a total capacity of 44.56 GiB of which 772.69 MiB is free. Including non-PyTorch memory, this process has 43.80 GiB memory in use. Of the allocated memory 41.99 GiB is allocated by PyTorch, and 675.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:12:07,654] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9748295425181396, 'batch_size': 51, 'attention_heads': 13, 'hidden_dimension': 192, 'number_of_hidden_layers': 4, 'dropout_rate': 0.49425474368000333, 'global_pooling': 'max', 'learning_rate': 0.004664027864431761, 'weight_decay': 1.5927431759512717e-05, 'beta_0': 0.8039370928170889, 'beta_1': 0.9871827178600339, 'epsilon': 5.509358096013087e-07, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 43.48 GiB memory in use. Of the allocated memory 42.13 GiB is allocated by PyTorch, and 208.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:30:32,130] Trial 59 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9459497754279447, 'batch_size': 126, 'attention_heads': 4, 'hidden_dimension': 205, 'number_of_hidden_layers': 1, 'dropout_rate': 0.45159456752429816, 'global_pooling': 'mean', 'learning_rate': 0.019499196338177795, 'weight_decay': 0.0007968175012549253, 'beta_0': 0.8204950618678126, 'beta_1': 0.9945228971144418, 'epsilon': 9.44154447026078e-07, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 19, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 48 with value: 0.5081531127424952.
CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 64.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 42.41 GiB is allocated by PyTorch, and 953.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:48:06,352] Trial 60 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9067214286743984, 'batch_size': 64, 'attention_heads': 9, 'hidden_dimension': 167, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38424659034921, 'global_pooling': 'sum', 'learning_rate': 0.0013060521038602722, 'weight_decay': 7.034074343640121e-06, 'beta_0': 0.8378374272978969, 'beta_1': 0.9829777360379134, 'epsilon': 2.2005627425226696e-06, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 7}. Best is trial 48 with value: 0.5081531127424952.
[I 2025-02-27 02:04:35,261] Trial 61 finished with value: 0.5159970815551687 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9912149211954169, 'batch_size': 32, 'attention_heads': 5, 'hidden_dimension': 182, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3471531764633749, 'global_pooling': 'sum', 'learning_rate': 0.0005794129198740482, 'weight_decay': 0.00025995931432627604, 'beta_0': 0.8885142189176355, 'beta_1': 0.9902898548655753, 'epsilon': 1.05200655829188e-07, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 61 with value: 0.5159970815551687.
[I 2025-02-27 02:20:28,950] Trial 62 finished with value: 0.5086036725870391 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9932785565954806, 'batch_size': 33, 'attention_heads': 5, 'hidden_dimension': 176, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3419302740091185, 'global_pooling': 'sum', 'learning_rate': 0.0006377416440877196, 'weight_decay': 0.0002761522062718165, 'beta_0': 0.8807197191502368, 'beta_1': 0.9914808331547593, 'epsilon': 1.0565193743936833e-07, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 22, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 61 with value: 0.5159970815551687.
[I 2025-02-27 02:37:26,033] Trial 63 finished with value: 0.4870053556953793 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9906657756316719, 'batch_size': 35, 'attention_heads': 5, 'hidden_dimension': 184, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3433078225623743, 'global_pooling': 'sum', 'learning_rate': 0.00039589406030407596, 'weight_decay': 0.00032424801922474437, 'beta_0': 0.8823008162853605, 'beta_1': 0.991356808673966, 'epsilon': 5.9477931689738915e-08, 'balanced_loss': False, 'epochs': 84, 'early_stopping_patience': 22, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 61 with value: 0.5159970815551687.
[I 2025-02-27 02:54:56,549] Trial 64 finished with value: 0.49044992254880787 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9868573531633065, 'batch_size': 34, 'attention_heads': 6, 'hidden_dimension': 210, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31080969394954083, 'global_pooling': 'sum', 'learning_rate': 0.0006511539691553723, 'weight_decay': 0.0002637435744838001, 'beta_0': 0.8949326545713389, 'beta_1': 0.9901073205332827, 'epsilon': 1.0638662983740863e-07, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 22, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 61 with value: 0.5159970815551687.
[I 2025-02-27 03:14:27,714] Trial 65 finished with value: 0.5401724205048527 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9825394602592485, 'batch_size': 33, 'attention_heads': 7, 'hidden_dimension': 210, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30459682279545125, 'global_pooling': 'sum', 'learning_rate': 0.0005948388207854971, 'weight_decay': 0.0002752950531736661, 'beta_0': 0.8936719150180028, 'beta_1': 0.9899443638464852, 'epsilon': 2.7756649255595258e-08, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 23, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 65 with value: 0.5401724205048527.
[I 2025-02-27 03:32:00,861] Trial 66 finished with value: 0.49483923432774407 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9945236904902469, 'batch_size': 36, 'attention_heads': 7, 'hidden_dimension': 222, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30160522036440623, 'global_pooling': 'sum', 'learning_rate': 0.000520843185437385, 'weight_decay': 0.0006714668232800963, 'beta_0': 0.895586110759388, 'beta_1': 0.9898797586651944, 'epsilon': 3.3158289235120154e-08, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 23, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 65 with value: 0.5401724205048527.
[I 2025-02-27 03:51:34,904] Trial 67 finished with value: 0.472090749407516 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9946459745119838, 'batch_size': 32, 'attention_heads': 7, 'hidden_dimension': 227, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3244100232094783, 'global_pooling': 'sum', 'learning_rate': 0.00020103977230405797, 'weight_decay': 0.0006152536122146723, 'beta_0': 0.8899229819221115, 'beta_1': 0.988262144479472, 'epsilon': 2.031770995799946e-08, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 23, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 65 with value: 0.5401724205048527.
[I 2025-02-27 04:13:35,684] Trial 68 finished with value: 0.4734526169430813 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9826157684368458, 'batch_size': 41, 'attention_heads': 8, 'hidden_dimension': 245, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30031861976007734, 'global_pooling': 'sum', 'learning_rate': 9.706946297771717e-05, 'weight_decay': 0.0009910054607753785, 'beta_0': 0.8685056703102759, 'beta_1': 0.989987124916389, 'epsilon': 3.26752015140374e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 15, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 65 with value: 0.5401724205048527.
[I 2025-02-27 04:30:01,171] Trial 69 finished with value: 0.2494417342008419 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9978903928830556, 'batch_size': 38, 'attention_heads': 7, 'hidden_dimension': 234, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3219023223608255, 'global_pooling': 'mean', 'learning_rate': 0.0005407720975247288, 'weight_decay': 0.00038885474549107875, 'beta_0': 0.87590841663911, 'beta_1': 0.9915846559534789, 'epsilon': 1.0682655758765541e-08, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 23, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 65 with value: 0.5401724205048527.
CUDA out of memory. Tried to allocate 1.59 GiB. GPU 0 has a total capacity of 44.56 GiB of which 160.69 MiB is free. Including non-PyTorch memory, this process has 44.40 GiB memory in use. Of the allocated memory 41.97 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 04:47:23,251] Trial 70 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9690597227552393, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 220, 'number_of_hidden_layers': 3, 'dropout_rate': 0.33807970105416385, 'global_pooling': 'sum', 'learning_rate': 0.00037487099736053717, 'weight_decay': 0.0006812043402811868, 'beta_0': 0.8897800274149433, 'beta_1': 0.9905562966846807, 'epsilon': 3.7827823927271207e-08, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 10, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 65 with value: 0.5401724205048527.
[I 2025-02-27 05:07:45,214] Trial 71 finished with value: 0.5430149698295269 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9809624640062955, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 199, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3586045984304013, 'global_pooling': 'sum', 'learning_rate': 0.0002577119972760756, 'weight_decay': 0.0004421386165581558, 'beta_0': 0.8994343726026681, 'beta_1': 0.9895460605110376, 'epsilon': 5.235994627422477e-08, 'balanced_loss': True, 'epochs': 107, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 05:30:09,059] Trial 72 finished with value: 0.5036608315282963 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9825537457955403, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 214, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35354628889026407, 'global_pooling': 'sum', 'learning_rate': 0.00011926356981208386, 'weight_decay': 0.0004700185794426779, 'beta_0': 0.8982487873435784, 'beta_1': 0.989170754192155, 'epsilon': 4.333918878413898e-08, 'balanced_loss': True, 'epochs': 106, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 05:51:45,838] Trial 73 finished with value: 0.44425802314535284 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9818615796787818, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 213, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3448862395782513, 'global_pooling': 'sum', 'learning_rate': 5.3183190651401576e-05, 'weight_decay': 0.0004648401423578746, 'beta_0': 0.8991963495652248, 'beta_1': 0.9891691746722265, 'epsilon': 5.903392861560085e-08, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 06:11:21,685] Trial 74 finished with value: 0.48512886964794694 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9912721918628673, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 222, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3570957193795542, 'global_pooling': 'sum', 'learning_rate': 0.0001474078079497757, 'weight_decay': 0.00031387205042198726, 'beta_0': 0.8949604305953612, 'beta_1': 0.988703735112013, 'epsilon': 4.891171546360747e-08, 'balanced_loss': True, 'epochs': 105, 'early_stopping_patience': 20, 'plateau_patience': 12, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 06:33:49,819] Trial 75 finished with value: 0.48252638375980494 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9723070696274652, 'batch_size': 35, 'attention_heads': 9, 'hidden_dimension': 203, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3000002866153095, 'global_pooling': 'sum', 'learning_rate': 0.0002663702093674614, 'weight_decay': 0.0006531753570615349, 'beta_0': 0.8884329591669216, 'beta_1': 0.9903640882752168, 'epsilon': 1.9128277117768827e-08, 'balanced_loss': True, 'epochs': 109, 'early_stopping_patience': 21, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 06:54:16,328] Trial 76 finished with value: 0.496660806845064 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9798753506112426, 'batch_size': 115, 'attention_heads': 7, 'hidden_dimension': 200, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3313542368224025, 'global_pooling': 'sum', 'learning_rate': 0.0005112125848875471, 'weight_decay': 0.00022238119097850735, 'beta_0': 0.8951778814578445, 'beta_1': 0.9896910957539435, 'epsilon': 4.035382371423776e-08, 'balanced_loss': True, 'epochs': 78, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 07:18:03,392] Trial 77 finished with value: 0.4961150282010312 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9770924908434587, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 199, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3336433135973468, 'global_pooling': 'sum', 'learning_rate': 8.596123570217368e-05, 'weight_decay': 0.00022604378115329037, 'beta_0': 0.8858465537495435, 'beta_1': 0.9908376164885945, 'epsilon': 4.226336728002298e-08, 'balanced_loss': True, 'epochs': 79, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 07:41:30,390] Trial 78 finished with value: 0.18657123048981844 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9849947684611889, 'batch_size': 116, 'attention_heads': 6, 'hidden_dimension': 175, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3171978860300518, 'global_pooling': 'max', 'learning_rate': 2.628493210954273e-05, 'weight_decay': 0.00044864017710236184, 'beta_0': 0.8812563946346398, 'beta_1': 0.9882970490918626, 'epsilon': 7.942541094251356e-08, 'balanced_loss': True, 'epochs': 64, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 08:00:48,589] Trial 79 finished with value: 0.4625876514456117 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9797391601301655, 'batch_size': 114, 'attention_heads': 7, 'hidden_dimension': 188, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3511164024805776, 'global_pooling': 'mean', 'learning_rate': 0.00015129021831671775, 'weight_decay': 0.00018656162203238412, 'beta_0': 0.8923220105910352, 'beta_1': 0.9895693023316283, 'epsilon': 1.4822515706952633e-08, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 18, 'plateau_patience': 12, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 08:23:19,788] Trial 80 finished with value: 0.5078804719033564 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9664825324812064, 'batch_size': 32, 'attention_heads': 8, 'hidden_dimension': 211, 'number_of_hidden_layers': 2, 'dropout_rate': 0.36276133432080737, 'global_pooling': 'sum', 'learning_rate': 0.0003208122879881355, 'weight_decay': 0.00026077870465406787, 'beta_0': 0.8977201199011274, 'beta_1': 0.9891339435604477, 'epsilon': 2.247342346777317e-08, 'balanced_loss': True, 'epochs': 103, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-27 08:23:21,473] Trial 81 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9702117611010422, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 209, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3644769034492003, 'global_pooling': 'sum', 'learning_rate': 0.00021775239579314396, 'weight_decay': 0.0002629932602761826, 'beta_0': 0.8749118501973662, 'beta_1': 0.9918598888882534, 'epsilon': 1.0201449312945128e-08, 'balanced_loss': True, 'epochs': 102, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 08:47:30,058] Trial 82 finished with value: 0.5199390356546967 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9628316507604814, 'batch_size': 38, 'attention_heads': 8, 'hidden_dimension': 200, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35083910896570086, 'global_pooling': 'sum', 'learning_rate': 0.000334612191328464, 'weight_decay': 0.00016358384409881038, 'beta_0': 0.8972653354002454, 'beta_1': 0.9872470159356128, 'epsilon': 2.096250610248489e-08, 'balanced_loss': True, 'epochs': 109, 'early_stopping_patience': 20, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 09:10:56,209] Trial 83 finished with value: 0.47614144310919887 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9625813431184623, 'batch_size': 39, 'attention_heads': 8, 'hidden_dimension': 233, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3500647641986779, 'global_pooling': 'sum', 'learning_rate': 0.0008542701289455149, 'weight_decay': 0.00039070990271096256, 'beta_0': 0.8996873369331896, 'beta_1': 0.9877345055085861, 'epsilon': 2.228513516820486e-08, 'balanced_loss': True, 'epochs': 110, 'early_stopping_patience': 13, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 09:29:31,426] Trial 84 finished with value: 0.5285098443692765 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9892831595903862, 'batch_size': 34, 'attention_heads': 8, 'hidden_dimension': 214, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3603557918597254, 'global_pooling': 'sum', 'learning_rate': 0.0003077535649074889, 'weight_decay': 0.00016434737817368875, 'beta_0': 0.8925183296216931, 'beta_1': 0.9888638180859302, 'epsilon': 5.9372169071423904e-08, 'balanced_loss': True, 'epochs': 119, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 09:50:16,277] Trial 85 finished with value: 0.49227020482421596 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9879458519786337, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 195, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37564908458551727, 'global_pooling': 'sum', 'learning_rate': 0.00029996834674997196, 'weight_decay': 0.00013949811277435704, 'beta_0': 0.8868607955466147, 'beta_1': 0.9871476322600963, 'epsilon': 1.6944346882806954e-08, 'balanced_loss': True, 'epochs': 129, 'early_stopping_patience': 16, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 10:03:43,965] Trial 86 finished with value: 0.048613780882782134 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9997259382631223, 'batch_size': 122, 'attention_heads': 7, 'hidden_dimension': 163, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3585201592967986, 'global_pooling': 'sum', 'learning_rate': 0.000364078422255885, 'weight_decay': 0.00015437261960148585, 'beta_0': 0.8837659837667924, 'beta_1': 0.9887471295069197, 'epsilon': 2.5959427252617182e-08, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 14, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 71 with value: 0.5430149698295269.
CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 43.46 GiB memory in use. Of the allocated memory 41.04 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 10:17:30,859] Trial 87 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9556212115704126, 'batch_size': 34, 'attention_heads': 9, 'hidden_dimension': 189, 'number_of_hidden_layers': 3, 'dropout_rate': 0.36300030057427174, 'global_pooling': 'sum', 'learning_rate': 0.0006099815923598506, 'weight_decay': 0.0003377314392909658, 'beta_0': 0.8927062082017678, 'beta_1': 0.9813734614095567, 'epsilon': 1.5313926319001814e-07, 'balanced_loss': True, 'epochs': 117, 'early_stopping_patience': 16, 'plateau_patience': 10, 'plateau_divider': 10}. Best is trial 71 with value: 0.5430149698295269.
[I 2025-02-27 10:41:50,242] Trial 88 finished with value: 0.5445577248127786 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.966119635745693, 'batch_size': 36, 'attention_heads': 11, 'hidden_dimension': 172, 'number_of_hidden_layers': 2, 'dropout_rate': 0.34239395960824026, 'global_pooling': 'mean', 'learning_rate': 0.00017615259145454097, 'weight_decay': 0.0002745707825413159, 'beta_0': 0.8923884434977924, 'beta_1': 0.9910120199153427, 'epsilon': 5.820530269947083e-08, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 88 with value: 0.5445577248127786.
CUDA out of memory. Tried to allocate 180.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 78.69 MiB is free. Including non-PyTorch memory, this process has 44.48 GiB memory in use. Of the allocated memory 42.65 GiB is allocated by PyTorch, and 697.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 10:51:45,497] Trial 89 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8687375166668955, 'batch_size': 38, 'attention_heads': 11, 'hidden_dimension': 156, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3439783231436353, 'global_pooling': 'max', 'learning_rate': 0.00018830477677525672, 'weight_decay': 0.0002890017117091019, 'beta_0': 0.8917872172485808, 'beta_1': 0.9912911441175971, 'epsilon': 1.0113113578837328e-07, 'balanced_loss': True, 'epochs': 126, 'early_stopping_patience': 17, 'plateau_patience': 10, 'plateau_divider': 9}. Best is trial 88 with value: 0.5445577248127786.
[I 2025-02-27 11:16:43,564] Trial 90 finished with value: 0.5267816078127201 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9649388087625036, 'batch_size': 42, 'attention_heads': 11, 'hidden_dimension': 172, 'number_of_hidden_layers': 2, 'dropout_rate': 0.31886750299907907, 'global_pooling': 'mean', 'learning_rate': 0.00011611698060097108, 'weight_decay': 0.00020531265512375296, 'beta_0': 0.8798507491642348, 'beta_1': 0.9924720961875655, 'epsilon': 6.13285502483576e-08, 'balanced_loss': True, 'epochs': 134, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 88 with value: 0.5445577248127786.
CUDA out of memory. Tried to allocate 264.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 32.69 MiB is free. Including non-PyTorch memory, this process has 44.52 GiB memory in use. Of the allocated memory 42.63 GiB is allocated by PyTorch, and 753.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 11:24:30,288] Trial 91 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8256374938534885, 'batch_size': 43, 'attention_heads': 12, 'hidden_dimension': 173, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3170044448874844, 'global_pooling': 'mean', 'learning_rate': 6.261698488432452e-05, 'weight_decay': 0.00023946296068707334, 'beta_0': 0.8733962697145241, 'beta_1': 0.993138984599889, 'epsilon': 5.9463982012376485e-08, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 18, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 88 with value: 0.5445577248127786.
[I 2025-02-27 11:45:39,839] Trial 92 finished with value: 0.5256629891394483 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.97509125501228, 'batch_size': 35, 'attention_heads': 11, 'hidden_dimension': 149, 'number_of_hidden_layers': 2, 'dropout_rate': 0.32851979726657354, 'global_pooling': 'mean', 'learning_rate': 0.0002855598226735926, 'weight_decay': 0.00019302353523124123, 'beta_0': 0.8789397409701385, 'beta_1': 0.990989634585533, 'epsilon': 6.909448033008456e-08, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 10}. Best is trial 88 with value: 0.5445577248127786.
slurmstepd: error: *** JOB 15030013 ON gpu015 CANCELLED AT 2025-02-27T12:02:49 DUE TO TIME LIMIT ***
