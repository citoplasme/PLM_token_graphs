[I 2025-02-19 01:09:28,787] Using an existing study with name 'Ohsumed-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-Unitary_Weights-0.0-0.0' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-19 01:31:00,765] Trial 46 finished with value: 0.46609389286758135 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9928005969573476, 'batch_size': 35, 'attention_heads': 6, 'hidden_dimension': 213, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4924998040360201, 'global_pooling': 'sum', 'learning_rate': 0.0002656836893415814, 'weight_decay': 2.5445246684419048e-05, 'beta_0': 0.8890336759511478, 'beta_1': 0.9850507561121042, 'epsilon': 3.0070162326071327e-06, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 17, 'plateau_patience': 11, 'plateau_divider': 9}. Best is trial 1 with value: 0.5806214206842546.
[I 2025-02-19 02:28:47,885] Trial 47 finished with value: 0.559243090381483 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9818174560038898, 'batch_size': 32, 'attention_heads': 9, 'hidden_dimension': 244, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5280196788125456, 'global_pooling': 'max', 'learning_rate': 5.0724741895995576e-05, 'weight_decay': 4.7230249343848574e-05, 'beta_0': 0.8719002899138656, 'beta_1': 0.9869808923532087, 'epsilon': 7.174139236758935e-07, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 19, 'plateau_patience': 10, 'plateau_divider': 8}. Best is trial 1 with value: 0.5806214206842546.
[I 2025-02-19 02:41:38,457] Trial 48 finished with value: 0.060520989681913974 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9997530899591419, 'batch_size': 41, 'attention_heads': 6, 'hidden_dimension': 220, 'number_of_hidden_layers': 2, 'dropout_rate': 0.43603678301229837, 'global_pooling': 'sum', 'learning_rate': 0.00020269106019950176, 'weight_decay': 8.985401592231217e-05, 'beta_0': 0.8819285326343245, 'beta_1': 0.9886338127951154, 'epsilon': 3.704177745631667e-07, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 7}. Best is trial 1 with value: 0.5806214206842546.
[I 2025-02-19 03:02:23,192] Trial 49 finished with value: 0.5024561143315655 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9805437424028444, 'batch_size': 39, 'attention_heads': 7, 'hidden_dimension': 196, 'number_of_hidden_layers': 3, 'dropout_rate': 0.568588293473148, 'global_pooling': 'mean', 'learning_rate': 0.0021859681875689026, 'weight_decay': 8.289090752009089e-06, 'beta_0': 0.8712138213286957, 'beta_1': 0.9807398505047836, 'epsilon': 1.3030029679889261e-06, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 19, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 1 with value: 0.5806214206842546.
[I 2025-02-19 03:21:37,574] Trial 50 finished with value: 0.5926815734153034 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9462497566936152, 'batch_size': 44, 'attention_heads': 5, 'hidden_dimension': 232, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4853063106645305, 'global_pooling': 'max', 'learning_rate': 0.0003895852682376518, 'weight_decay': 7.793616922748033e-05, 'beta_0': 0.8961521030633705, 'beta_1': 0.9850073344456693, 'epsilon': 2.8353459605003708e-08, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 50 with value: 0.5926815734153034.
CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 42.51 GiB memory in use. Of the allocated memory 39.28 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 03:34:37,100] Trial 51 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9492315105371557, 'batch_size': 83, 'attention_heads': 5, 'hidden_dimension': 253, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3196127842558901, 'global_pooling': 'max', 'learning_rate': 0.00013126169764259384, 'weight_decay': 3.4176396561825864e-05, 'beta_0': 0.8981752259338325, 'beta_1': 0.983978481215101, 'epsilon': 3.175418149980879e-08, 'balanced_loss': True, 'epochs': 99, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 50 with value: 0.5926815734153034.
[I 2025-02-19 03:50:41,916] Trial 52 finished with value: 0.575676007800242 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9759954174255362, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 229, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5079517276779748, 'global_pooling': 'max', 'learning_rate': 0.00039079691759804925, 'weight_decay': 7.754519018443237e-05, 'beta_0': 0.8920979459285414, 'beta_1': 0.9849557416767325, 'epsilon': 1.402916162194375e-07, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 50 with value: 0.5926815734153034.
[I 2025-02-19 04:07:37,947] Trial 53 finished with value: 0.6034435336622864 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9663494948464529, 'batch_size': 46, 'attention_heads': 5, 'hidden_dimension': 228, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5080383406238922, 'global_pooling': 'max', 'learning_rate': 0.00040540615931591677, 'weight_decay': 4.868387868634513e-05, 'beta_0': 0.890745677384094, 'beta_1': 0.9906787232858661, 'epsilon': 2.7119856449899437e-08, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 14, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 53 with value: 0.6034435336622864.
[I 2025-02-19 04:25:30,706] Trial 54 finished with value: 0.6044176441627539 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9585151304442037, 'batch_size': 43, 'attention_heads': 5, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5320287309575688, 'global_pooling': 'max', 'learning_rate': 0.00039287306338722463, 'weight_decay': 4.842489453112827e-05, 'beta_0': 0.8903507003135536, 'beta_1': 0.9896912012451299, 'epsilon': 2.0537380612784764e-08, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 1.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 43.42 GiB memory in use. Of the allocated memory 40.93 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 04:38:11,606] Trial 55 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9408712414021303, 'batch_size': 42, 'attention_heads': 5, 'hidden_dimension': 231, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5356193935779241, 'global_pooling': 'max', 'learning_rate': 7.581256762521692e-05, 'weight_decay': 5.283790250071106e-05, 'beta_0': 0.8926911375260893, 'beta_1': 0.9902274048837368, 'epsilon': 2.4899765565926673e-08, 'balanced_loss': False, 'epochs': 93, 'early_stopping_patience': 15, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 1.93 GiB. GPU 0 has a total capacity of 44.56 GiB of which 744.69 MiB is free. Including non-PyTorch memory, this process has 43.83 GiB memory in use. Of the allocated memory 39.01 GiB is allocated by PyTorch, and 3.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 04:50:47,921] Trial 56 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.962286815815674, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 247, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5056626354686946, 'global_pooling': 'max', 'learning_rate': 0.00047801716426872315, 'weight_decay': 2.360585708158479e-05, 'beta_0': 0.8931948129845985, 'beta_1': 0.9910492543478352, 'epsilon': 9.503266153329647e-08, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 44.56 GiB of which 32.69 MiB is free. Including non-PyTorch memory, this process has 44.52 GiB memory in use. Of the allocated memory 42.60 GiB is allocated by PyTorch, and 790.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 05:03:23,044] Trial 57 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9267614113661927, 'batch_size': 44, 'attention_heads': 6, 'hidden_dimension': 230, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5610002783421334, 'global_pooling': 'max', 'learning_rate': 0.00035511752276349613, 'weight_decay': 7.113846622736206e-05, 'beta_0': 0.8869656401751745, 'beta_1': 0.9902749355702517, 'epsilon': 2.03382066116752e-08, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 05:21:30,200] Trial 58 finished with value: 0.579183533916794 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9553093909412592, 'batch_size': 33, 'attention_heads': 10, 'hidden_dimension': 235, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5106003413007321, 'global_pooling': 'max', 'learning_rate': 0.000228665617933174, 'weight_decay': 1.6998488920876823e-05, 'beta_0': 0.8933252900108964, 'beta_1': 0.9928296205386875, 'epsilon': 6.336189129396455e-08, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 05:42:02,809] Trial 59 finished with value: 0.5644035940747341 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9572357011364934, 'batch_size': 33, 'attention_heads': 10, 'hidden_dimension': 237, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5197063038197698, 'global_pooling': 'max', 'learning_rate': 0.0001505427095636556, 'weight_decay': 1.9243594385025e-05, 'beta_0': 0.8836423662995498, 'beta_1': 0.992745586843739, 'epsilon': 6.462434759718165e-08, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 14, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 5.63 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 42.94 GiB memory in use. Of the allocated memory 41.19 GiB is allocated by PyTorch, and 609.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 05:54:38,936] Trial 60 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9320642412062499, 'batch_size': 122, 'attention_heads': 10, 'hidden_dimension': 210, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5854974241367045, 'global_pooling': 'max', 'learning_rate': 6.889188787626341e-05, 'weight_decay': 4.420308933412844e-05, 'beta_0': 0.8996086874950135, 'beta_1': 0.9929463081949101, 'epsilon': 3.7255726096608685e-08, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 1.67 GiB. GPU 0 has a total capacity of 44.56 GiB of which 552.69 MiB is free. Including non-PyTorch memory, this process has 44.01 GiB memory in use. Of the allocated memory 42.49 GiB is allocated by PyTorch, and 378.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 06:07:13,188] Trial 61 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9427225547200919, 'batch_size': 50, 'attention_heads': 8, 'hidden_dimension': 248, 'number_of_hidden_layers': 1, 'dropout_rate': 0.47985032305378844, 'global_pooling': 'max', 'learning_rate': 2.7273998980834613e-05, 'weight_decay': 1.2727135546699677e-05, 'beta_0': 0.8793106051855617, 'beta_1': 0.9915643233274138, 'epsilon': 1.6800025816910592e-08, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 16, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 06:28:37,869] Trial 62 finished with value: 0.5846678011037837 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9897922803387524, 'batch_size': 45, 'attention_heads': 16, 'hidden_dimension': 228, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5052258038998145, 'global_pooling': 'max', 'learning_rate': 0.00019585474879423514, 'weight_decay': 2.938241719759326e-05, 'beta_0': 0.8923363969139522, 'beta_1': 0.9942383436157521, 'epsilon': 1.058141617467461e-08, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 15, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 06:46:18,287] Trial 63 finished with value: 0.5780429799441684 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9908699740288107, 'batch_size': 36, 'attention_heads': 11, 'hidden_dimension': 223, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5303912319451423, 'global_pooling': 'max', 'learning_rate': 0.0002216730493777118, 'weight_decay': 3.456870316586897e-05, 'beta_0': 0.894868698071596, 'beta_1': 0.9949774870811773, 'epsilon': 1.072482278024919e-08, 'balanced_loss': False, 'epochs': 124, 'early_stopping_patience': 14, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 07:05:14,528] Trial 64 finished with value: 0.5707126039218635 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9890958165297057, 'batch_size': 36, 'attention_heads': 13, 'hidden_dimension': 240, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5349965945422578, 'global_pooling': 'max', 'learning_rate': 0.0002150223591032778, 'weight_decay': 2.8440748118779584e-05, 'beta_0': 0.8954138963359831, 'beta_1': 0.9945014548529897, 'epsilon': 1.3445836703655883e-08, 'balanced_loss': False, 'epochs': 141, 'early_stopping_patience': 12, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 07:29:08,920] Trial 65 finished with value: 0.5502959777132579 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9943338253293129, 'batch_size': 46, 'attention_heads': 16, 'hidden_dimension': 256, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5510292349549133, 'global_pooling': 'max', 'learning_rate': 0.00012592370056821453, 'weight_decay': 1.813358776508468e-05, 'beta_0': 0.888879972279838, 'beta_1': 0.9941176735202841, 'epsilon': 2.360607474649904e-08, 'balanced_loss': False, 'epochs': 124, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 07:50:03,723] Trial 66 finished with value: 0.5711816310081976 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9687977608228723, 'batch_size': 55, 'attention_heads': 12, 'hidden_dimension': 224, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5204056341869076, 'global_pooling': 'max', 'learning_rate': 0.0002130581753431502, 'weight_decay': 3.745630732166453e-05, 'beta_0': 0.8965017423170717, 'beta_1': 0.9957410505567011, 'epsilon': 1.0419473395011748e-08, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 08:10:39,097] Trial 67 finished with value: 0.5721966895579889 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9848547236127305, 'batch_size': 38, 'attention_heads': 15, 'hidden_dimension': 232, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5003877256853952, 'global_pooling': 'max', 'learning_rate': 0.001052993123427486, 'weight_decay': 1.4115497628824779e-05, 'beta_0': 0.8864947004481882, 'beta_1': 0.9936792429953178, 'epsilon': 5.645302579972609e-08, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 15, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.20 GiB is free. Including non-PyTorch memory, this process has 42.35 GiB memory in use. Of the allocated memory 40.36 GiB is allocated by PyTorch, and 863.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 08:23:55,544] Trial 68 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9463371177808613, 'batch_size': 35, 'attention_heads': 15, 'hidden_dimension': 219, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5271966450266312, 'global_pooling': 'max', 'learning_rate': 0.0005026277644375459, 'weight_decay': 6.427866451896479e-06, 'beta_0': 0.8916847464567108, 'beta_1': 0.9954742539925363, 'epsilon': 1.8353957578166402e-08, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 13, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 08:48:37,148] Trial 69 finished with value: 0.5242163261131038 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9564192433758089, 'batch_size': 50, 'attention_heads': 11, 'hidden_dimension': 85, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48634806253283197, 'global_pooling': 'max', 'learning_rate': 3.8990543927029595e-05, 'weight_decay': 3.132408257484564e-05, 'beta_0': 0.89986087646536, 'beta_1': 0.9933520816145512, 'epsilon': 3.1342997869681246e-08, 'balanced_loss': True, 'epochs': 82, 'early_stopping_patience': 14, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 106.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 42.44 GiB is allocated by PyTorch, and 881.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 09:01:55,441] Trial 70 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9065141209840462, 'batch_size': 111, 'attention_heads': 12, 'hidden_dimension': 237, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5095196982833541, 'global_pooling': 'max', 'learning_rate': 6.62643079471653e-05, 'weight_decay': 1.429397426197647e-05, 'beta_0': 0.8818219750983894, 'beta_1': 0.9922288214085544, 'epsilon': 4.26961130674177e-08, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 09:27:15,310] Trial 71 finished with value: 0.5831001050770391 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9708605690677623, 'batch_size': 55, 'attention_heads': 14, 'hidden_dimension': 204, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5453946796023776, 'global_pooling': 'max', 'learning_rate': 0.00015893702849425708, 'weight_decay': 3.9401994737392843e-05, 'beta_0': 0.8895659195788153, 'beta_1': 0.9973241894967487, 'epsilon': 8.591593862852739e-08, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 09:59:02,659] Trial 72 finished with value: 0.5861398249030504 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9719671649001963, 'batch_size': 46, 'attention_heads': 14, 'hidden_dimension': 205, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5548035614942474, 'global_pooling': 'max', 'learning_rate': 9.42206939671857e-05, 'weight_decay': 5.610868516629607e-05, 'beta_0': 0.8948039294825427, 'beta_1': 0.9964731710009204, 'epsilon': 7.78447435531833e-08, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 2.41 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.00 GiB is free. Including non-PyTorch memory, this process has 42.56 GiB memory in use. Of the allocated memory 40.06 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 10:14:51,301] Trial 73 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9697211303051125, 'batch_size': 54, 'attention_heads': 15, 'hidden_dimension': 206, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5475781812340755, 'global_pooling': 'max', 'learning_rate': 9.560028811597308e-05, 'weight_decay': 5.876563335637689e-05, 'beta_0': 0.8893585305909104, 'beta_1': 0.9981438447075327, 'epsilon': 1.2093414714659696e-07, 'balanced_loss': False, 'epochs': 103, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 2.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.93 GiB is free. Including non-PyTorch memory, this process has 42.62 GiB memory in use. Of the allocated memory 39.80 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 10:27:30,188] Trial 74 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9623142999543002, 'batch_size': 46, 'attention_heads': 14, 'hidden_dimension': 249, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5638431913579767, 'global_pooling': 'max', 'learning_rate': 0.00015212643232222163, 'weight_decay': 4.854553116451749e-05, 'beta_0': 0.8902316486699707, 'beta_1': 0.997308394821985, 'epsilon': 6.6591178465923e-08, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 11:15:23,331] Trial 75 finished with value: 0.512328196750069 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9755644601903122, 'batch_size': 57, 'attention_heads': 16, 'hidden_dimension': 200, 'number_of_hidden_layers': 1, 'dropout_rate': 0.587512769402483, 'global_pooling': 'max', 'learning_rate': 1.6655102725508075e-05, 'weight_decay': 4.0381080591400834e-05, 'beta_0': 0.8844924408758087, 'beta_1': 0.9963988642645819, 'epsilon': 8.185345679230186e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 11:34:10,193] Trial 76 finished with value: 0.5786022100396166 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9864427547852221, 'batch_size': 49, 'attention_heads': 14, 'hidden_dimension': 191, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5542525509007328, 'global_pooling': 'max', 'learning_rate': 0.00010951104136319912, 'weight_decay': 2.090157676965908e-05, 'beta_0': 0.8764787103228581, 'beta_1': 0.9970867119592183, 'epsilon': 2.8688093232285036e-08, 'balanced_loss': False, 'epochs': 93, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 2.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 43.10 GiB memory in use. Of the allocated memory 41.68 GiB is allocated by PyTorch, and 280.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 11:46:53,028] Trial 77 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9337257677918103, 'batch_size': 45, 'attention_heads': 13, 'hidden_dimension': 208, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5767325208133022, 'global_pooling': 'max', 'learning_rate': 6.215383016207512e-05, 'weight_decay': 2.8363012604996422e-05, 'beta_0': 0.8958100709480036, 'beta_1': 0.9978059153707267, 'epsilon': 4.529879594499769e-08, 'balanced_loss': False, 'epochs': 80, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 54 with value: 0.6044176441627539.
CUDA out of memory. Tried to allocate 2.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.06 GiB is free. Including non-PyTorch memory, this process has 42.50 GiB memory in use. Of the allocated memory 38.36 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 11:59:37,926] Trial 78 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9564908232594606, 'batch_size': 53, 'attention_heads': 14, 'hidden_dimension': 228, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5421210550557626, 'global_pooling': 'max', 'learning_rate': 0.0003605316020312997, 'weight_decay': 0.00010977662172035916, 'beta_0': 0.881344561509367, 'beta_1': 0.9961276825099931, 'epsilon': 1.7845674844086782e-07, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 54 with value: 0.6044176441627539.
[I 2025-02-19 12:24:50,602] Trial 79 finished with value: 0.6105217845937871 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.972954451296498, 'batch_size': 42, 'attention_heads': 15, 'hidden_dimension': 217, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5156288795279413, 'global_pooling': 'max', 'learning_rate': 0.00017463520252594355, 'weight_decay': 0.00024178205147309733, 'beta_0': 0.8864723743710043, 'beta_1': 0.9907624472189338, 'epsilon': 1.632777629021694e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 79 with value: 0.6105217845937871.
[I 2025-02-19 12:52:23,017] Trial 80 finished with value: 0.5914349108962779 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9723782063834019, 'batch_size': 51, 'attention_heads': 16, 'hidden_dimension': 215, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4984978005232828, 'global_pooling': 'max', 'learning_rate': 0.00014828853755310866, 'weight_decay': 0.00025447982814319746, 'beta_0': 0.88687086613163, 'beta_1': 0.9892236706190917, 'epsilon': 1.4070055830673782e-08, 'balanced_loss': True, 'epochs': 70, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
[I 2025-02-19 13:19:59,601] Trial 81 finished with value: 0.5728603614490073 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9729301582701202, 'batch_size': 51, 'attention_heads': 16, 'hidden_dimension': 214, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49672092892498537, 'global_pooling': 'max', 'learning_rate': 0.00015965364650006182, 'weight_decay': 0.0002324956793322175, 'beta_0': 0.8867047402782791, 'beta_1': 0.9892958420621891, 'epsilon': 1.4804822041551402e-08, 'balanced_loss': True, 'epochs': 70, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 42.64 GiB memory in use. Of the allocated memory 39.92 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 13:33:04,953] Trial 82 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9654187405584088, 'batch_size': 48, 'attention_heads': 15, 'hidden_dimension': 197, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5160638769310342, 'global_pooling': 'max', 'learning_rate': 0.0001176252719404632, 'weight_decay': 0.00044230912495725293, 'beta_0': 0.8866028521010116, 'beta_1': 0.9906568245179131, 'epsilon': 2.0603045298623514e-08, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
[I 2025-02-19 13:57:38,054] Trial 83 finished with value: 0.5946732115831483 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9717517663665851, 'batch_size': 42, 'attention_heads': 16, 'hidden_dimension': 220, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4644095270889577, 'global_pooling': 'max', 'learning_rate': 0.00017756980760107172, 'weight_decay': 0.0006884319938223727, 'beta_0': 0.8974925423860387, 'beta_1': 0.9886196155259619, 'epsilon': 1.4297668390291798e-08, 'balanced_loss': True, 'epochs': 86, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
[I 2025-02-19 14:23:14,606] Trial 84 finished with value: 0.5748774237501992 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9717266157816093, 'batch_size': 41, 'attention_heads': 16, 'hidden_dimension': 216, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4639220669645539, 'global_pooling': 'max', 'learning_rate': 9.286313041548141e-05, 'weight_decay': 0.0005721359943765516, 'beta_0': 0.8976047737347255, 'beta_1': 0.989713750540991, 'epsilon': 1.3081736015681617e-08, 'balanced_loss': True, 'epochs': 83, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
CUDA out of memory. Tried to allocate 140.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 138.69 MiB is free. Including non-PyTorch memory, this process has 44.42 GiB memory in use. Of the allocated memory 42.36 GiB is allocated by PyTorch, and 929.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 14:32:03,323] Trial 85 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8639604558082631, 'batch_size': 44, 'attention_heads': 16, 'hidden_dimension': 204, 'number_of_hidden_layers': 1, 'dropout_rate': 0.476088587479237, 'global_pooling': 'max', 'learning_rate': 0.0006266657156024131, 'weight_decay': 0.0007300338145665485, 'beta_0': 0.8890843791255624, 'beta_1': 0.9883622997146431, 'epsilon': 1.56269711108542e-08, 'balanced_loss': True, 'epochs': 106, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
CUDA out of memory. Tried to allocate 2.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.65 GiB is free. Including non-PyTorch memory, this process has 42.90 GiB memory in use. Of the allocated memory 39.68 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 14:44:35,843] Trial 86 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9624895276916664, 'batch_size': 52, 'attention_heads': 15, 'hidden_dimension': 222, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48773671432064936, 'global_pooling': 'max', 'learning_rate': 0.0002920891294769615, 'weight_decay': 0.0003323048139785311, 'beta_0': 0.8943257766603281, 'beta_1': 0.9874377037659304, 'epsilon': 2.426981061169545e-08, 'balanced_loss': True, 'epochs': 90, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
CUDA out of memory. Tried to allocate 2.28 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1004.69 MiB is free. Including non-PyTorch memory, this process has 43.57 GiB memory in use. Of the allocated memory 42.00 GiB is allocated by PyTorch, and 436.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 14:57:14,756] Trial 87 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9501909429121851, 'batch_size': 47, 'attention_heads': 15, 'hidden_dimension': 210, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5235452439317693, 'global_pooling': 'max', 'learning_rate': 0.00016200486237778918, 'weight_decay': 0.00015420251856372282, 'beta_0': 0.875724409734473, 'beta_1': 0.9890661096700177, 'epsilon': 3.383602855086093e-08, 'balanced_loss': True, 'epochs': 86, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 79 with value: 0.6105217845937871.
[I 2025-02-19 15:20:10,647] Trial 88 finished with value: 0.6151604306560179 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9785791953031678, 'batch_size': 39, 'attention_heads': 16, 'hidden_dimension': 225, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5034070487431913, 'global_pooling': 'max', 'learning_rate': 0.000406742112513865, 'weight_decay': 5.7752513573094674e-05, 'beta_0': 0.8838696652396026, 'beta_1': 0.9910711548122634, 'epsilon': 1.8241221764846338e-08, 'balanced_loss': True, 'epochs': 102, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 88 with value: 0.6151604306560179.
[I 2025-02-19 15:43:09,751] Trial 89 finished with value: 0.5822889998755038 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.983882754663692, 'batch_size': 39, 'attention_heads': 16, 'hidden_dimension': 241, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5030385983158351, 'global_pooling': 'max', 'learning_rate': 0.001045965763893364, 'weight_decay': 6.866266106452083e-05, 'beta_0': 0.8841045454090176, 'beta_1': 0.9918754794979856, 'epsilon': 2.1024715417342884e-08, 'balanced_loss': True, 'epochs': 96, 'early_stopping_patience': 16, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 88 with value: 0.6151604306560179.
[I 2025-02-19 16:04:13,149] Trial 90 finished with value: 0.603950796264911 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.979693193471084, 'batch_size': 42, 'attention_heads': 16, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4685270831132478, 'global_pooling': 'max', 'learning_rate': 0.0004019650037791666, 'weight_decay': 0.0002425280652117742, 'beta_0': 0.8797022893343334, 'beta_1': 0.9908286852269544, 'epsilon': 1.0518120056553479e-08, 'balanced_loss': True, 'epochs': 77, 'early_stopping_patience': 15, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 88 with value: 0.6151604306560179.
[I 2025-02-19 16:21:21,949] Trial 91 finished with value: 0.6015064433791973 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9767603990950835, 'batch_size': 42, 'attention_heads': 15, 'hidden_dimension': 122, 'number_of_hidden_layers': 1, 'dropout_rate': 0.473114246450899, 'global_pooling': 'max', 'learning_rate': 0.0005212374881253555, 'weight_decay': 0.0002361840535021466, 'beta_0': 0.8804554171085668, 'beta_1': 0.9911788592643556, 'epsilon': 1.644771142393096e-08, 'balanced_loss': True, 'epochs': 78, 'early_stopping_patience': 16, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 88 with value: 0.6151604306560179.
[I 2025-02-19 16:38:42,486] Trial 92 finished with value: 0.5735363534906401 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9769987519220632, 'batch_size': 42, 'attention_heads': 15, 'hidden_dimension': 106, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49423187347680725, 'global_pooling': 'max', 'learning_rate': 0.0004036849104748287, 'weight_decay': 0.00023346861665481661, 'beta_0': 0.8800149026431723, 'beta_1': 0.9909966939115303, 'epsilon': 1.650394127020296e-08, 'balanced_loss': True, 'epochs': 78, 'early_stopping_patience': 16, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 88 with value: 0.6151604306560179.
[I 2025-02-19 16:55:57,192] Trial 93 finished with value: 0.5849992107666968 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9792609378299474, 'batch_size': 37, 'attention_heads': 16, 'hidden_dimension': 131, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48198617600336613, 'global_pooling': 'max', 'learning_rate': 0.0007690113976435071, 'weight_decay': 0.00025221985477491066, 'beta_0': 0.8846384278447661, 'beta_1': 0.9887736308268467, 'epsilon': 1.2451651352052025e-08, 'balanced_loss': True, 'epochs': 71, 'early_stopping_patience': 15, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 88 with value: 0.6151604306560179.
slurmstepd: error: *** JOB 14932820 ON gpu009 CANCELLED AT 2025-02-19T17:08:43 DUE TO TIME LIMIT ***
