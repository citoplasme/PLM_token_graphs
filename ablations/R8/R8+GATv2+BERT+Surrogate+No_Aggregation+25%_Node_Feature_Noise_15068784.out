[I 2025-03-01 09:02:43,278] Using an existing study with name 'R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.25' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
[I 2025-03-01 09:16:25,093] Trial 310 finished with value: 0.9417602369172587 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8096749738585696, 'batch_size': 110, 'attention_heads': 8, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5645977664640984, 'global_pooling': 'max', 'learning_rate': 0.0013201450802698092, 'weight_decay': 0.0005858902160985224, 'beta_0': 0.8705751270006783, 'beta_1': 0.9946473483065418, 'epsilon': 5.795873516955976e-07, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 09:31:07,091] Trial 311 finished with value: 0.9401143837553402 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8114290471841052, 'batch_size': 112, 'attention_heads': 8, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5693051440072697, 'global_pooling': 'max', 'learning_rate': 0.0015006195281769019, 'weight_decay': 0.0005774197111400489, 'beta_0': 0.8705462210084289, 'beta_1': 0.9949450517685602, 'epsilon': 6.663419208530292e-06, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 09:44:39,564] Trial 312 finished with value: 0.934673152647918 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8059602535052871, 'batch_size': 102, 'attention_heads': 8, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5702363099996062, 'global_pooling': 'max', 'learning_rate': 0.0014699917625735836, 'weight_decay': 0.0005858573655941425, 'beta_0': 0.870305344894169, 'beta_1': 0.9948648598839533, 'epsilon': 5.378756905604634e-07, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 09:56:22,746] Trial 313 finished with value: 0.9224519919487341 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8073047715984296, 'batch_size': 116, 'attention_heads': 8, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5749317285970453, 'global_pooling': 'max', 'learning_rate': 0.001611096730779113, 'weight_decay': 0.0006995905237986201, 'beta_0': 0.870741627742405, 'beta_1': 0.9946597625968204, 'epsilon': 7.906408534286882e-06, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 41.92 GiB is allocated by PyTorch, and 481.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 10:04:12,219] Trial 314 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7831697592304898, 'batch_size': 108, 'attention_heads': 8, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5654777770646249, 'global_pooling': 'mean', 'learning_rate': 0.0013025656011244732, 'weight_decay': 0.0005703249238119313, 'beta_0': 0.8685458425675442, 'beta_1': 0.9931927027610586, 'epsilon': 4.241105478086615e-07, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 606.69 MiB is free. Including non-PyTorch memory, this process has 43.96 GiB memory in use. Of the allocated memory 42.61 GiB is allocated by PyTorch, and 208.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 10:11:34,513] Trial 315 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7970232911445988, 'batch_size': 109, 'attention_heads': 14, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5677260386867182, 'global_pooling': 'max', 'learning_rate': 0.07612358265851497, 'weight_decay': 0.0008233039544828743, 'beta_0': 0.8715335798501462, 'beta_1': 0.9942416837246826, 'epsilon': 5.404359023703554e-06, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 10:24:14,302] Trial 316 finished with value: 0.9189181447735646 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8098077943115252, 'batch_size': 101, 'attention_heads': 8, 'hidden_dimension': 38, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5937546123665083, 'global_pooling': 'max', 'learning_rate': 0.0019194343534903384, 'weight_decay': 0.0007220077822315376, 'beta_0': 0.87746158088745, 'beta_1': 0.9945336756515976, 'epsilon': 5.857706613850782e-06, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 958.69 MiB is free. Including non-PyTorch memory, this process has 43.62 GiB memory in use. Of the allocated memory 41.60 GiB is allocated by PyTorch, and 886.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 10:32:08,585] Trial 317 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8012530277223269, 'batch_size': 112, 'attention_heads': 8, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5591075058385391, 'global_pooling': 'max', 'learning_rate': 0.0012604177226031722, 'weight_decay': 0.0006258861576360292, 'beta_0': 0.8731266666282567, 'beta_1': 0.9948520565642812, 'epsilon': 3.880830956570901e-06, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 43.03 GiB memory in use. Of the allocated memory 40.23 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 10:39:41,847] Trial 318 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8168909493185674, 'batch_size': 229, 'attention_heads': 8, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5661807796588849, 'global_pooling': 'max', 'learning_rate': 0.0010137067775149989, 'weight_decay': 0.0005610824307125747, 'beta_0': 0.8691492188107777, 'beta_1': 0.9954747199315142, 'epsilon': 8.002409691029504e-07, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 10:51:24,085] Trial 319 finished with value: 0.9414623021022168 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8129982456647744, 'batch_size': 95, 'attention_heads': 8, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5766255329907162, 'global_pooling': 'max', 'learning_rate': 0.0015595923678565456, 'weight_decay': 0.0008691403099008674, 'beta_0': 0.8671363925254371, 'beta_1': 0.9911513542382624, 'epsilon': 9.044326606968489e-07, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 11:05:40,231] Trial 320 finished with value: 0.9387386524642607 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8041978144625818, 'batch_size': 95, 'attention_heads': 8, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5831359917261739, 'global_pooling': 'max', 'learning_rate': 0.002295799870910567, 'weight_decay': 0.0008745170203917944, 'beta_0': 0.8665021850274096, 'beta_1': 0.9904346928743263, 'epsilon': 9.978538519851226e-07, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 972.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 474.69 MiB is free. Including non-PyTorch memory, this process has 44.09 GiB memory in use. Of the allocated memory 41.96 GiB is allocated by PyTorch, and 1005.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 11:13:14,186] Trial 321 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7881712413800364, 'batch_size': 97, 'attention_heads': 8, 'hidden_dimension': 43, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5806393390747076, 'global_pooling': 'max', 'learning_rate': 0.0023041763228542557, 'weight_decay': 0.0009331404163668316, 'beta_0': 0.8668494974477116, 'beta_1': 0.9910219868996395, 'epsilon': 6.28443531216657e-07, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 44.56 GiB of which 176.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 42.67 GiB is allocated by PyTorch, and 572.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 11:21:01,065] Trial 322 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7979771610074435, 'batch_size': 103, 'attention_heads': 8, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5882709976045096, 'global_pooling': 'max', 'learning_rate': 0.001655832196247529, 'weight_decay': 0.0008742111159750538, 'beta_0': 0.8652676345809757, 'beta_1': 0.9911447220553284, 'epsilon': 1.0612437211854905e-06, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 11:33:07,968] Trial 323 finished with value: 0.9197037592848938 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8094111251596358, 'batch_size': 94, 'attention_heads': 7, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.585773263170492, 'global_pooling': 'max', 'learning_rate': 0.0021881569969118154, 'weight_decay': 0.0009908900710423154, 'beta_0': 0.8704007076142276, 'beta_1': 0.9902423865730714, 'epsilon': 7.302079860602799e-07, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 11:44:54,465] Trial 324 finished with value: 0.927981973634004 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8050584219706319, 'batch_size': 98, 'attention_heads': 8, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5785102347383416, 'global_pooling': 'max', 'learning_rate': 0.001946576925837937, 'weight_decay': 0.0008116426132541017, 'beta_0': 0.8655111684124465, 'beta_1': 0.9904535845900111, 'epsilon': 8.821333455125664e-07, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 11:58:57,265] Trial 325 finished with value: 0.9296610957001425 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.813571041011697, 'batch_size': 103, 'attention_heads': 8, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5926660713337815, 'global_pooling': 'max', 'learning_rate': 0.002818528056052675, 'weight_decay': 0.0007962746989478552, 'beta_0': 0.8689504351795471, 'beta_1': 0.9914550881457919, 'epsilon': 5.339342454890809e-07, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 12:11:22,724] Trial 326 finished with value: 0.9318682682294008 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7995503415990174, 'batch_size': 93, 'attention_heads': 8, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5757970779089834, 'global_pooling': 'mean', 'learning_rate': 0.0013252102612560488, 'weight_decay': 0.0008578792345454314, 'beta_0': 0.8713362525387272, 'beta_1': 0.9916685071969286, 'epsilon': 9.417229280133206e-07, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 12:25:26,954] Trial 327 finished with value: 0.9326695859237359 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8258743986139075, 'batch_size': 110, 'attention_heads': 8, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5527412684978554, 'global_pooling': 'max', 'learning_rate': 0.0011044807796902695, 'weight_decay': 0.0007320648324430056, 'beta_0': 0.8728353570547618, 'beta_1': 0.9907708957560707, 'epsilon': 1.1701322940087489e-06, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-03-01 12:38:13,381] Trial 328 finished with value: 0.9524112102417622 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8118612971423742, 'batch_size': 89, 'attention_heads': 7, 'hidden_dimension': 34, 'number_of_hidden_layers': 2, 'dropout_rate': 0.570711519474897, 'global_pooling': 'max', 'learning_rate': 0.0009603185232741348, 'weight_decay': 1.029904476432445e-06, 'beta_0': 0.8666410211129291, 'beta_1': 0.9906333122788743, 'epsilon': 1.6987676129930815e-06, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 44.56 GiB of which 276.69 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 42.89 GiB is allocated by PyTorch, and 244.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 12:45:42,166] Trial 329 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8146021894536466, 'batch_size': 90, 'attention_heads': 7, 'hidden_dimension': 110, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5799970904074361, 'global_pooling': 'max', 'learning_rate': 0.0015761617762423486, 'weight_decay': 1.2000589846647985e-06, 'beta_0': 0.8674372782705025, 'beta_1': 0.9904883543756043, 'epsilon': 1.6994252753030795e-06, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 12:57:09,148] Trial 330 finished with value: 0.9353751505676633 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7942853040119355, 'batch_size': 97, 'attention_heads': 7, 'hidden_dimension': 35, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5718687422262505, 'global_pooling': 'max', 'learning_rate': 0.0009863990131887053, 'weight_decay': 2.7037093377624727e-06, 'beta_0': 0.8700447512062333, 'beta_1': 0.9897357026677588, 'epsilon': 6.6601066429467475e-06, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 13:09:54,462] Trial 331 finished with value: 0.9277996512011935 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8095751947008852, 'batch_size': 89, 'attention_heads': 8, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5744274654716496, 'global_pooling': 'max', 'learning_rate': 0.0008753509207550495, 'weight_decay': 1.845672763110208e-05, 'beta_0': 0.8662888368063402, 'beta_1': 0.9910034144575498, 'epsilon': 1.3542444836645048e-06, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 13:22:33,345] Trial 332 finished with value: 0.9423026881756814 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.80477298939581, 'batch_size': 83, 'attention_heads': 8, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5830812563568674, 'global_pooling': 'max', 'learning_rate': 0.002406181745055278, 'weight_decay': 2.119562897377061e-06, 'beta_0': 0.8685669033269243, 'beta_1': 0.9959252906894803, 'epsilon': 3.5216931253518803e-07, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 44.56 GiB of which 616.69 MiB is free. Including non-PyTorch memory, this process has 43.95 GiB memory in use. Of the allocated memory 42.34 GiB is allocated by PyTorch, and 471.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 13:30:01,972] Trial 333 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8047612312902136, 'batch_size': 82, 'attention_heads': 7, 'hidden_dimension': 151, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5976733663499525, 'global_pooling': 'max', 'learning_rate': 0.0030086778090742095, 'weight_decay': 0.0005105910150063435, 'beta_0': 0.8645772874944837, 'beta_1': 0.990155789983398, 'epsilon': 2.5742305078256946e-06, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 3.84 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.66 GiB is free. Including non-PyTorch memory, this process has 40.89 GiB memory in use. Of the allocated memory 39.21 GiB is allocated by PyTorch, and 545.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 13:37:29,131] Trial 334 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7944631608250375, 'batch_size': 83, 'attention_heads': 8, 'hidden_dimension': 214, 'number_of_hidden_layers': 2, 'dropout_rate': 0.588074633365002, 'global_pooling': 'max', 'learning_rate': 0.00217101731467944, 'weight_decay': 0.0005140187813144257, 'beta_0': 0.8678594158999396, 'beta_1': 0.9906355965801675, 'epsilon': 3.847392118492315e-07, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 13:50:39,626] Trial 335 finished with value: 0.9371220610735014 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8201248047138192, 'batch_size': 78, 'attention_heads': 8, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.583121657199014, 'global_pooling': 'max', 'learning_rate': 0.0027232264148290314, 'weight_decay': 1.8362504070468296e-06, 'beta_0': 0.8688994270818248, 'beta_1': 0.9959347904884203, 'epsilon': 6.84187844147026e-07, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.58 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 43.34 GiB memory in use. Of the allocated memory 41.87 GiB is allocated by PyTorch, and 331.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 13:58:10,039] Trial 336 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8020424830350186, 'batch_size': 218, 'attention_heads': 7, 'hidden_dimension': 51, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5682352799213539, 'global_pooling': 'max', 'learning_rate': 0.0006634594477765073, 'weight_decay': 2.095238287351008e-06, 'beta_0': 0.8660771784635621, 'beta_1': 0.9961473875112281, 'epsilon': 9.201630428575463e-07, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 14:11:10,706] Trial 337 finished with value: 0.9286587499104881 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8297144665636502, 'batch_size': 88, 'attention_heads': 8, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5824304524392496, 'global_pooling': 'max', 'learning_rate': 0.0032909931582432903, 'weight_decay': 0.0006360111372315491, 'beta_0': 0.8436346856681699, 'beta_1': 0.9894952718432067, 'epsilon': 3.3013334874183307e-07, 'balanced_loss': False, 'epochs': 194, 'early_stopping_patience': 17, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 896.69 MiB is free. Including non-PyTorch memory, this process has 43.68 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 14:19:00,982] Trial 338 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7875624964609904, 'batch_size': 84, 'attention_heads': 8, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5781965643024121, 'global_pooling': 'mean', 'learning_rate': 0.002278133427583581, 'weight_decay': 1.0996664533421816e-06, 'beta_0': 0.8638946293011797, 'beta_1': 0.9828692894186937, 'epsilon': 1.876570970456958e-07, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.33 GiB is free. Including non-PyTorch memory, this process has 43.22 GiB memory in use. Of the allocated memory 41.23 GiB is allocated by PyTorch, and 858.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 14:26:33,022] Trial 339 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.820299476737274, 'batch_size': 209, 'attention_heads': 9, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5716163017068894, 'global_pooling': 'max', 'learning_rate': 0.0008800335468836325, 'weight_decay': 1.324794195365401e-06, 'beta_0': 0.869227592075926, 'beta_1': 0.988134393851402, 'epsilon': 4.253696690707935e-07, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 18, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 3.70 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.58 GiB is free. Including non-PyTorch memory, this process has 41.97 GiB memory in use. Of the allocated memory 40.40 GiB is allocated by PyTorch, and 427.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 14:34:00,405] Trial 340 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.812256157837097, 'batch_size': 77, 'attention_heads': 8, 'hidden_dimension': 236, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5910555033191139, 'global_pooling': 'max', 'learning_rate': 0.039348461092948, 'weight_decay': 1.000284304713467e-06, 'beta_0': 0.8674460899455504, 'beta_1': 0.9955960522483562, 'epsilon': 8.037828715867098e-07, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 16, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 14:47:17,833] Trial 341 finished with value: 0.9376295740521357 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8264749262568029, 'batch_size': 85, 'attention_heads': 7, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5617201723124156, 'global_pooling': 'max', 'learning_rate': 0.0018961217737790792, 'weight_decay': 1.6147473457033527e-06, 'beta_0': 0.8742822655622478, 'beta_1': 0.9913790107657307, 'epsilon': 2.0235269355675283e-06, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 742.69 MiB is free. Including non-PyTorch memory, this process has 43.83 GiB memory in use. Of the allocated memory 41.87 GiB is allocated by PyTorch, and 827.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 14:55:08,138] Trial 342 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8037903286455498, 'batch_size': 104, 'attention_heads': 8, 'hidden_dimension': 55, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5729153063050824, 'global_pooling': 'max', 'learning_rate': 0.0011659617550356005, 'weight_decay': 3.469398205499982e-06, 'beta_0': 0.8660267075232634, 'beta_1': 0.9923709159105113, 'epsilon': 3.407889156014728e-07, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 15:08:24,258] Trial 343 finished with value: 0.9343960271162096 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8327509425451947, 'batch_size': 113, 'attention_heads': 9, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5630864825605203, 'global_pooling': 'max', 'learning_rate': 0.0006158238685408633, 'weight_decay': 3.172794189182862e-05, 'beta_0': 0.870248670026578, 'beta_1': 0.9872986584711058, 'epsilon': 5.585163703230396e-07, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 18, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 15:19:38,891] Trial 344 finished with value: 0.9364532683027824 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.952308891879354, 'batch_size': 92, 'attention_heads': 8, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5665835786333087, 'global_pooling': 'max', 'learning_rate': 0.000723551060008869, 'weight_decay': 1.23242712631619e-05, 'beta_0': 0.8722731383964951, 'beta_1': 0.9887216224370698, 'epsilon': 5.643024516923098e-07, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 15:34:17,004] Trial 345 finished with value: 0.9426006808050724 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.816593847156911, 'batch_size': 75, 'attention_heads': 8, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5790583768310935, 'global_pooling': 'max', 'learning_rate': 0.0009910975266208036, 'weight_decay': 4.768969728318131e-06, 'beta_0': 0.8632688999415319, 'beta_1': 0.9919883767517922, 'epsilon': 1.0553032074169638e-06, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 15:46:40,276] Trial 346 finished with value: 0.9272991777935811 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8230984865497668, 'batch_size': 74, 'attention_heads': 9, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5557887792928283, 'global_pooling': 'max', 'learning_rate': 0.0009733140846980373, 'weight_decay': 2.1742495544550806e-06, 'beta_0': 0.8633889499481668, 'beta_1': 0.9927978103120508, 'epsilon': 4.7023596517143744e-07, 'balanced_loss': False, 'epochs': 77, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 15:57:58,846] Trial 347 finished with value: 0.9224537994481408 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8168532211540135, 'batch_size': 79, 'attention_heads': 5, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5616532177784427, 'global_pooling': 'max', 'learning_rate': 0.0008595982185237185, 'weight_decay': 5.250906380155595e-06, 'beta_0': 0.8618941013702407, 'beta_1': 0.9822783939645281, 'epsilon': 1.4386641133889883e-06, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 16:09:43,371] Trial 348 finished with value: 0.9444070450776465 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.834751428322863, 'batch_size': 75, 'attention_heads': 7, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49083043441531427, 'global_pooling': 'max', 'learning_rate': 0.001249617155482344, 'weight_decay': 1.514355424064275e-06, 'beta_0': 0.8744984433118272, 'beta_1': 0.9831570736985031, 'epsilon': 6.406622208874615e-07, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 16:21:06,465] Trial 349 finished with value: 0.9506000925891444 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.829990098119431, 'batch_size': 74, 'attention_heads': 7, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4993874443285102, 'global_pooling': 'max', 'learning_rate': 0.0011485650711865836, 'weight_decay': 1.293674736155554e-06, 'beta_0': 0.873981376734986, 'beta_1': 0.9829130714232821, 'epsilon': 8.349313903361472e-07, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 16:34:11,396] Trial 350 finished with value: 0.9270479739209463 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8331645274783268, 'batch_size': 74, 'attention_heads': 7, 'hidden_dimension': 36, 'number_of_hidden_layers': 3, 'dropout_rate': 0.48786250612997534, 'global_pooling': 'max', 'learning_rate': 0.0011976719321790375, 'weight_decay': 1.6098358280482019e-06, 'beta_0': 0.8757130257470729, 'beta_1': 0.9826078854228434, 'epsilon': 7.1202785368048e-07, 'balanced_loss': False, 'epochs': 72, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 16:45:01,334] Trial 351 finished with value: 0.9295202873969916 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8237626964812672, 'batch_size': 76, 'attention_heads': 7, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49410091765057484, 'global_pooling': 'max', 'learning_rate': 0.0013201963000277648, 'weight_decay': 1.3621763199007855e-06, 'beta_0': 0.874078932471814, 'beta_1': 0.983262532501854, 'epsilon': 6.565624941010194e-07, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 17, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 16:55:12,085] Trial 352 finished with value: 0.9294520460943706 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8139027185356338, 'batch_size': 77, 'attention_heads': 7, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5043948888049496, 'global_pooling': 'max', 'learning_rate': 0.0011142473002978821, 'weight_decay': 1.0765854286737244e-06, 'beta_0': 0.8717980210527798, 'beta_1': 0.98310268879418, 'epsilon': 8.311031742718169e-07, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 13, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 17:07:03,883] Trial 353 finished with value: 0.9412927699603248 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8262784559617856, 'batch_size': 71, 'attention_heads': 7, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4863735213125983, 'global_pooling': 'max', 'learning_rate': 0.0005413656524579667, 'weight_decay': 1.3215897604749056e-06, 'beta_0': 0.8757421665589243, 'beta_1': 0.9829578823194314, 'epsilon': 1.088568902451147e-06, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 17:19:12,922] Trial 354 finished with value: 0.9333316749230804 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8287252366876338, 'batch_size': 71, 'attention_heads': 7, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49832705337816263, 'global_pooling': 'max', 'learning_rate': 0.0005115365322122422, 'weight_decay': 1.2912318724933838e-06, 'beta_0': 0.8763817052409619, 'beta_1': 0.9836201671236725, 'epsilon': 1.1059252246266245e-06, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 17:32:15,317] Trial 355 finished with value: 0.9442130633201915 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8194578743982015, 'batch_size': 82, 'attention_heads': 7, 'hidden_dimension': 35, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4822960619990865, 'global_pooling': 'max', 'learning_rate': 0.000417390816055289, 'weight_decay': 1.5124959201952324e-06, 'beta_0': 0.8115180151287442, 'beta_1': 0.9831602495376018, 'epsilon': 6.316722318781313e-07, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 17:44:57,334] Trial 356 finished with value: 0.9385201670084587 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8183158487244548, 'batch_size': 84, 'attention_heads': 7, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.486269962990418, 'global_pooling': 'max', 'learning_rate': 0.0004056779327743275, 'weight_decay': 1.616862199638984e-06, 'beta_0': 0.818816428289657, 'beta_1': 0.982972194629675, 'epsilon': 6.032360811510363e-07, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 17:58:08,366] Trial 357 finished with value: 0.9346053034126449 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.81420995406069, 'batch_size': 80, 'attention_heads': 7, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4780426737276546, 'global_pooling': 'max', 'learning_rate': 0.000345684124163193, 'weight_decay': 1.5066078290060508e-06, 'beta_0': 0.8081754687566398, 'beta_1': 0.9919984935706306, 'epsilon': 4.817106166796318e-07, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 18:10:25,508] Trial 358 finished with value: 0.9305912537205578 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8221137270238622, 'batch_size': 73, 'attention_heads': 7, 'hidden_dimension': 39, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4826278446403051, 'global_pooling': 'max', 'learning_rate': 0.00048444878506267715, 'weight_decay': 1.2813105461992014e-06, 'beta_0': 0.827268994726528, 'beta_1': 0.9824438057160739, 'epsilon': 8.200683080598467e-07, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 18:22:26,474] Trial 359 finished with value: 0.9188464638421857 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8093455404338696, 'batch_size': 80, 'attention_heads': 7, 'hidden_dimension': 36, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4916109607853426, 'global_pooling': 'max', 'learning_rate': 0.000597148799123295, 'weight_decay': 1.4680838856400804e-06, 'beta_0': 0.8733515883343296, 'beta_1': 0.9827692582339613, 'epsilon': 6.384186189485407e-07, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 18:36:15,113] Trial 360 finished with value: 0.9320528559541348 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8196154275153101, 'batch_size': 71, 'attention_heads': 7, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.469047149815043, 'global_pooling': 'max', 'learning_rate': 0.0005442965505358731, 'weight_decay': 1.1673888741262078e-06, 'beta_0': 0.8252467462948375, 'beta_1': 0.9832346230316841, 'epsilon': 6.230629376495638e-05, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 18:48:40,131] Trial 361 finished with value: 0.9430159551712193 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8359096851458451, 'batch_size': 86, 'attention_heads': 7, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4912923896618014, 'global_pooling': 'max', 'learning_rate': 0.0004165260759171115, 'weight_decay': 1.7158697715925007e-06, 'beta_0': 0.8159804983530796, 'beta_1': 0.9840747045228914, 'epsilon': 7.176002871376763e-07, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 19:03:09,089] Trial 362 finished with value: 0.9369671775395422 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8095244540162633, 'batch_size': 84, 'attention_heads': 7, 'hidden_dimension': 35, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4833612325028083, 'global_pooling': 'max', 'learning_rate': 0.0003425765481370097, 'weight_decay': 1.6799153441308496e-06, 'beta_0': 0.8062665997489206, 'beta_1': 0.9841713391343437, 'epsilon': 7.536082010341084e-07, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 19:17:23,022] Trial 363 finished with value: 0.9445310597889559 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8282780267096315, 'batch_size': 87, 'attention_heads': 7, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48979618167213157, 'global_pooling': 'max', 'learning_rate': 0.0004152591122542506, 'weight_decay': 1.8767863039428332e-06, 'beta_0': 0.8148210224649975, 'beta_1': 0.9835367233413354, 'epsilon': 9.434506985062426e-07, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 19:30:08,658] Trial 364 finished with value: 0.9426946115233896 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8280738367555028, 'batch_size': 87, 'attention_heads': 7, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49201970595997674, 'global_pooling': 'max', 'learning_rate': 0.0003973598930449985, 'weight_decay': 2.510568660501964e-06, 'beta_0': 0.817361163468426, 'beta_1': 0.9834795614634144, 'epsilon': 1.0772760207057977e-06, 'balanced_loss': False, 'epochs': 69, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 1.97 GiB. GPU 0 has a total capacity of 44.56 GiB of which 108.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 43.00 GiB is allocated by PyTorch, and 304.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 19:37:48,068] Trial 365 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8351382422085237, 'batch_size': 87, 'attention_heads': 7, 'hidden_dimension': 139, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4945689122399201, 'global_pooling': 'max', 'learning_rate': 0.0003591486654699353, 'weight_decay': 2.4999911783322683e-06, 'beta_0': 0.8173556585075922, 'beta_1': 0.9843325392619352, 'epsilon': 8.801331566544297e-07, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 19:53:28,503] Trial 366 finished with value: 0.9414309525523488 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8254687743658543, 'batch_size': 87, 'attention_heads': 7, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4908753294883638, 'global_pooling': 'max', 'learning_rate': 0.00043316034221681617, 'weight_decay': 1.732959376086083e-06, 'beta_0': 0.81245558314735, 'beta_1': 0.983737793023403, 'epsilon': 1.0124555262275654e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 20:06:31,346] Trial 367 finished with value: 0.9381531353290877 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8291439170285533, 'batch_size': 89, 'attention_heads': 7, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4890749631096554, 'global_pooling': 'max', 'learning_rate': 0.00042098998100406154, 'weight_decay': 1.7935903311004912e-06, 'beta_0': 0.8134639409503559, 'beta_1': 0.9837134427172906, 'epsilon': 1.0331600549059156e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 20:19:48,216] Trial 368 finished with value: 0.9391459204183819 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.83416859796907, 'batch_size': 87, 'attention_heads': 7, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5000179674903829, 'global_pooling': 'max', 'learning_rate': 0.000384942134755454, 'weight_decay': 1.9002626519679306e-06, 'beta_0': 0.8136217612477082, 'beta_1': 0.9834591122072259, 'epsilon': 1.1819143539219544e-06, 'balanced_loss': False, 'epochs': 59, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 20:33:13,390] Trial 369 finished with value: 0.9363163369185079 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8263550296494443, 'batch_size': 82, 'attention_heads': 7, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.492064812839232, 'global_pooling': 'max', 'learning_rate': 0.00043466896727822234, 'weight_decay': 1.7935176043824724e-06, 'beta_0': 0.8112641296441571, 'beta_1': 0.9837781715499401, 'epsilon': 9.248487346068656e-07, 'balanced_loss': False, 'epochs': 62, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
[I 2025-03-01 20:46:04,549] Trial 370 finished with value: 0.9333441487983005 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8382428494539377, 'batch_size': 86, 'attention_heads': 7, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.483266763225228, 'global_pooling': 'max', 'learning_rate': 0.0004492498045263607, 'weight_decay': 2.5758658803636466e-06, 'beta_0': 0.8178648740637716, 'beta_1': 0.9839185800437493, 'epsilon': 1.2827059454703437e-06, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 46.69 MiB is free. Including non-PyTorch memory, this process has 44.51 GiB memory in use. Of the allocated memory 42.93 GiB is allocated by PyTorch, and 437.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-03-01 20:53:34,995] Trial 371 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.6536941901394503, 'batch_size': 92, 'attention_heads': 7, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.49035488074620853, 'global_pooling': 'max', 'learning_rate': 0.0003171753596926766, 'weight_decay': 2.1120874408964758e-06, 'beta_0': 0.8119439506656766, 'beta_1': 0.9832617703785894, 'epsilon': 1.0322579661512304e-06, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 22, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 328 with value: 0.9524112102417622.
slurmstepd: error: *** JOB 15068784 ON gpu040 CANCELLED AT 2025-03-01T21:02:39 DUE TO TIME LIMIT ***
