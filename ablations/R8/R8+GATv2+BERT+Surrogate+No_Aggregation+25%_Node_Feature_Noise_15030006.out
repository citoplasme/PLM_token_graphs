[I 2025-02-26 18:55:11,415] Using an existing study with name 'R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.25' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-26 19:05:31,826] Trial 64 finished with value: 0.9145773561454043 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8479589256075843, 'batch_size': 54, 'attention_heads': 4, 'hidden_dimension': 47, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4761686282887618, 'global_pooling': 'max', 'learning_rate': 0.00216889337055194, 'weight_decay': 0.0002741359885357485, 'beta_0': 0.8266599193087915, 'beta_1': 0.9957599573277238, 'epsilon': 4.08757822588279e-07, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 24, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 19:14:06,783] Trial 65 finished with value: 0.9313820639097139 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8677223941614312, 'batch_size': 60, 'attention_heads': 6, 'hidden_dimension': 39, 'number_of_hidden_layers': 1, 'dropout_rate': 0.44247975882907736, 'global_pooling': 'max', 'learning_rate': 0.0007548746503113683, 'weight_decay': 0.0009935246402391566, 'beta_0': 0.8198518217069697, 'beta_1': 0.994865533706576, 'epsilon': 1.8166415933308483e-07, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 25, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 19:21:59,515] Trial 66 finished with value: 0.9269255904639179 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8758335513872649, 'batch_size': 43, 'attention_heads': 6, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4392499488134854, 'global_pooling': 'max', 'learning_rate': 0.00041743304798689217, 'weight_decay': 0.0009631753161012506, 'beta_0': 0.815121379929693, 'beta_1': 0.9945779450577756, 'epsilon': 1.4917823752550926e-06, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 19:29:55,022] Trial 67 finished with value: 0.9223746921129381 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8923401977748506, 'batch_size': 84, 'attention_heads': 6, 'hidden_dimension': 41, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4157421228701516, 'global_pooling': 'max', 'learning_rate': 0.0007147323450995723, 'weight_decay': 0.0005412179916302952, 'beta_0': 0.823397595015505, 'beta_1': 0.9934280931717988, 'epsilon': 4.5762304963361586e-06, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 19:36:46,933] Trial 68 finished with value: 0.9179849094832163 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9694355943713608, 'batch_size': 72, 'attention_heads': 7, 'hidden_dimension': 65, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3826829856200387, 'global_pooling': 'max', 'learning_rate': 0.0010209051300784943, 'weight_decay': 0.000746686114859598, 'beta_0': 0.8759248277886491, 'beta_1': 0.9918732086365554, 'epsilon': 2.481481751323402e-06, 'balanced_loss': False, 'epochs': 121, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 7}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 19:49:02,464] Trial 69 finished with value: 0.9180116191548158 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9293583823485868, 'batch_size': 37, 'attention_heads': 8, 'hidden_dimension': 175, 'number_of_hidden_layers': 1, 'dropout_rate': 0.42597423150083935, 'global_pooling': 'sum', 'learning_rate': 0.0013546759114428828, 'weight_decay': 5.125266198304268e-05, 'beta_0': 0.809309930858162, 'beta_1': 0.9851616734513537, 'epsilon': 8.65787066491962e-06, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 994.69 MiB is free. Including non-PyTorch memory, this process has 43.58 GiB memory in use. Of the allocated memory 41.85 GiB is allocated by PyTorch, and 597.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 19:55:25,855] Trial 70 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9109082295186787, 'batch_size': 193, 'attention_heads': 4, 'hidden_dimension': 164, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5683333140769617, 'global_pooling': 'max', 'learning_rate': 0.00023271273095102705, 'weight_decay': 0.0005890245253100653, 'beta_0': 0.8166038815823609, 'beta_1': 0.9858708325128167, 'epsilon': 1.1598630173023627e-05, 'balanced_loss': False, 'epochs': 134, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 20:10:48,646] Trial 71 finished with value: 0.9376166478346862 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8638814699812319, 'batch_size': 48, 'attention_heads': 6, 'hidden_dimension': 37, 'number_of_hidden_layers': 4, 'dropout_rate': 0.45005699654981046, 'global_pooling': 'max', 'learning_rate': 0.0006942122852547163, 'weight_decay': 0.00041970786366354054, 'beta_0': 0.8599565254653472, 'beta_1': 0.9844525427783799, 'epsilon': 3.381133744133899e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 20:23:31,483] Trial 72 finished with value: 0.8268196395694414 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9909196452527024, 'batch_size': 93, 'attention_heads': 5, 'hidden_dimension': 149, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5003955324817476, 'global_pooling': 'sum', 'learning_rate': 0.00044784822715334104, 'weight_decay': 0.00035802407963981365, 'beta_0': 0.8531951388427234, 'beta_1': 0.98396379585038, 'epsilon': 3.316385911618019e-05, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 20:40:15,139] Trial 73 finished with value: 0.927342875474477 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8503809049950127, 'batch_size': 63, 'attention_heads': 6, 'hidden_dimension': 38, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4455085395507379, 'global_pooling': 'max', 'learning_rate': 0.0008104722840275849, 'weight_decay': 0.0008180176781715666, 'beta_0': 0.8587622266681751, 'beta_1': 0.9825497185113121, 'epsilon': 6.674178161585299e-08, 'balanced_loss': False, 'epochs': 198, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 21:00:56,792] Trial 74 finished with value: 0.9360181883510302 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8411055195965359, 'batch_size': 46, 'attention_heads': 6, 'hidden_dimension': 57, 'number_of_hidden_layers': 4, 'dropout_rate': 0.46159120996449127, 'global_pooling': 'max', 'learning_rate': 0.001888668911580101, 'weight_decay': 0.0004247768436025435, 'beta_0': 0.8713306064758806, 'beta_1': 0.9807164514549909, 'epsilon': 1.7772632634718166e-07, 'balanced_loss': False, 'epochs': 174, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 980.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 228.69 MiB is free. Including non-PyTorch memory, this process has 44.33 GiB memory in use. Of the allocated memory 42.25 GiB is allocated by PyTorch, and 951.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 21:12:03,430] Trial 75 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8239583354455845, 'batch_size': 49, 'attention_heads': 7, 'hidden_dimension': 56, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4561507693369057, 'global_pooling': 'max', 'learning_rate': 0.001761485376359508, 'weight_decay': 0.00048133263330397876, 'beta_0': 0.8708746763656436, 'beta_1': 0.9807335201076076, 'epsilon': 2.6588660073128442e-08, 'balanced_loss': False, 'epochs': 179, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 738.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 316.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 42.07 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 21:22:32,529] Trial 76 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7916184596331333, 'batch_size': 46, 'attention_heads': 4, 'hidden_dimension': 70, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4900798992068587, 'global_pooling': 'max', 'learning_rate': 0.0010359580744267228, 'weight_decay': 0.00022689340327638154, 'beta_0': 0.8817899418431547, 'beta_1': 0.9811711522292608, 'epsilon': 2.2772690886919042e-08, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 3}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 21:33:34,664] Trial 77 finished with value: 0.7389137391477092 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8426757861533962, 'batch_size': 33, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 4, 'dropout_rate': 0.42848339972617416, 'global_pooling': 'max', 'learning_rate': 0.005031652867425798, 'weight_decay': 0.0001813330812948988, 'beta_0': 0.8673029158622353, 'beta_1': 0.980080588341089, 'epsilon': 8.341153704075312e-08, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 44.56 GiB of which 414.69 MiB is free. Including non-PyTorch memory, this process has 44.15 GiB memory in use. Of the allocated memory 42.49 GiB is allocated by PyTorch, and 517.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 21:41:13,469] Trial 78 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8601550263679204, 'batch_size': 241, 'attention_heads': 11, 'hidden_dimension': 60, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5841929784783239, 'global_pooling': 'max', 'learning_rate': 0.0036384134137532097, 'weight_decay': 0.00037481261667043946, 'beta_0': 0.8639858563644192, 'beta_1': 0.982715556515712, 'epsilon': 3.326563949360609e-08, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 3}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 21:58:11,619] Trial 79 finished with value: 0.8988201984514264 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8852431055677865, 'batch_size': 109, 'attention_heads': 5, 'hidden_dimension': 51, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5259859185776512, 'global_pooling': 'max', 'learning_rate': 0.00035364230082403147, 'weight_decay': 0.00013279630301660117, 'beta_0': 0.875175234277765, 'beta_1': 0.9839777023122664, 'epsilon': 1.5610057797941003e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 488.69 MiB is free. Including non-PyTorch memory, this process has 44.08 GiB memory in use. Of the allocated memory 41.95 GiB is allocated by PyTorch, and 998.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 22:05:42,952] Trial 80 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8324834536320829, 'batch_size': 77, 'attention_heads': 6, 'hidden_dimension': 78, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4621187179294167, 'global_pooling': 'mean', 'learning_rate': 0.0005325128770492455, 'weight_decay': 1.476624792687571e-06, 'beta_0': 0.8578275250615223, 'beta_1': 0.981726426481447, 'epsilon': 1.9101200391103016e-06, 'balanced_loss': True, 'epochs': 169, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.92 GiB is free. Including non-PyTorch memory, this process has 41.63 GiB memory in use. Of the allocated memory 40.17 GiB is allocated by PyTorch, and 319.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 22:12:52,280] Trial 81 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7993993165880555, 'batch_size': 69, 'attention_heads': 7, 'hidden_dimension': 233, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5154716202727472, 'global_pooling': 'sum', 'learning_rate': 0.0022044454590297575, 'weight_decay': 0.0005118050436843165, 'beta_0': 0.8796012492999522, 'beta_1': 0.9845250917359677, 'epsilon': 2.8510498638729484e-06, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 22:24:39,208] Trial 82 finished with value: 0.9206859491384811 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8647597491024492, 'batch_size': 119, 'attention_heads': 4, 'hidden_dimension': 47, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5327932460155168, 'global_pooling': 'max', 'learning_rate': 0.0012621992964837766, 'weight_decay': 0.00010081822965998037, 'beta_0': 0.8551687334402689, 'beta_1': 0.9873529000211195, 'epsilon': 4.605778281487138e-06, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 22:32:51,243] Trial 83 finished with value: 0.9305631848391178 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8683326950311251, 'batch_size': 60, 'attention_heads': 6, 'hidden_dimension': 35, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4410666770053707, 'global_pooling': 'max', 'learning_rate': 0.0006604945189765788, 'weight_decay': 0.000990917984035498, 'beta_0': 0.8711417904507031, 'beta_1': 0.9832800441788928, 'epsilon': 1.114227596355019e-07, 'balanced_loss': False, 'epochs': 184, 'early_stopping_patience': 25, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 43.23 GiB memory in use. Of the allocated memory 41.92 GiB is allocated by PyTorch, and 169.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 22:38:50,308] Trial 84 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8144185344655684, 'batch_size': 52, 'attention_heads': 6, 'hidden_dimension': 248, 'number_of_hidden_layers': 4, 'dropout_rate': 0.44942779465586175, 'global_pooling': 'max', 'learning_rate': 0.0016327710873722202, 'weight_decay': 0.0006753429166470384, 'beta_0': 0.8613161504254412, 'beta_1': 0.9820086377827837, 'epsilon': 1.9083743764909575e-07, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 22:51:39,893] Trial 85 finished with value: 0.9323350832141187 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8759673283362346, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 188, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40176309941262567, 'global_pooling': 'max', 'learning_rate': 0.0007849887054728791, 'weight_decay': 0.000623803351460651, 'beta_0': 0.8649515100826174, 'beta_1': 0.9806166610911696, 'epsilon': 4.552235420935642e-08, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 23:09:21,364] Trial 86 finished with value: 0.9391765503628418 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8790089548155486, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 199, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3386479041983292, 'global_pooling': 'max', 'learning_rate': 0.0009334802796623439, 'weight_decay': 0.0003672257053842502, 'beta_0': 0.8437655672429552, 'beta_1': 0.9825025136922109, 'epsilon': 5.1057465523171373e-08, 'balanced_loss': False, 'epochs': 192, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 23:25:31,978] Trial 87 finished with value: 0.9376930269024601 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8800220201332664, 'batch_size': 41, 'attention_heads': 5, 'hidden_dimension': 191, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35381630404960457, 'global_pooling': 'max', 'learning_rate': 0.0002664729605124136, 'weight_decay': 0.00041627826090243243, 'beta_0': 0.8391722303336673, 'beta_1': 0.9809941449264458, 'epsilon': 5.50174473736937e-08, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 43.49 GiB memory in use. Of the allocated memory 41.11 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 23:32:48,787] Trial 88 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8954162488175778, 'batch_size': 34, 'attention_heads': 16, 'hidden_dimension': 210, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3542557020331932, 'global_pooling': 'max', 'learning_rate': 6.818299024017053e-05, 'weight_decay': 0.00042209535819183024, 'beta_0': 0.8434791919703173, 'beta_1': 0.9811246754848182, 'epsilon': 5.613897417951334e-08, 'balanced_loss': False, 'epochs': 199, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.40 GiB. GPU 0 has a total capacity of 44.56 GiB of which 670.69 MiB is free. Including non-PyTorch memory, this process has 43.90 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-26 23:40:46,622] Trial 89 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8557768174231091, 'batch_size': 46, 'attention_heads': 5, 'hidden_dimension': 183, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3329369247294333, 'global_pooling': 'max', 'learning_rate': 7.975270775105055e-05, 'weight_decay': 0.00024220705839973246, 'beta_0': 0.8400085568366004, 'beta_1': 0.9829173888171051, 'epsilon': 3.099792986421481e-08, 'balanced_loss': False, 'epochs': 172, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-26 23:55:56,951] Trial 90 finished with value: 0.9103370142898333 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9080142233922018, 'batch_size': 42, 'attention_heads': 4, 'hidden_dimension': 204, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3014562378365587, 'global_pooling': 'max', 'learning_rate': 0.0002352037483104478, 'weight_decay': 0.0004369479609925901, 'beta_0': 0.8366584766905455, 'beta_1': 0.9821458113162661, 'epsilon': 3.663134954044975e-08, 'balanced_loss': False, 'epochs': 193, 'early_stopping_patience': 23, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 2.21 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.15 GiB is free. Including non-PyTorch memory, this process has 42.40 GiB memory in use. Of the allocated memory 40.50 GiB is allocated by PyTorch, and 774.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 00:03:48,479] Trial 91 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.846802272941861, 'batch_size': 51, 'attention_heads': 5, 'hidden_dimension': 198, 'number_of_hidden_layers': 2, 'dropout_rate': 0.35090528912280544, 'global_pooling': 'max', 'learning_rate': 0.00027608083054677805, 'weight_decay': 0.00030684782822013707, 'beta_0': 0.8343926009040653, 'beta_1': 0.9814467219071265, 'epsilon': 6.254207858316247e-08, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 24, 'plateau_patience': 25, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 00:18:20,290] Trial 92 finished with value: 0.9276340964892822 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9193807044334898, 'batch_size': 56, 'attention_heads': 4, 'hidden_dimension': 217, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3660971377098415, 'global_pooling': 'max', 'learning_rate': 0.0009205563853250939, 'weight_decay': 0.00033461528363555045, 'beta_0': 0.848380434521251, 'beta_1': 0.9834627822446479, 'epsilon': 1.724874671580478e-08, 'balanced_loss': False, 'epochs': 183, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 00:41:40,393] Trial 93 finished with value: 0.9111275639617471 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8810434772896077, 'batch_size': 37, 'attention_heads': 5, 'hidden_dimension': 188, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3259430045073388, 'global_pooling': 'max', 'learning_rate': 1.6655102725508075e-05, 'weight_decay': 0.0003688325915476075, 'beta_0': 0.8451303845408279, 'beta_1': 0.9804964315522673, 'epsilon': 1.213965801256414e-08, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 00:54:36,958] Trial 94 finished with value: 0.9246360556170339 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.873423340236678, 'batch_size': 41, 'attention_heads': 5, 'hidden_dimension': 180, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3807130513346908, 'global_pooling': 'max', 'learning_rate': 0.0001446925136849684, 'weight_decay': 0.0005181315367491574, 'beta_0': 0.852717946869144, 'beta_1': 0.9807851604439145, 'epsilon': 4.751660167174394e-08, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.73 GiB. GPU 0 has a total capacity of 44.56 GiB of which 720.69 MiB is free. Including non-PyTorch memory, this process has 43.85 GiB memory in use. Of the allocated memory 41.34 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:02:07,343] Trial 95 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8271613386578226, 'batch_size': 67, 'attention_heads': 4, 'hidden_dimension': 169, 'number_of_hidden_layers': 2, 'dropout_rate': 0.40251788351276674, 'global_pooling': 'max', 'learning_rate': 0.0006557632557669553, 'weight_decay': 0.0006316585615200701, 'beta_0': 0.8664544070944497, 'beta_1': 0.9803339994493874, 'epsilon': 1.3052727919198115e-07, 'balanced_loss': False, 'epochs': 191, 'early_stopping_patience': 25, 'plateau_patience': 12, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.37 GiB. GPU 0 has a total capacity of 44.56 GiB of which 132.69 MiB is free. Including non-PyTorch memory, this process has 44.42 GiB memory in use. Of the allocated memory 41.91 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:09:35,206] Trial 96 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8369239972527076, 'batch_size': 32, 'attention_heads': 6, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34380971508110003, 'global_pooling': 'max', 'learning_rate': 0.0005038350314949632, 'weight_decay': 0.0007471520059534935, 'beta_0': 0.8388094733141261, 'beta_1': 0.9823112177794515, 'epsilon': 4.180305261845508e-08, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 43.36 GiB memory in use. Of the allocated memory 41.42 GiB is allocated by PyTorch, and 801.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:17:00,589] Trial 97 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8528805034481698, 'batch_size': 48, 'attention_heads': 5, 'hidden_dimension': 199, 'number_of_hidden_layers': 4, 'dropout_rate': 0.31351491795839087, 'global_pooling': 'max', 'learning_rate': 0.001221672356233004, 'weight_decay': 0.00018751796077692745, 'beta_0': 0.8316211451766152, 'beta_1': 0.9983951079083484, 'epsilon': 9.065363538863293e-08, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 01:40:58,822] Trial 98 finished with value: 0.17341012772792685 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9006289497766036, 'batch_size': 87, 'attention_heads': 4, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5538120610566291, 'global_pooling': 'max', 'learning_rate': 0.05289639133248508, 'weight_decay': 0.00028004539846026276, 'beta_0': 0.8498058773702185, 'beta_1': 0.9812561233874122, 'epsilon': 1.8493748827647936e-08, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 25, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 44.56 GiB of which 480.69 MiB is free. Including non-PyTorch memory, this process has 44.08 GiB memory in use. Of the allocated memory 40.49 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 01:47:00,627] Trial 99 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8083170401259785, 'batch_size': 78, 'attention_heads': 5, 'hidden_dimension': 190, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4120032614859853, 'global_pooling': 'max', 'learning_rate': 0.00269178270916618, 'weight_decay': 0.0005675271909756888, 'beta_0': 0.8624905183323504, 'beta_1': 0.9819398341073519, 'epsilon': 7.335526754285506e-08, 'balanced_loss': True, 'epochs': 184, 'early_stopping_patience': 24, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 02:01:15,225] Trial 100 finished with value: 0.8916315588149413 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9582588588868515, 'batch_size': 40, 'attention_heads': 5, 'hidden_dimension': 218, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4191180979810545, 'global_pooling': 'mean', 'learning_rate': 0.0003683551347818754, 'weight_decay': 0.0008434135595367406, 'beta_0': 0.8436313732015656, 'beta_1': 0.9808226435506812, 'epsilon': 7.767140236530216e-07, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 9}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 258.69 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 42.18 GiB is allocated by PyTorch, and 990.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 02:08:29,449] Trial 101 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.697830228005001, 'batch_size': 59, 'attention_heads': 4, 'hidden_dimension': 171, 'number_of_hidden_layers': 4, 'dropout_rate': 0.38625219060147736, 'global_pooling': 'max', 'learning_rate': 0.0009170539718950071, 'weight_decay': 0.0004267128378654917, 'beta_0': 0.8592741800361542, 'beta_1': 0.9817231518135208, 'epsilon': 5.31386380962054e-08, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 25, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 43.03 GiB memory in use. Of the allocated memory 41.15 GiB is allocated by PyTorch, and 749.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 02:15:28,344] Trial 102 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.774441131817783, 'batch_size': 45, 'attention_heads': 6, 'hidden_dimension': 193, 'number_of_hidden_layers': 4, 'dropout_rate': 0.36800924233387056, 'global_pooling': 'max', 'learning_rate': 0.0019455075662576091, 'weight_decay': 0.0006309080295834808, 'beta_0': 0.8476101878284222, 'beta_1': 0.9800612331001858, 'epsilon': 1.9632561853423956e-05, 'balanced_loss': False, 'epochs': 128, 'early_stopping_patience': 15, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.77 GiB. GPU 0 has a total capacity of 44.56 GiB of which 30.69 MiB is free. Including non-PyTorch memory, this process has 44.52 GiB memory in use. Of the allocated memory 42.80 GiB is allocated by PyTorch, and 585.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 02:22:27,529] Trial 103 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8794216493598845, 'batch_size': 128, 'attention_heads': 5, 'hidden_dimension': 223, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5411074355463177, 'global_pooling': 'sum', 'learning_rate': 0.0014446483389207604, 'weight_decay': 3.379402567847907e-05, 'beta_0': 0.8684530891941004, 'beta_1': 0.9952366898753452, 'epsilon': 2.4506696863678423e-06, 'balanced_loss': False, 'epochs': 78, 'early_stopping_patience': 22, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 43.24 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 471.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 02:29:23,128] Trial 104 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8615201771704383, 'batch_size': 104, 'attention_heads': 8, 'hidden_dimension': 132, 'number_of_hidden_layers': 4, 'dropout_rate': 0.554882163631602, 'global_pooling': 'sum', 'learning_rate': 0.0011448742252400332, 'weight_decay': 7.109214664317712e-05, 'beta_0': 0.8647338769107213, 'beta_1': 0.9897931044912286, 'epsilon': 3.691371073914921e-06, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 12, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 02:44:59,694] Trial 105 finished with value: 0.9329029455054376 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8891146210135646, 'batch_size': 137, 'attention_heads': 7, 'hidden_dimension': 62, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43414910186776434, 'global_pooling': 'sum', 'learning_rate': 0.0006755391699955667, 'weight_decay': 9.35371761231514e-05, 'beta_0': 0.8720486326760656, 'beta_1': 0.9938716913806837, 'epsilon': 7.995967499821634e-06, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 02:56:22,060] Trial 106 finished with value: 0.9160780810568783 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8935985646345487, 'batch_size': 137, 'attention_heads': 15, 'hidden_dimension': 60, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43033848421065535, 'global_pooling': 'sum', 'learning_rate': 0.0006777499851073269, 'weight_decay': 0.00015134555073339579, 'beta_0': 0.8735620148495077, 'beta_1': 0.9970078217798175, 'epsilon': 7.8689309096783e-06, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 24, 'plateau_patience': 11, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 03:12:34,790] Trial 107 finished with value: 0.9274712148149467 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8714227364108921, 'batch_size': 72, 'attention_heads': 6, 'hidden_dimension': 69, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4653329404702456, 'global_pooling': 'sum', 'learning_rate': 0.00047143472413167266, 'weight_decay': 0.0004759225830981657, 'beta_0': 0.8541493143256028, 'beta_1': 0.9931231582137251, 'epsilon': 1.2282793077401564e-05, 'balanced_loss': False, 'epochs': 93, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 42.82 GiB memory in use. Of the allocated memory 41.52 GiB is allocated by PyTorch, and 146.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 03:18:30,187] Trial 108 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8438677402826014, 'batch_size': 58, 'attention_heads': 7, 'hidden_dimension': 197, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3354642586256782, 'global_pooling': 'max', 'learning_rate': 0.0006143687490272822, 'weight_decay': 9.01290258949225e-05, 'beta_0': 0.8720569425054361, 'beta_1': 0.993856129394274, 'epsilon': 5.629673851015114e-06, 'balanced_loss': False, 'epochs': 162, 'early_stopping_patience': 23, 'plateau_patience': 12, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 03:31:59,322] Trial 109 finished with value: 0.924082827940351 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.881497649848088, 'batch_size': 37, 'attention_heads': 5, 'hidden_dimension': 45, 'number_of_hidden_layers': 4, 'dropout_rate': 0.47715809082841276, 'global_pooling': 'max', 'learning_rate': 0.0008628000963207226, 'weight_decay': 0.00034629449466120124, 'beta_0': 0.8514595848097359, 'beta_1': 0.9975277267595881, 'epsilon': 1.7687037178840663e-05, 'balanced_loss': False, 'epochs': 178, 'early_stopping_patience': 24, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 03:48:28,189] Trial 110 finished with value: 0.925671868247096 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.819798914400455, 'batch_size': 52, 'attention_heads': 6, 'hidden_dimension': 51, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4067697582089704, 'global_pooling': 'sum', 'learning_rate': 0.000278649926112987, 'weight_decay': 5.312116323571456e-05, 'beta_0': 0.8684895200260119, 'beta_1': 0.9846105020679233, 'epsilon': 1.2912146598206084e-06, 'balanced_loss': False, 'epochs': 58, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 258.69 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 42.20 GiB is allocated by PyTorch, and 972.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 03:54:34,178] Trial 111 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8304896617875891, 'batch_size': 151, 'attention_heads': 4, 'hidden_dimension': 83, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4538480590830871, 'global_pooling': 'max', 'learning_rate': 0.00041311605372735036, 'weight_decay': 0.00020742216420348698, 'beta_0': 0.8753759209167549, 'beta_1': 0.9966245654725585, 'epsilon': 2.7997249195550896e-05, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 25, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 04:12:51,287] Trial 112 finished with value: 0.9152186061465186 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9284072668363513, 'batch_size': 66, 'attention_heads': 6, 'hidden_dimension': 181, 'number_of_hidden_layers': 4, 'dropout_rate': 0.43596070894409467, 'global_pooling': 'max', 'learning_rate': 0.001117597953393624, 'weight_decay': 9.563229705822837e-06, 'beta_0': 0.8414527877846736, 'beta_1': 0.9890229245514522, 'epsilon': 6.573906756701848e-06, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 22, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 04:30:01,993] Trial 113 finished with value: 0.937714555798012 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8894748733088694, 'batch_size': 125, 'attention_heads': 7, 'hidden_dimension': 54, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5471161241452727, 'global_pooling': 'sum', 'learning_rate': 0.0017918296268255162, 'weight_decay': 0.00011040659372820133, 'beta_0': 0.8846737286891129, 'beta_1': 0.9940891250386372, 'epsilon': 9.147953599163083e-06, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 04:47:48,299] Trial 114 finished with value: 0.9247961503794568 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9036839998411168, 'batch_size': 132, 'attention_heads': 8, 'hidden_dimension': 66, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5693089050657455, 'global_pooling': 'sum', 'learning_rate': 0.0018898713902961311, 'weight_decay': 0.00015099314867911864, 'beta_0': 0.884165019465634, 'beta_1': 0.9952532255592877, 'epsilon': 1.3153964072878442e-05, 'balanced_loss': False, 'epochs': 79, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 05:03:15,103] Trial 115 finished with value: 0.9221345923096754 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.890873504134679, 'batch_size': 123, 'attention_heads': 9, 'hidden_dimension': 55, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5366537933615835, 'global_pooling': 'sum', 'learning_rate': 0.0024538359739164723, 'weight_decay': 0.00025478244987429147, 'beta_0': 0.8876471207716307, 'beta_1': 0.9940177828969902, 'epsilon': 9.674481266532524e-06, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 43.50 GiB memory in use. Of the allocated memory 41.10 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 05:20:33,866] Trial 116 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8654795924813211, 'batch_size': 115, 'attention_heads': 7, 'hidden_dimension': 62, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5277290636933294, 'global_pooling': 'sum', 'learning_rate': 0.0008172767181168893, 'weight_decay': 0.00011063059244133034, 'beta_0': 0.8783434349596224, 'beta_1': 0.9927908149020541, 'epsilon': 7.845838833362182e-06, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 05:34:07,287] Trial 117 finished with value: 0.9254632156212219 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8599934536876134, 'batch_size': 44, 'attention_heads': 7, 'hidden_dimension': 37, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5216139930495075, 'global_pooling': 'sum', 'learning_rate': 0.0015253295484354082, 'weight_decay': 0.00039346455027425504, 'beta_0': 0.8605810764405202, 'beta_1': 0.9945964337242555, 'epsilon': 4.305093148757567e-08, 'balanced_loss': False, 'epochs': 75, 'early_stopping_patience': 24, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 05:45:27,201] Trial 118 finished with value: 0.9098085585993259 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.938023112359428, 'batch_size': 139, 'attention_heads': 6, 'hidden_dimension': 75, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3911324116001489, 'global_pooling': 'sum', 'learning_rate': 0.0005365729523958462, 'weight_decay': 0.0007724748741275454, 'beta_0': 0.8558189607110742, 'beta_1': 0.9921924760124936, 'epsilon': 2.6443342337832696e-08, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 05:57:52,982] Trial 119 finished with value: 0.9387560786786735 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8533292722838659, 'batch_size': 217, 'attention_heads': 5, 'hidden_dimension': 43, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5471977365588735, 'global_pooling': 'mean', 'learning_rate': 0.0009970071510273448, 'weight_decay': 0.0005548446207115114, 'beta_0': 0.8904577862588565, 'beta_1': 0.9910588574538683, 'epsilon': 4.913936224935757e-06, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 19, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 892.69 MiB is free. Including non-PyTorch memory, this process has 43.68 GiB memory in use. Of the allocated memory 42.11 GiB is allocated by PyTorch, and 431.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 06:03:57,736] Trial 120 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8530003458820393, 'batch_size': 224, 'attention_heads': 8, 'hidden_dimension': 40, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5490793811070772, 'global_pooling': 'mean', 'learning_rate': 0.0031362806201833244, 'weight_decay': 0.0004839473406254227, 'beta_0': 0.8935163180625628, 'beta_1': 0.9933378413752391, 'epsilon': 5.358950354419662e-06, 'balanced_loss': False, 'epochs': 87, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 920.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 876.69 MiB is free. Including non-PyTorch memory, this process has 43.70 GiB memory in use. Of the allocated memory 38.56 GiB is allocated by PyTorch, and 3.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 06:15:36,339] Trial 121 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8423828725361886, 'batch_size': 196, 'attention_heads': 4, 'hidden_dimension': 50, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5627307081803404, 'global_pooling': 'sum', 'learning_rate': 0.0010237131138098985, 'weight_decay': 5.9732339695299494e-05, 'beta_0': 0.8915335025905163, 'beta_1': 0.9908366778430439, 'epsilon': 4.383597583976472e-06, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 06:24:49,525] Trial 122 finished with value: 0.9016743703685506 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9502097112971236, 'batch_size': 146, 'attention_heads': 7, 'hidden_dimension': 44, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5367841617788233, 'global_pooling': 'mean', 'learning_rate': 0.0014171091031598534, 'weight_decay': 0.0002935273542016314, 'beta_0': 0.8999194777493699, 'beta_1': 0.9956575527461089, 'epsilon': 2.6095602582075085e-07, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 17, 'plateau_patience': 13, 'plateau_divider': 6}. Best is trial 14 with value: 0.9451449407066621.
[I 2025-02-27 06:40:39,556] Trial 123 finished with value: 0.9163503449471424 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8739813087981755, 'batch_size': 217, 'attention_heads': 5, 'hidden_dimension': 57, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5471747196571027, 'global_pooling': 'mean', 'learning_rate': 0.0006866728883524554, 'weight_decay': 0.0005614333403498348, 'beta_0': 0.8882825007465672, 'beta_1': 0.9810129627626213, 'epsilon': 9.413830538409147e-06, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 2.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 924.69 MiB is free. Including non-PyTorch memory, this process has 43.65 GiB memory in use. Of the allocated memory 42.25 GiB is allocated by PyTorch, and 252.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 06:46:36,108] Trial 124 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8826809890662541, 'batch_size': 181, 'attention_heads': 6, 'hidden_dimension': 155, 'number_of_hidden_layers': 3, 'dropout_rate': 0.37634023499514463, 'global_pooling': 'mean', 'learning_rate': 0.000837036612549301, 'weight_decay': 0.0006868077833214816, 'beta_0': 0.8894738501424297, 'beta_1': 0.9815001751235561, 'epsilon': 6.7277443091721074e-06, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 19, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 44.56 GiB of which 502.69 MiB is free. Including non-PyTorch memory, this process has 44.06 GiB memory in use. Of the allocated memory 41.41 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-27 06:54:03,079] Trial 125 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8950662792681907, 'batch_size': 96, 'attention_heads': 5, 'hidden_dimension': 186, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5782562020634716, 'global_pooling': 'mean', 'learning_rate': 0.0012349235644304303, 'weight_decay': 0.00011898613613900921, 'beta_0': 0.8844865898557803, 'beta_1': 0.9912978706942985, 'epsilon': 3.2112572821872188e-06, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 14 with value: 0.9451449407066621.
slurmstepd: error: *** JOB 15030006 ON gpu052 CANCELLED AT 2025-02-27T06:55:05 DUE TO TIME LIMIT ***
