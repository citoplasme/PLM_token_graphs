[I 2025-02-28 09:03:14,651] Using an existing study with name 'R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-No_Ablation-1.0-0.25' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
[I 2025-02-28 09:17:14,656] Trial 240 finished with value: 0.9303637670423663 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8163913613978244, 'batch_size': 75, 'attention_heads': 9, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5543556754118554, 'global_pooling': 'max', 'learning_rate': 0.0006575449219527969, 'weight_decay': 0.0004940931977178178, 'beta_0': 0.865067975287972, 'beta_1': 0.9850432862936814, 'epsilon': 1.0856468303532115e-07, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 09:32:28,887] Trial 241 finished with value: 0.9356494913234268 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.83450951498039, 'batch_size': 57, 'attention_heads': 9, 'hidden_dimension': 63, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5473797336254252, 'global_pooling': 'sum', 'learning_rate': 0.0002564237460314689, 'weight_decay': 0.00041498350809409323, 'beta_0': 0.8663671515076814, 'beta_1': 0.9854141813047891, 'epsilon': 7.068662957607434e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
CUDA out of memory. Tried to allocate 3.01 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 43.52 GiB memory in use. Of the allocated memory 42.03 GiB is allocated by PyTorch, and 351.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 09:38:19,980] Trial 242 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8239585740166897, 'batch_size': 65, 'attention_heads': 9, 'hidden_dimension': 202, 'number_of_hidden_layers': 2, 'dropout_rate': 0.583117618553539, 'global_pooling': 'max', 'learning_rate': 0.00041044732893177037, 'weight_decay': 0.0007121650612263862, 'beta_0': 0.8592093538419192, 'beta_1': 0.9866368709151822, 'epsilon': 9.54315151823035e-08, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 09:50:21,668] Trial 243 finished with value: 0.9251803510163394 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9223557856718364, 'batch_size': 70, 'attention_heads': 9, 'hidden_dimension': 78, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5544452277663426, 'global_pooling': 'mean', 'learning_rate': 0.0005318258116025201, 'weight_decay': 0.00036239035997107446, 'beta_0': 0.8702061344004005, 'beta_1': 0.9944144995737124, 'epsilon': 1.5485975557552904e-07, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
CUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 44.56 GiB of which 316.69 MiB is free. Including non-PyTorch memory, this process has 44.24 GiB memory in use. Of the allocated memory 42.54 GiB is allocated by PyTorch, and 571.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 09:57:53,604] Trial 244 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8001008354555714, 'batch_size': 63, 'attention_heads': 8, 'hidden_dimension': 55, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5490536206837584, 'global_pooling': 'max', 'learning_rate': 0.0008126395397937389, 'weight_decay': 0.0007775386905841436, 'beta_0': 0.8673754223577738, 'beta_1': 0.9934942516801992, 'epsilon': 6.935940928657304e-08, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 10:09:11,513] Trial 245 finished with value: 0.9378305694747003 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8177524067241485, 'batch_size': 87, 'attention_heads': 8, 'hidden_dimension': 50, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4221752944838412, 'global_pooling': 'max', 'learning_rate': 0.0007713110085205419, 'weight_decay': 0.0004561578894095279, 'beta_0': 0.8629895425863667, 'beta_1': 0.986030909987602, 'epsilon': 4.901889126169232e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 10:22:19,066] Trial 246 finished with value: 0.9402397939836394 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8140551532930843, 'batch_size': 92, 'attention_heads': 8, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5068714963381926, 'global_pooling': 'max', 'learning_rate': 0.0007478918380974337, 'weight_decay': 0.00047215663097237125, 'beta_0': 0.8611006906309476, 'beta_1': 0.9858708325128167, 'epsilon': 4.9531184111266044e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
CUDA out of memory. Tried to allocate 3.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 692.69 MiB is free. Including non-PyTorch memory, this process has 43.88 GiB memory in use. Of the allocated memory 42.05 GiB is allocated by PyTorch, and 689.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 10:28:08,672] Trial 247 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.815205275254782, 'batch_size': 89, 'attention_heads': 8, 'hidden_dimension': 197, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5066951597960437, 'global_pooling': 'max', 'learning_rate': 0.0007328699844517237, 'weight_decay': 0.0004365649996662158, 'beta_0': 0.8628579780491519, 'beta_1': 0.9862338448976151, 'epsilon': 4.7321073957581094e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 10:39:40,076] Trial 248 finished with value: 0.9231020507315011 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8061679307418741, 'batch_size': 85, 'attention_heads': 8, 'hidden_dimension': 50, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4198747030203146, 'global_pooling': 'max', 'learning_rate': 0.000628310667923391, 'weight_decay': 0.00031967743313024216, 'beta_0': 0.8615476571574662, 'beta_1': 0.9866394713150625, 'epsilon': 4.277117707883431e-08, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 43.01 GiB memory in use. Of the allocated memory 41.26 GiB is allocated by PyTorch, and 615.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 10:45:56,197] Trial 249 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8124752046670263, 'batch_size': 81, 'attention_heads': 10, 'hidden_dimension': 60, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5123141986123718, 'global_pooling': 'max', 'learning_rate': 0.0008709533087250165, 'weight_decay': 0.0004744207552053475, 'beta_0': 0.8639952028264016, 'beta_1': 0.9857317548030103, 'epsilon': 5.5789182258039716e-08, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 10:57:38,999] Trial 250 finished with value: 0.9303418621921727 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8206464330499081, 'batch_size': 92, 'attention_heads': 8, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5203274830556134, 'global_pooling': 'max', 'learning_rate': 0.00048535091804720337, 'weight_decay': 0.0003793579792849514, 'beta_0': 0.8655670244317971, 'beta_1': 0.9858992469186513, 'epsilon': 3.447113750553969e-08, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 11:11:35,178] Trial 251 finished with value: 0.9372902186731498 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8293964603197566, 'batch_size': 100, 'attention_heads': 9, 'hidden_dimension': 56, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4992246934602176, 'global_pooling': 'max', 'learning_rate': 0.000729417830673755, 'weight_decay': 0.0006057215737158265, 'beta_0': 0.8602704405001415, 'beta_1': 0.9871471584109336, 'epsilon': 7.648560504443713e-08, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 11:25:04,305] Trial 252 finished with value: 0.9373445865332624 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8274248941929087, 'batch_size': 93, 'attention_heads': 9, 'hidden_dimension': 54, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5158497771561065, 'global_pooling': 'max', 'learning_rate': 0.0006915829932706228, 'weight_decay': 0.0006040548720597059, 'beta_0': 0.8590447687422107, 'beta_1': 0.9871563140339604, 'epsilon': 7.31006463927152e-08, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 11:36:31,313] Trial 253 finished with value: 0.9276100659207498 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8105382877857382, 'batch_size': 88, 'attention_heads': 9, 'hidden_dimension': 47, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3641287571169788, 'global_pooling': 'max', 'learning_rate': 0.0006394556597096496, 'weight_decay': 0.0005275308113987565, 'beta_0': 0.8574169695358989, 'beta_1': 0.9875584599006721, 'epsilon': 5.252686638068647e-08, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 11:48:09,352] Trial 254 finished with value: 0.9362281361631274 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8239569323499282, 'batch_size': 99, 'attention_heads': 9, 'hidden_dimension': 38, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5045944762042162, 'global_pooling': 'max', 'learning_rate': 0.0009065532893502821, 'weight_decay': 0.000618604320761581, 'beta_0': 0.860097291795719, 'beta_1': 0.9866587129398743, 'epsilon': 6.305648964546238e-08, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 11:59:43,307] Trial 255 finished with value: 0.9341907031038826 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8337091339816449, 'batch_size': 78, 'attention_heads': 9, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5128440382931614, 'global_pooling': 'max', 'learning_rate': 0.000552834305728222, 'weight_decay': 0.00043537522932791993, 'beta_0': 0.8620014573046655, 'beta_1': 0.9857273008242003, 'epsilon': 4.4128728986232995e-08, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 12:11:42,776] Trial 256 finished with value: 0.9275123629208475 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8175331927123248, 'batch_size': 92, 'attention_heads': 8, 'hidden_dimension': 51, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5235783190569067, 'global_pooling': 'max', 'learning_rate': 0.0007687872011684721, 'weight_decay': 0.0004858726051756724, 'beta_0': 0.8585237450552781, 'beta_1': 0.9860348545594706, 'epsilon': 8.27068757696777e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 225 with value: 0.9507461710473065.
[I 2025-02-28 12:24:56,304] Trial 257 finished with value: 0.9507714196182853 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8266156715424157, 'batch_size': 75, 'attention_heads': 10, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5171473110049466, 'global_pooling': 'max', 'learning_rate': 0.0009550761101699984, 'weight_decay': 0.0005513969871411423, 'beta_0': 0.8634311870476901, 'beta_1': 0.9883866092533207, 'epsilon': 4.9607682742848286e-08, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 12:39:26,229] Trial 258 finished with value: 0.9348766313389545 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8202922578366575, 'batch_size': 76, 'attention_heads': 11, 'hidden_dimension': 42, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5330230013726631, 'global_pooling': 'max', 'learning_rate': 0.001017073670575269, 'weight_decay': 0.0005306512272252873, 'beta_0': 0.8646401294277545, 'beta_1': 0.9852703369492717, 'epsilon': 3.281708043619503e-08, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 12:50:49,038] Trial 259 finished with value: 0.92585491826235 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8101847285827899, 'batch_size': 69, 'attention_heads': 10, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5302540441316552, 'global_pooling': 'max', 'learning_rate': 0.0009940554517256688, 'weight_decay': 0.0009565734829499641, 'beta_0': 0.8688129105543291, 'beta_1': 0.9883270287784631, 'epsilon': 2.83262368393003e-08, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 3.32 GiB. GPU 0 has a total capacity of 44.56 GiB of which 418.69 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 39.69 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 12:56:49,462] Trial 260 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8373890365499308, 'batch_size': 74, 'attention_heads': 8, 'hidden_dimension': 248, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5261491192737612, 'global_pooling': 'max', 'learning_rate': 0.0003656486570821933, 'weight_decay': 0.00040820856013940797, 'beta_0': 0.8642453283916477, 'beta_1': 0.9876166838169369, 'epsilon': 3.9986261710087445e-08, 'balanced_loss': False, 'epochs': 71, 'early_stopping_patience': 24, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 44.56 GiB of which 498.69 MiB is free. Including non-PyTorch memory, this process has 44.07 GiB memory in use. Of the allocated memory 42.13 GiB is allocated by PyTorch, and 801.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 13:03:07,287] Trial 261 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7973764503010908, 'batch_size': 84, 'attention_heads': 10, 'hidden_dimension': 46, 'number_of_hidden_layers': 2, 'dropout_rate': 0.520941638526666, 'global_pooling': 'max', 'learning_rate': 0.0008488049751714995, 'weight_decay': 0.0007454200003506239, 'beta_0': 0.8724472635568739, 'beta_1': 0.9889472866473589, 'epsilon': 2.521777402553311e-05, 'balanced_loss': False, 'epochs': 60, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 13:12:37,217] Trial 262 finished with value: 0.8709939242423043 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9378283583366657, 'batch_size': 81, 'attention_heads': 9, 'hidden_dimension': 42, 'number_of_hidden_layers': 4, 'dropout_rate': 0.31963607026896457, 'global_pooling': 'max', 'learning_rate': 0.00020500067338091399, 'weight_decay': 0.0004585686758726413, 'beta_0': 0.862322841121816, 'beta_1': 0.9883736216816574, 'epsilon': 5.099192821418674e-08, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 11, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.60 GiB. GPU 0 has a total capacity of 44.56 GiB of which 806.69 MiB is free. Including non-PyTorch memory, this process has 43.77 GiB memory in use. Of the allocated memory 41.89 GiB is allocated by PyTorch, and 744.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 13:19:18,074] Trial 263 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8283480030888589, 'batch_size': 213, 'attention_heads': 8, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3567462754747118, 'global_pooling': 'max', 'learning_rate': 0.0004733890101921108, 'weight_decay': 0.0003457448693183432, 'beta_0': 0.866816675012877, 'beta_1': 0.9928144520250517, 'epsilon': 4.468797285892804e-08, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 62.69 MiB is free. Including non-PyTorch memory, this process has 44.49 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 13:25:13,007] Trial 264 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.842624538961302, 'batch_size': 201, 'attention_heads': 10, 'hidden_dimension': 48, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5083963301439418, 'global_pooling': 'mean', 'learning_rate': 0.001081162904880756, 'weight_decay': 0.0005654015615023706, 'beta_0': 0.8692548483168605, 'beta_1': 0.995514397066399, 'epsilon': 2.950924525332413e-05, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 13:35:27,228] Trial 265 finished with value: 0.9338903255103561 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8182385580420382, 'batch_size': 70, 'attention_heads': 4, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.42934782046068604, 'global_pooling': 'max', 'learning_rate': 0.0009343588708977658, 'weight_decay': 0.0006581759095227661, 'beta_0': 0.8888125206455132, 'beta_1': 0.9880002492717497, 'epsilon': 6.233074861197327e-08, 'balanced_loss': False, 'epochs': 99, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 13:46:19,233] Trial 266 finished with value: 0.9020180389238615 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9627310379313895, 'batch_size': 76, 'attention_heads': 9, 'hidden_dimension': 35, 'number_of_hidden_layers': 4, 'dropout_rate': 0.48606238705243027, 'global_pooling': 'max', 'learning_rate': 0.0006037999832843102, 'weight_decay': 0.00085570031496789, 'beta_0': 0.8634352854670659, 'beta_1': 0.9863276089817417, 'epsilon': 2.0613592154683876e-05, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 2.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.94 GiB is free. Including non-PyTorch memory, this process has 42.61 GiB memory in use. Of the allocated memory 41.21 GiB is allocated by PyTorch, and 262.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 13:52:16,620] Trial 267 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8354166717441748, 'batch_size': 53, 'attention_heads': 11, 'hidden_dimension': 163, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5378984899547161, 'global_pooling': 'max', 'learning_rate': 0.00028772886185207897, 'weight_decay': 0.00027515490910063017, 'beta_0': 0.8661817115373784, 'beta_1': 0.9854226126245041, 'epsilon': 1.6142811163790846e-05, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 14:08:10,090] Trial 268 finished with value: 0.9228888846651787 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9082976048265523, 'batch_size': 41, 'attention_heads': 4, 'hidden_dimension': 187, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5288195587093509, 'global_pooling': 'max', 'learning_rate': 0.0007856568014088881, 'weight_decay': 0.0004633264710013613, 'beta_0': 0.8714751448509739, 'beta_1': 0.9879270403993856, 'epsilon': 4.7666656430780195e-07, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.01 GiB. GPU 0 has a total capacity of 44.56 GiB of which 280.69 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 42.71 GiB is allocated by PyTorch, and 424.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 14:14:03,402] Trial 269 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8021352127809802, 'batch_size': 67, 'attention_heads': 8, 'hidden_dimension': 67, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5180906674820184, 'global_pooling': 'max', 'learning_rate': 0.0011820815756970152, 'weight_decay': 0.00039287687468769625, 'beta_0': 0.8612438345348893, 'beta_1': 0.9848992296752309, 'epsilon': 1.2551791748418949e-07, 'balanced_loss': False, 'epochs': 50, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 14:25:16,609] Trial 270 finished with value: 0.9259162078428795 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8255308213925502, 'batch_size': 184, 'attention_heads': 4, 'hidden_dimension': 51, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5410527843099309, 'global_pooling': 'max', 'learning_rate': 0.0004312313864848281, 'weight_decay': 0.000536907341409499, 'beta_0': 0.8727845749378202, 'beta_1': 0.994950876579554, 'epsilon': 3.873463819407426e-08, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 23, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 14:34:37,874] Trial 271 finished with value: 0.9300207799265956 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8130490679639016, 'batch_size': 72, 'attention_heads': 7, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.33895262934117376, 'global_pooling': 'max', 'learning_rate': 0.0009663066247279671, 'weight_decay': 0.0006800648581665662, 'beta_0': 0.867796986828966, 'beta_1': 0.9861781225400169, 'epsilon': 2.202129368474955e-05, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 14:50:48,006] Trial 272 finished with value: 0.9350357354393785 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8412730169582684, 'batch_size': 60, 'attention_heads': 4, 'hidden_dimension': 59, 'number_of_hidden_layers': 4, 'dropout_rate': 0.47770669459483, 'global_pooling': 'max', 'learning_rate': 0.0006765604501591547, 'weight_decay': 0.0009942988622470765, 'beta_0': 0.8775367243328976, 'beta_1': 0.9965741126666154, 'epsilon': 9.963655957898086e-08, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 4.31 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 42.69 GiB memory in use. Of the allocated memory 40.88 GiB is allocated by PyTorch, and 678.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 14:57:58,441] Trial 273 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8310325440295226, 'batch_size': 221, 'attention_heads': 9, 'hidden_dimension': 124, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5628675558734538, 'global_pooling': 'max', 'learning_rate': 0.000535896350892749, 'weight_decay': 0.0004901153349999301, 'beta_0': 0.8696618823108077, 'beta_1': 0.9822572509290592, 'epsilon': 1.6646074679298462e-07, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 836.69 MiB is free. Including non-PyTorch memory, this process has 43.74 GiB memory in use. Of the allocated memory 42.07 GiB is allocated by PyTorch, and 527.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:06:32,149] Trial 274 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8239413017401829, 'batch_size': 87, 'attention_heads': 9, 'hidden_dimension': 41, 'number_of_hidden_layers': 4, 'dropout_rate': 0.535880942076987, 'global_pooling': 'max', 'learning_rate': 0.0011204751518585048, 'weight_decay': 0.0007795016237604039, 'beta_0': 0.8641215565732263, 'beta_1': 0.9857282694854141, 'epsilon': 2.330132288313087e-08, 'balanced_loss': False, 'epochs': 94, 'early_stopping_patience': 20, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.36 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 43.43 GiB memory in use. Of the allocated memory 41.05 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:15:10,509] Trial 275 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8081846626373427, 'batch_size': 120, 'attention_heads': 8, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5101820378713224, 'global_pooling': 'mean', 'learning_rate': 0.000816089462224876, 'weight_decay': 0.0003256895659650367, 'beta_0': 0.8830897934560545, 'beta_1': 0.9940455074966158, 'epsilon': 5.209952982081947e-08, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 354.69 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 42.84 GiB is allocated by PyTorch, and 226.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:21:11,436] Trial 276 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.7887439202536886, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 208, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4381940548039887, 'global_pooling': 'max', 'learning_rate': 0.0003569490189199434, 'weight_decay': 0.0006063255313141769, 'beta_0': 0.8731987267987247, 'beta_1': 0.9957868713349066, 'epsilon': 3.613387790273582e-05, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 18, 'plateau_patience': 14, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 302.69 MiB is free. Including non-PyTorch memory, this process has 44.26 GiB memory in use. Of the allocated memory 42.60 GiB is allocated by PyTorch, and 520.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:28:26,872] Trial 277 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.697830228005001, 'batch_size': 64, 'attention_heads': 8, 'hidden_dimension': 51, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5268977989314907, 'global_pooling': 'max', 'learning_rate': 0.0012337813041069597, 'weight_decay': 0.000201874545021964, 'beta_0': 0.8566772794803843, 'beta_1': 0.9892877163588478, 'epsilon': 1.8570952832462738e-05, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 808.69 MiB is free. Including non-PyTorch memory, this process has 43.76 GiB memory in use. Of the allocated memory 42.29 GiB is allocated by PyTorch, and 326.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:35:39,728] Trial 278 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.7637980067203133, 'batch_size': 79, 'attention_heads': 10, 'hidden_dimension': 56, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3483595617290453, 'global_pooling': 'max', 'learning_rate': 0.0001609653085009803, 'weight_decay': 0.00041825337577799227, 'beta_0': 0.8660342990546369, 'beta_1': 0.985012243152884, 'epsilon': 7.945564308182335e-06, 'balanced_loss': False, 'epochs': 188, 'early_stopping_patience': 24, 'plateau_patience': 13, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 712.69 MiB is free. Including non-PyTorch memory, this process has 43.86 GiB memory in use. Of the allocated memory 42.53 GiB is allocated by PyTorch, and 177.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 15:41:38,162] Trial 279 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8183111718652332, 'batch_size': 71, 'attention_heads': 4, 'hidden_dimension': 194, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4963674717263134, 'global_pooling': 'max', 'learning_rate': 0.0006938707784164005, 'weight_decay': 6.282459843356561e-06, 'beta_0': 0.8334570698487809, 'beta_1': 0.9866046676657254, 'epsilon': 7.230504781278686e-07, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 3}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 15:51:42,219] Trial 280 finished with value: 0.9228327895969177 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7418030657435342, 'batch_size': 50, 'attention_heads': 4, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5451941295898384, 'global_pooling': 'max', 'learning_rate': 0.0009039492984566536, 'weight_decay': 4.0841229297252746e-05, 'beta_0': 0.8006453265052721, 'beta_1': 0.9947259219029423, 'epsilon': 3.0334515274192024e-08, 'balanced_loss': False, 'epochs': 93, 'early_stopping_patience': 14, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 16:06:46,001] Trial 281 finished with value: 0.9090271246047301 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8956017998633949, 'batch_size': 255, 'attention_heads': 4, 'hidden_dimension': 62, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5584543725374864, 'global_pooling': 'max', 'learning_rate': 0.0005115791459415403, 'weight_decay': 0.0005210116149669817, 'beta_0': 0.8922962844957422, 'beta_1': 0.9961739921091898, 'epsilon': 2.3454802591559e-07, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 22, 'plateau_patience': 14, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 2.21 GiB. GPU 0 has a total capacity of 44.56 GiB of which 390.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 40.63 GiB is allocated by PyTorch, and 2.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 16:14:02,733] Trial 282 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8352013783306379, 'batch_size': 116, 'attention_heads': 15, 'hidden_dimension': 71, 'number_of_hidden_layers': 2, 'dropout_rate': 0.30638132995893713, 'global_pooling': 'max', 'learning_rate': 0.00061812007529612, 'weight_decay': 0.000704739601154645, 'beta_0': 0.8372453789585343, 'beta_1': 0.9938767908041897, 'epsilon': 1.3747084665975988e-05, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 16:22:01,087] Trial 283 finished with value: 0.9361911846450492 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8443779980635409, 'batch_size': 66, 'attention_heads': 9, 'hidden_dimension': 47, 'number_of_hidden_layers': 0, 'dropout_rate': 0.53284307510137, 'global_pooling': 'max', 'learning_rate': 0.0010542627967970394, 'weight_decay': 0.00015871562307201927, 'beta_0': 0.8794492429472137, 'beta_1': 0.992229801674064, 'epsilon': 4.6058196611711515e-08, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 24, 'plateau_patience': 15, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 744.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 646.69 MiB is free. Including non-PyTorch memory, this process has 43.92 GiB memory in use. Of the allocated memory 37.84 GiB is allocated by PyTorch, and 4.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 16:41:17,649] Trial 284 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8255699327006795, 'batch_size': 124, 'attention_heads': 5, 'hidden_dimension': 40, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5203193255451299, 'global_pooling': 'max', 'learning_rate': 0.00038751221212909745, 'weight_decay': 0.0003731445730413135, 'beta_0': 0.8967962645467898, 'beta_1': 0.9886032951095411, 'epsilon': 6.390052772326608e-08, 'balanced_loss': False, 'epochs': 157, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 16:52:16,037] Trial 285 finished with value: 0.9347869377490621 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8153915309276142, 'batch_size': 57, 'attention_heads': 7, 'hidden_dimension': 54, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5738878696110317, 'global_pooling': 'mean', 'learning_rate': 0.0008170020995571705, 'weight_decay': 0.00011865223708082923, 'beta_0': 0.8706875966707964, 'beta_1': 0.9824499645180104, 'epsilon': 2.361248122621241e-05, 'balanced_loss': False, 'epochs': 129, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 17:08:08,950] Trial 286 finished with value: 0.9313093915758353 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8322339136028979, 'batch_size': 83, 'attention_heads': 5, 'hidden_dimension': 44, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5140476088995409, 'global_pooling': 'max', 'learning_rate': 0.0013250175820152127, 'weight_decay': 0.0005685547427995355, 'beta_0': 0.8609015544190883, 'beta_1': 0.9819068125624666, 'epsilon': 1.7092467500706487e-05, 'balanced_loss': False, 'epochs': 57, 'early_stopping_patience': 23, 'plateau_patience': 13, 'plateau_divider': 4}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 17:19:35,469] Trial 287 finished with value: 0.8888141550168924 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.9868710557517423, 'batch_size': 74, 'attention_heads': 4, 'hidden_dimension': 157, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5669610484686114, 'global_pooling': 'max', 'learning_rate': 0.00029419074137780913, 'weight_decay': 0.000801629139839682, 'beta_0': 0.8741178906676887, 'beta_1': 0.9877047131746874, 'epsilon': 2.881523235952737e-07, 'balanced_loss': False, 'epochs': 113, 'early_stopping_patience': 21, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 17:33:30,680] Trial 288 finished with value: 0.9376424235627786 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8393997754541478, 'batch_size': 68, 'attention_heads': 9, 'hidden_dimension': 50, 'number_of_hidden_layers': 4, 'dropout_rate': 0.37416621524880617, 'global_pooling': 'sum', 'learning_rate': 0.0007540993725889779, 'weight_decay': 0.0004474937394144454, 'beta_0': 0.8677339814188119, 'beta_1': 0.9855030291491476, 'epsilon': 1.0416306121565994e-07, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.19 GiB. GPU 0 has a total capacity of 44.56 GiB of which 234.69 MiB is free. Including non-PyTorch memory, this process has 44.32 GiB memory in use. Of the allocated memory 42.83 GiB is allocated by PyTorch, and 348.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 17:43:01,062] Trial 289 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8394839495640315, 'batch_size': 78, 'attention_heads': 9, 'hidden_dimension': 52, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3784021013703584, 'global_pooling': 'sum', 'learning_rate': 0.0009394149671370462, 'weight_decay': 4.899133531859542e-05, 'beta_0': 0.8678355575386916, 'beta_1': 0.9858714740809313, 'epsilon': 1.0527554375648557e-07, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 17:55:10,116] Trial 290 finished with value: 0.9286072227405948 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8229323671285315, 'batch_size': 67, 'attention_heads': 9, 'hidden_dimension': 58, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3284736590584836, 'global_pooling': 'sum', 'learning_rate': 0.0011830052284770467, 'weight_decay': 0.00046433906985553767, 'beta_0': 0.869868735207745, 'beta_1': 0.9854448385792717, 'epsilon': 6.230629376495638e-05, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 21, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 750.69 MiB is free. Including non-PyTorch memory, this process has 43.82 GiB memory in use. Of the allocated memory 41.82 GiB is allocated by PyTorch, and 868.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:01:34,143] Trial 291 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8043218571532661, 'batch_size': 73, 'attention_heads': 10, 'hidden_dimension': 49, 'number_of_hidden_layers': 2, 'dropout_rate': 0.37092229600606524, 'global_pooling': 'max', 'learning_rate': 0.00046061561962581305, 'weight_decay': 0.0006526948185157194, 'beta_0': 0.8658012872342751, 'beta_1': 0.9869716733057233, 'epsilon': 1.1177920078761922e-07, 'balanced_loss': False, 'epochs': 94, 'early_stopping_patience': 21, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 420.69 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 42.67 GiB is allocated by PyTorch, and 334.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:07:33,528] Trial 292 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8314555065499294, 'batch_size': 97, 'attention_heads': 9, 'hidden_dimension': 66, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5409865516462686, 'global_pooling': 'sum', 'learning_rate': 0.0007515888397205729, 'weight_decay': 0.0005505920883313677, 'beta_0': 0.86831194022198, 'beta_1': 0.9898050595355931, 'epsilon': 1.4422569813883256e-07, 'balanced_loss': False, 'epochs': 104, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.38 GiB is free. Including non-PyTorch memory, this process has 43.18 GiB memory in use. Of the allocated memory 41.06 GiB is allocated by PyTorch, and 986.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:16:42,122] Trial 293 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8378803841529483, 'batch_size': 61, 'attention_heads': 8, 'hidden_dimension': 87, 'number_of_hidden_layers': 2, 'dropout_rate': 0.358851307223353, 'global_pooling': 'sum', 'learning_rate': 0.0006093070903990967, 'weight_decay': 0.00046661111046608633, 'beta_0': 0.8769038498317168, 'beta_1': 0.9861455145750841, 'epsilon': 8.126576408251265e-08, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 44.56 GiB of which 836.69 MiB is free. Including non-PyTorch memory, this process has 43.74 GiB memory in use. Of the allocated memory 42.37 GiB is allocated by PyTorch, and 222.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:22:41,146] Trial 294 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8191422896676479, 'batch_size': 106, 'attention_heads': 9, 'hidden_dimension': 55, 'number_of_hidden_layers': 4, 'dropout_rate': 0.3865689695126068, 'global_pooling': 'sum', 'learning_rate': 0.0009848479481546957, 'weight_decay': 0.0008473555770234806, 'beta_0': 0.8816068912878254, 'beta_1': 0.9943151794671321, 'epsilon': 1.53042004974464e-06, 'balanced_loss': False, 'epochs': 64, 'early_stopping_patience': 20, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.32 GiB is free. Including non-PyTorch memory, this process has 43.24 GiB memory in use. Of the allocated memory 41.23 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:29:06,816] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8126200400598528, 'batch_size': 68, 'attention_heads': 9, 'hidden_dimension': 62, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4132141171939776, 'global_pooling': 'sum', 'learning_rate': 0.0014196971537304669, 'weight_decay': 0.0006407412616565198, 'beta_0': 0.8406116365349737, 'beta_1': 0.9856603689299457, 'epsilon': 1.2530697415298954e-07, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 19, 'plateau_patience': 12, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 958.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 918.69 MiB is free. Including non-PyTorch memory, this process has 43.66 GiB memory in use. Of the allocated memory 37.55 GiB is allocated by PyTorch, and 4.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 18:36:48,914] Trial 296 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8461095468634128, 'batch_size': 81, 'attention_heads': 10, 'hidden_dimension': 49, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5029504172317144, 'global_pooling': 'sum', 'learning_rate': 0.00021522167444354065, 'weight_decay': 0.0003956151894607002, 'beta_0': 0.8632432738454456, 'beta_1': 0.9916086463909429, 'epsilon': 9.78954928261094e-08, 'balanced_loss': False, 'epochs': 83, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 18:50:00,032] Trial 297 finished with value: 0.9409967063353902 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.828856621365309, 'batch_size': 87, 'attention_heads': 8, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5477909594713164, 'global_pooling': 'max', 'learning_rate': 0.0007925164073459625, 'weight_decay': 0.0007280510193104273, 'beta_0': 0.8750048195870336, 'beta_1': 0.9932152077535996, 'epsilon': 7.713570502154055e-08, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 19:02:35,272] Trial 298 finished with value: 0.9410800147910803 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.82701630990511, 'batch_size': 90, 'attention_heads': 8, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5542427257946979, 'global_pooling': 'max', 'learning_rate': 0.0011426773828885684, 'weight_decay': 0.0007600263333896023, 'beta_0': 0.8753738264626116, 'beta_1': 0.9950447683961532, 'epsilon': 6.808134239394546e-08, 'balanced_loss': False, 'epochs': 196, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 2.82 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.69 GiB is free. Including non-PyTorch memory, this process has 42.86 GiB memory in use. Of the allocated memory 41.04 GiB is allocated by PyTorch, and 685.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 19:08:39,703] Trial 299 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8266275518721203, 'batch_size': 88, 'attention_heads': 8, 'hidden_dimension': 168, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5526027201480527, 'global_pooling': 'max', 'learning_rate': 0.0012047574574599559, 'weight_decay': 0.000725482739266833, 'beta_0': 0.8759704282861597, 'beta_1': 0.995172445067949, 'epsilon': 6.192086788233664e-08, 'balanced_loss': False, 'epochs': 196, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 3.72 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.14 GiB is free. Including non-PyTorch memory, this process has 43.41 GiB memory in use. Of the allocated memory 41.82 GiB is allocated by PyTorch, and 452.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 19:14:36,285] Trial 300 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8302094029522363, 'batch_size': 93, 'attention_heads': 8, 'hidden_dimension': 226, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5466339936263827, 'global_pooling': 'max', 'learning_rate': 0.0009202192889687582, 'weight_decay': 0.0009055882305312122, 'beta_0': 0.874304685106756, 'beta_1': 0.99365391657879, 'epsilon': 7.814828930613214e-08, 'balanced_loss': False, 'epochs': 102, 'early_stopping_patience': 22, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 19:28:26,751] Trial 301 finished with value: 0.9293358594154513 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8201881591756063, 'batch_size': 87, 'attention_heads': 8, 'hidden_dimension': 41, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5614787690215342, 'global_pooling': 'max', 'learning_rate': 0.0013384349370665473, 'weight_decay': 0.0007777167818632199, 'beta_0': 0.8787933832675654, 'beta_1': 0.9945632743192031, 'epsilon': 6.639081585169759e-08, 'balanced_loss': False, 'epochs': 53, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 19:45:10,166] Trial 302 finished with value: 0.93862422733142 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8116812918902245, 'batch_size': 95, 'attention_heads': 8, 'hidden_dimension': 45, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5502634885070302, 'global_pooling': 'max', 'learning_rate': 0.001504773870919937, 'weight_decay': 0.0007082355923650793, 'beta_0': 0.8718329359833211, 'beta_1': 0.99511430241778, 'epsilon': 6.395053051803874e-06, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 19:56:57,680] Trial 303 finished with value: 0.9187917284758895 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8093607120272699, 'batch_size': 97, 'attention_heads': 8, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5551018664448473, 'global_pooling': 'max', 'learning_rate': 0.0010858206323335732, 'weight_decay': 0.0007161965161036406, 'beta_0': 0.8718336699259267, 'beta_1': 0.9956176616182522, 'epsilon': 2.8691910793660242e-05, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 20:09:07,343] Trial 304 finished with value: 0.9193948828335932 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8017322416626464, 'batch_size': 90, 'attention_heads': 8, 'hidden_dimension': 37, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5501086332481508, 'global_pooling': 'max', 'learning_rate': 0.00108479087108742, 'weight_decay': 0.0008627322025866784, 'beta_0': 0.87324721197797, 'beta_1': 0.9949899275085375, 'epsilon': 4.851541677308772e-06, 'balanced_loss': False, 'epochs': 198, 'early_stopping_patience': 21, 'plateau_patience': 16, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 20:21:25,232] Trial 305 finished with value: 0.9306596735795419 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8145662422733875, 'batch_size': 96, 'attention_heads': 8, 'hidden_dimension': 44, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5372783966617808, 'global_pooling': 'max', 'learning_rate': 0.0008450452821995857, 'weight_decay': 0.000646265029926632, 'beta_0': 0.8710627626415726, 'beta_1': 0.9958428275942687, 'epsilon': 7.795619951739934e-08, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 20:34:40,425] Trial 306 finished with value: 0.9305351791682599 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8252131770833548, 'batch_size': 85, 'attention_heads': 12, 'hidden_dimension': 40, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5562637321232522, 'global_pooling': 'max', 'learning_rate': 0.0015331559763777766, 'weight_decay': 0.0007722592493801067, 'beta_0': 0.8753978939169842, 'beta_1': 0.995099845808774, 'epsilon': 5.602256082784683e-08, 'balanced_loss': False, 'epochs': 103, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
[I 2025-02-28 20:45:23,958] Trial 307 finished with value: 0.9295555252736196 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8222876006526042, 'batch_size': 91, 'attention_heads': 8, 'hidden_dimension': 32, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5441777007885145, 'global_pooling': 'max', 'learning_rate': 0.0005636989768283932, 'weight_decay': 0.0006458585090359773, 'beta_0': 0.8729225015661447, 'beta_1': 0.9953262468125088, 'epsilon': 5.948886943125914e-06, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 20, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 257 with value: 0.9507714196182853.
CUDA out of memory. Tried to allocate 1.44 GiB. GPU 0 has a total capacity of 44.56 GiB of which 82.69 MiB is free. Including non-PyTorch memory, this process has 44.47 GiB memory in use. Of the allocated memory 42.33 GiB is allocated by PyTorch, and 1011.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-28 20:51:29,987] Trial 308 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7946311728502103, 'batch_size': 82, 'attention_heads': 11, 'hidden_dimension': 47, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5286598029614138, 'global_pooling': 'max', 'learning_rate': 0.0007493656221818698, 'weight_decay': 0.0009867385799736702, 'beta_0': 0.8751437018340141, 'beta_1': 0.9968791482603412, 'epsilon': 1.5037845724188553e-05, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 257 with value: 0.9507714196182853.
slurmstepd: error: *** JOB 15063695 ON gpu054 CANCELLED AT 2025-02-28T21:03:02 DUE TO TIME LIMIT ***
