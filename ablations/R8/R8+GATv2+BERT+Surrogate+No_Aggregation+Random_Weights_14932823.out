[I 2025-02-18 12:08:01,730] Using an existing study with name 'R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-Random_Weights-0.0-0.0' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 2.61 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 43.20 GiB memory in use. Of the allocated memory 40.15 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:17:59,700] Trial 17 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.9460302843621626, 'batch_size': 89, 'attention_heads': 12, 'hidden_dimension': 234, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41388127302606703, 'global_pooling': 'sum', 'learning_rate': 0.001235243959513857, 'weight_decay': 0.00025803209755264993, 'beta_0': 0.8427215519050506, 'beta_1': 0.9865956484077749, 'epsilon': 1.999207033896459e-05, 'balanced_loss': False, 'epochs': 74, 'early_stopping_patience': 22, 'plateau_patience': 15, 'plateau_divider': 2}. Best is trial 1 with value: 0.9353883292948859.
CUDA out of memory. Tried to allocate 3.71 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 43.40 GiB memory in use. Of the allocated memory 41.68 GiB is allocated by PyTorch, and 585.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:25:03,110] Trial 18 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8676498966410703, 'batch_size': 67, 'attention_heads': 16, 'hidden_dimension': 178, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4342362381903506, 'global_pooling': 'sum', 'learning_rate': 9.633002812951362e-05, 'weight_decay': 0.0009762491478868123, 'beta_0': 0.8971659016559956, 'beta_1': 0.9950634905290833, 'epsilon': 5.031615601329827e-07, 'balanced_loss': False, 'epochs': 117, 'early_stopping_patience': 23, 'plateau_patience': 12, 'plateau_divider': 6}. Best is trial 1 with value: 0.9353883292948859.
[I 2025-02-18 12:34:18,030] Trial 19 finished with value: 0.9012502368816262 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9563334445744213, 'batch_size': 184, 'attention_heads': 12, 'hidden_dimension': 122, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3670239888078982, 'global_pooling': 'sum', 'learning_rate': 0.012781351764554414, 'weight_decay': 6.245762171917978e-05, 'beta_0': 0.8263404443204572, 'beta_1': 0.9825570247611354, 'epsilon': 9.243660734348854e-06, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 1 with value: 0.9353883292948859.
CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 44.56 GiB of which 624.69 MiB is free. Including non-PyTorch memory, this process has 43.94 GiB memory in use. Of the allocated memory 42.13 GiB is allocated by PyTorch, and 680.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:41:40,701] Trial 20 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8276170425311333, 'batch_size': 111, 'attention_heads': 10, 'hidden_dimension': 40, 'number_of_hidden_layers': 4, 'dropout_rate': 0.527575182517475, 'global_pooling': 'max', 'learning_rate': 0.0007197853900208403, 'weight_decay': 0.00027225256362317234, 'beta_0': 0.8590020905179274, 'beta_1': 0.9855151048251092, 'epsilon': 1.802597631460787e-06, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 24, 'plateau_patience': 23, 'plateau_divider': 5}. Best is trial 1 with value: 0.9353883292948859.
CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.88 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 40.33 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:47:56,522] Trial 21 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9285666184624556, 'batch_size': 227, 'attention_heads': 4, 'hidden_dimension': 228, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3477672621696472, 'global_pooling': 'max', 'learning_rate': 0.00012192092374672984, 'weight_decay': 1.515095407083922e-05, 'beta_0': 0.8466740379736385, 'beta_1': 0.9820061669770577, 'epsilon': 2.9927990921913846e-05, 'balanced_loss': False, 'epochs': 51, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 1 with value: 0.9353883292948859.
[I 2025-02-18 12:56:29,691] Trial 22 finished with value: 0.9159218630899304 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9704879701470259, 'batch_size': 68, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46632554369858814, 'global_pooling': 'mean', 'learning_rate': 0.0008298517094974879, 'weight_decay': 6.152548085358258e-05, 'beta_0': 0.8156363629117434, 'beta_1': 0.9817614939693845, 'epsilon': 3.8650976522372e-06, 'balanced_loss': False, 'epochs': 67, 'early_stopping_patience': 12, 'plateau_patience': 15, 'plateau_divider': 8}. Best is trial 1 with value: 0.9353883292948859.
[I 2025-02-18 13:05:52,291] Trial 23 finished with value: 0.936301954675923 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9732155496347028, 'batch_size': 164, 'attention_heads': 5, 'hidden_dimension': 171, 'number_of_hidden_layers': 3, 'dropout_rate': 0.4700740906820335, 'global_pooling': 'mean', 'learning_rate': 0.000519642201119707, 'weight_decay': 4.03673872830525e-06, 'beta_0': 0.8041201765972239, 'beta_1': 0.9800076717854886, 'epsilon': 4.556418207660355e-06, 'balanced_loss': False, 'epochs': 177, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 23 with value: 0.936301954675923.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 174.69 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB memory in use. Of the allocated memory 42.81 GiB is allocated by PyTorch, and 435.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 13:13:24,781] Trial 24 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9215331231860735, 'batch_size': 164, 'attention_heads': 7, 'hidden_dimension': 171, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4471071131760914, 'global_pooling': 'sum', 'learning_rate': 0.0017973500958217662, 'weight_decay': 6.999300410660135e-06, 'beta_0': 0.8012830744052762, 'beta_1': 0.9879913064102628, 'epsilon': 7.061956737193936e-06, 'balanced_loss': False, 'epochs': 86, 'early_stopping_patience': 16, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 23 with value: 0.936301954675923.
[I 2025-02-18 13:24:13,658] Trial 25 finished with value: 0.8306200760904061 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9824557414352845, 'batch_size': 115, 'attention_heads': 5, 'hidden_dimension': 133, 'number_of_hidden_layers': 2, 'dropout_rate': 0.48940683405698837, 'global_pooling': 'sum', 'learning_rate': 1.6573890483513393e-05, 'weight_decay': 6.959172134234658e-05, 'beta_0': 0.8307061517855947, 'beta_1': 0.9840107256260198, 'epsilon': 7.525898722406679e-07, 'balanced_loss': False, 'epochs': 173, 'early_stopping_patience': 19, 'plateau_patience': 15, 'plateau_divider': 9}. Best is trial 23 with value: 0.936301954675923.
[I 2025-02-18 13:32:29,527] Trial 26 finished with value: 0.8938092019394818 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.970526894935913, 'batch_size': 256, 'attention_heads': 7, 'hidden_dimension': 76, 'number_of_hidden_layers': 3, 'dropout_rate': 0.40566511632092095, 'global_pooling': 'mean', 'learning_rate': 0.0004107852440798244, 'weight_decay': 3.6950395955651225e-06, 'beta_0': 0.8716682348306779, 'beta_1': 0.9817872127587747, 'epsilon': 1.7086496310380202e-07, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 16, 'plateau_patience': 18, 'plateau_divider': 7}. Best is trial 23 with value: 0.936301954675923.
[I 2025-02-18 13:39:42,287] Trial 27 finished with value: 0.4366022060328365 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9994935765098346, 'batch_size': 203, 'attention_heads': 4, 'hidden_dimension': 240, 'number_of_hidden_layers': 4, 'dropout_rate': 0.4397514395630171, 'global_pooling': 'max', 'learning_rate': 0.00828323566755167, 'weight_decay': 2.212457771855963e-05, 'beta_0': 0.8557534713917432, 'beta_1': 0.9813456645587881, 'epsilon': 1.285595774204077e-05, 'balanced_loss': False, 'epochs': 66, 'early_stopping_patience': 23, 'plateau_patience': 16, 'plateau_divider': 9}. Best is trial 23 with value: 0.936301954675923.
CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.68 GiB is free. Including non-PyTorch memory, this process has 41.88 GiB memory in use. Of the allocated memory 40.20 GiB is allocated by PyTorch, and 536.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 13:45:47,921] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.8760044787176894, 'batch_size': 226, 'attention_heads': 8, 'hidden_dimension': 147, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5150472047075085, 'global_pooling': 'max', 'learning_rate': 0.00013528199664872794, 'weight_decay': 0.00018323097662974031, 'beta_0': 0.8407332571920951, 'beta_1': 0.985721253350006, 'epsilon': 1.341196069606771e-06, 'balanced_loss': True, 'epochs': 157, 'early_stopping_patience': 18, 'plateau_patience': 13, 'plateau_divider': 10}. Best is trial 23 with value: 0.936301954675923.
[I 2025-02-18 13:58:17,166] Trial 29 finished with value: 0.9416799236131963 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9241367246172159, 'batch_size': 160, 'attention_heads': 6, 'hidden_dimension': 210, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4720131204920962, 'global_pooling': 'sum', 'learning_rate': 0.00110607287226003, 'weight_decay': 0.0004052703818917837, 'beta_0': 0.8216715037843156, 'beta_1': 0.9837982305042585, 'epsilon': 3.2429933082188836e-05, 'balanced_loss': False, 'epochs': 83, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 29 with value: 0.9416799236131963.
CUDA out of memory. Tried to allocate 4.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.48 GiB is free. Including non-PyTorch memory, this process has 42.07 GiB memory in use. Of the allocated memory 40.60 GiB is allocated by PyTorch, and 331.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 14:04:23,089] Trial 30 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8402880427809072, 'batch_size': 155, 'attention_heads': 8, 'hidden_dimension': 211, 'number_of_hidden_layers': 0, 'dropout_rate': 0.589859064774157, 'global_pooling': 'mean', 'learning_rate': 0.004381527980169307, 'weight_decay': 0.00042630145143457307, 'beta_0': 0.8219548703364822, 'beta_1': 0.9922529609343613, 'epsilon': 4.382488256185845e-05, 'balanced_loss': True, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 29 with value: 0.9416799236131963.
CUDA out of memory. Tried to allocate 4.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.79 GiB is free. Including non-PyTorch memory, this process has 40.77 GiB memory in use. Of the allocated memory 39.01 GiB is allocated by PyTorch, and 617.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 14:11:41,973] Trial 31 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9124440048676236, 'batch_size': 185, 'attention_heads': 9, 'hidden_dimension': 244, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4757090964678372, 'global_pooling': 'max', 'learning_rate': 0.02898523305728184, 'weight_decay': 4.155047539112342e-06, 'beta_0': 0.811516838821011, 'beta_1': 0.9877541869741153, 'epsilon': 1.4042291242777296e-08, 'balanced_loss': False, 'epochs': 125, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 29 with value: 0.9416799236131963.
[I 2025-02-18 14:21:22,468] Trial 32 finished with value: 0.9328695921057706 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9374083654298814, 'batch_size': 161, 'attention_heads': 6, 'hidden_dimension': 199, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4646663960316107, 'global_pooling': 'sum', 'learning_rate': 0.0012419617383648115, 'weight_decay': 0.0001919090116633436, 'beta_0': 0.8478213379501864, 'beta_1': 0.9842152942150322, 'epsilon': 4.980116622650805e-06, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 21, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 29 with value: 0.9416799236131963.
[I 2025-02-18 14:33:18,751] Trial 33 finished with value: 0.9390168383237987 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9532573048786324, 'batch_size': 126, 'attention_heads': 5, 'hidden_dimension': 181, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49425474368000333, 'global_pooling': 'sum', 'learning_rate': 0.0004301963106038461, 'weight_decay': 0.0005054188686028893, 'beta_0': 0.8223806895162337, 'beta_1': 0.9831187367287774, 'epsilon': 2.3647725197397466e-06, 'balanced_loss': False, 'epochs': 80, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 5}. Best is trial 29 with value: 0.9416799236131963.
[I 2025-02-18 14:44:40,857] Trial 34 finished with value: 0.9405683493285816 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9640355568552303, 'batch_size': 125, 'attention_heads': 5, 'hidden_dimension': 224, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5323525696698727, 'global_pooling': 'sum', 'learning_rate': 0.0004103078348907538, 'weight_decay': 0.0009423817585372399, 'beta_0': 0.8069213572816584, 'beta_1': 0.9809810857596446, 'epsilon': 2.2470620652258545e-06, 'balanced_loss': False, 'epochs': 103, 'early_stopping_patience': 19, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 29 with value: 0.9416799236131963.
[I 2025-02-18 14:55:47,220] Trial 35 finished with value: 0.9276806547573853 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9602909134401154, 'batch_size': 125, 'attention_heads': 5, 'hidden_dimension': 218, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5420655914419791, 'global_pooling': 'sum', 'learning_rate': 0.00045138463831309524, 'weight_decay': 0.0008602132411537499, 'beta_0': 0.8056901622536239, 'beta_1': 0.9811046337529618, 'epsilon': 2.740789041549906e-06, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 20, 'plateau_patience': 16, 'plateau_divider': 6}. Best is trial 29 with value: 0.9416799236131963.
CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 92.69 MiB is free. Including non-PyTorch memory, this process has 44.46 GiB memory in use. Of the allocated memory 42.64 GiB is allocated by PyTorch, and 687.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:02:58,260] Trial 36 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.65482832803194, 'batch_size': 147, 'attention_heads': 6, 'hidden_dimension': 186, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5001106242820162, 'global_pooling': 'sum', 'learning_rate': 0.00022216632906261753, 'weight_decay': 0.000517765502078415, 'beta_0': 0.8166983169626402, 'beta_1': 0.9800858451919605, 'epsilon': 3.195786840554708e-07, 'balanced_loss': False, 'epochs': 103, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 29 with value: 0.9416799236131963.
[I 2025-02-18 15:14:43,868] Trial 37 finished with value: 0.9504238381020242 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8843518986488599, 'batch_size': 122, 'attention_heads': 5, 'hidden_dimension': 163, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5515637287227648, 'global_pooling': 'sum', 'learning_rate': 0.0015438838010616602, 'weight_decay': 0.0004119763303210328, 'beta_0': 0.8057602253141242, 'beta_1': 0.983196200508818, 'epsilon': 8.949338943385645e-05, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 15:29:53,008] Trial 38 finished with value: 0.9419612087402692 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8871652111971288, 'batch_size': 127, 'attention_heads': 6, 'hidden_dimension': 159, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5519240593921764, 'global_pooling': 'sum', 'learning_rate': 0.001841832258667839, 'weight_decay': 0.0003817442282310352, 'beta_0': 0.8101008624957998, 'beta_1': 0.9833810587127255, 'epsilon': 5.9852369999385916e-05, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 15:39:50,895] Trial 39 finished with value: 0.9317517882094541 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8730321275925306, 'batch_size': 111, 'attention_heads': 7, 'hidden_dimension': 159, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5672682554953525, 'global_pooling': 'sum', 'learning_rate': 0.006138986257114935, 'weight_decay': 0.0007132990659582262, 'beta_0': 0.8108011273594424, 'beta_1': 0.9839640684184363, 'epsilon': 7.490191658703809e-05, 'balanced_loss': True, 'epochs': 92, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 1.68 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 43.42 GiB memory in use. Of the allocated memory 40.91 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:47:19,106] Trial 40 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7968280693393824, 'batch_size': 78, 'attention_heads': 6, 'hidden_dimension': 110, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5505665694988948, 'global_pooling': 'sum', 'learning_rate': 0.002147015454603229, 'weight_decay': 0.00037081905421538976, 'beta_0': 0.829159017537469, 'beta_1': 0.9905317981809378, 'epsilon': 3.1643088471648846e-05, 'balanced_loss': False, 'epochs': 123, 'early_stopping_patience': 18, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 2.77 GiB. GPU 0 has a total capacity of 44.56 GiB of which 154.69 MiB is free. Including non-PyTorch memory, this process has 44.40 GiB memory in use. Of the allocated memory 40.31 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:54:49,174] Trial 41 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8863688518437018, 'batch_size': 135, 'attention_heads': 8, 'hidden_dimension': 201, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5976313313684999, 'global_pooling': 'sum', 'learning_rate': 0.0013086485733470685, 'weight_decay': 0.0006647027658050625, 'beta_0': 0.8000419798390922, 'beta_1': 0.9863538376303949, 'epsilon': 5.488849661544434e-05, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 2.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.32 GiB is free. Including non-PyTorch memory, this process has 42.23 GiB memory in use. Of the allocated memory 39.62 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 16:02:39,489] Trial 42 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8525336637738113, 'batch_size': 122, 'attention_heads': 5, 'hidden_dimension': 159, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5311017162982771, 'global_pooling': 'sum', 'learning_rate': 0.002791204179356926, 'weight_decay': 0.0004477658915951675, 'beta_0': 0.8221736411552287, 'beta_1': 0.9828853419506407, 'epsilon': 3.269733413141159e-05, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 16:15:57,086] Trial 43 finished with value: 0.9334836041459359 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.913457217428682, 'batch_size': 133, 'attention_heads': 5, 'hidden_dimension': 225, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5653444670276446, 'global_pooling': 'sum', 'learning_rate': 0.0018186175497588186, 'weight_decay': 0.000988298780268904, 'beta_0': 0.8071041777429558, 'beta_1': 0.981075772631048, 'epsilon': 9.342662772227489e-05, 'balanced_loss': False, 'epochs': 84, 'early_stopping_patience': 19, 'plateau_patience': 21, 'plateau_divider': 6}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 16:28:58,630] Trial 44 finished with value: 0.939155878419346 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8991108238383028, 'batch_size': 105, 'attention_heads': 6, 'hidden_dimension': 142, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5033524334817931, 'global_pooling': 'sum', 'learning_rate': 0.0009271904940316439, 'weight_decay': 0.0002383571970868323, 'beta_0': 0.8144494287598077, 'beta_1': 0.9847484053636616, 'epsilon': 2.1334907104535084e-05, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 16:38:21,517] Trial 45 finished with value: 0.9166033826923189 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8950399705655094, 'batch_size': 149, 'attention_heads': 6, 'hidden_dimension': 139, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5394189255310484, 'global_pooling': 'sum', 'learning_rate': 0.004635474287515669, 'weight_decay': 0.00023539863934247433, 'beta_0': 0.8142822627101527, 'beta_1': 0.9848567742221602, 'epsilon': 2.506073210632995e-05, 'balanced_loss': False, 'epochs': 95, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 16:50:42,336] Trial 46 finished with value: 0.9404840424183061 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.858184034920484, 'batch_size': 107, 'attention_heads': 7, 'hidden_dimension': 92, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5779635870122865, 'global_pooling': 'sum', 'learning_rate': 0.0009396555646093834, 'weight_decay': 9.60872492736724e-05, 'beta_0': 0.8113715529532803, 'beta_1': 0.9850318610278416, 'epsilon': 5.63265930325918e-05, 'balanced_loss': False, 'epochs': 90, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.15 GiB is free. Including non-PyTorch memory, this process has 43.41 GiB memory in use. Of the allocated memory 41.44 GiB is allocated by PyTorch, and 837.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 16:58:12,579] Trial 47 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8089638959491874, 'batch_size': 140, 'attention_heads': 7, 'hidden_dimension': 85, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5580084777089018, 'global_pooling': 'sum', 'learning_rate': 0.0029693912176988048, 'weight_decay': 8.438850039894916e-05, 'beta_0': 0.8062934512361052, 'beta_1': 0.9888949683118127, 'epsilon': 6.533437827420785e-05, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 17, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 17:09:38,124] Trial 48 finished with value: 0.9221478630907323 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8533835274363698, 'batch_size': 176, 'attention_heads': 4, 'hidden_dimension': 74, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5798121796052559, 'global_pooling': 'sum', 'learning_rate': 0.001645644668502809, 'weight_decay': 0.0003461346435820762, 'beta_0': 0.8191946812091515, 'beta_1': 0.9835608495878317, 'epsilon': 4.7551617577601084e-05, 'balanced_loss': True, 'epochs': 200, 'early_stopping_patience': 19, 'plateau_patience': 21, 'plateau_divider': 3}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.91 GiB is free. Including non-PyTorch memory, this process has 41.64 GiB memory in use. Of the allocated memory 39.87 GiB is allocated by PyTorch, and 633.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 17:17:06,197] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7360244295090469, 'batch_size': 101, 'attention_heads': 9, 'hidden_dimension': 118, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5749713755038202, 'global_pooling': 'sum', 'learning_rate': 0.00028558206580747133, 'weight_decay': 0.0006320001584196053, 'beta_0': 0.8093629424447408, 'beta_1': 0.985996340946006, 'epsilon': 4.2429349392125546e-05, 'balanced_loss': False, 'epochs': 160, 'early_stopping_patience': 18, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 17:26:35,759] Trial 50 finished with value: 0.925921306334039 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.859780268418266, 'batch_size': 84, 'attention_heads': 6, 'hidden_dimension': 67, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5838661370700173, 'global_pooling': 'sum', 'learning_rate': 0.0006380822804515089, 'weight_decay': 4.3569168399706705e-05, 'beta_0': 0.8255399613153324, 'beta_1': 0.9871971203665446, 'epsilon': 9.899757420973283e-06, 'balanced_loss': False, 'epochs': 70, 'early_stopping_patience': 21, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 17:37:27,907] Trial 51 finished with value: 0.8811438232451556 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8780312405737017, 'batch_size': 123, 'attention_heads': 4, 'hidden_dimension': 94, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5513034854175178, 'global_pooling': 'sum', 'learning_rate': 0.008320064011799003, 'weight_decay': 0.0001665694895896476, 'beta_0': 0.8029653301072923, 'beta_1': 0.9823805273801173, 'epsilon': 2.110281370476589e-08, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 19, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 17:49:34,884] Trial 52 finished with value: 0.9456082195710569 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9036431761732446, 'batch_size': 107, 'attention_heads': 6, 'hidden_dimension': 151, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5179119852987447, 'global_pooling': 'sum', 'learning_rate': 0.0011262162920444199, 'weight_decay': 0.00010555166512948717, 'beta_0': 0.8128966187067356, 'beta_1': 0.9846813783533507, 'epsilon': 1.885741172712422e-05, 'balanced_loss': False, 'epochs': 92, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 18:03:23,534] Trial 53 finished with value: 0.9350260995552745 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9070714765628157, 'batch_size': 118, 'attention_heads': 7, 'hidden_dimension': 163, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5199622400829156, 'global_pooling': 'sum', 'learning_rate': 0.0010399580699657217, 'weight_decay': 0.00010211266192693418, 'beta_0': 0.8124829016187513, 'beta_1': 0.9989828689610365, 'epsilon': 6.961032572451255e-05, 'balanced_loss': False, 'epochs': 89, 'early_stopping_patience': 18, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 18:16:18,099] Trial 54 finished with value: 0.9455487345887572 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8351089441935263, 'batch_size': 106, 'attention_heads': 7, 'hidden_dimension': 57, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5396524160909772, 'global_pooling': 'sum', 'learning_rate': 0.0003056588256801001, 'weight_decay': 0.00012784958441886844, 'beta_0': 0.8191023604623933, 'beta_1': 0.9845608589419961, 'epsilon': 1.7912063509257668e-05, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 20, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 18:31:28,220] Trial 55 finished with value: 0.942874616409727 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8399404759804008, 'batch_size': 76, 'attention_heads': 5, 'hidden_dimension': 150, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5364869332243047, 'global_pooling': 'sum', 'learning_rate': 0.00029642606111559145, 'weight_decay': 0.0003420820580058344, 'beta_0': 0.834116579259118, 'beta_1': 0.9835386252809004, 'epsilon': 1.5646224889886565e-05, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 202.69 MiB is free. Including non-PyTorch memory, this process has 44.36 GiB memory in use. Of the allocated memory 40.89 GiB is allocated by PyTorch, and 2.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:38:51,950] Trial 56 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8368748403086538, 'batch_size': 86, 'attention_heads': 6, 'hidden_dimension': 148, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5094093598595201, 'global_pooling': 'sum', 'learning_rate': 0.0001699540401785713, 'weight_decay': 0.00013211931504217538, 'beta_0': 0.8336799326201769, 'beta_1': 0.9838997812442255, 'epsilon': 1.4714031849902364e-05, 'balanced_loss': False, 'epochs': 127, 'early_stopping_patience': 22, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 43.28 GiB memory in use. Of the allocated memory 41.81 GiB is allocated by PyTorch, and 329.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:46:14,993] Trial 57 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7768630105320259, 'batch_size': 55, 'attention_heads': 13, 'hidden_dimension': 127, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5431603977966549, 'global_pooling': 'sum', 'learning_rate': 9.082325911259264e-05, 'weight_decay': 0.0002931167032824292, 'beta_0': 0.8186364627550393, 'beta_1': 0.9846429200375608, 'epsilon': 1.849398845316785e-05, 'balanced_loss': False, 'epochs': 119, 'early_stopping_patience': 21, 'plateau_patience': 22, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-18 18:46:16,813] Trial 58 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8227027719928883, 'batch_size': 71, 'attention_heads': 4, 'hidden_dimension': 135, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5593560079706782, 'global_pooling': 'sum', 'learning_rate': 0.0006074677015672592, 'weight_decay': 0.00036094434620458494, 'beta_0': 0.8271227412482784, 'beta_1': 0.9870620840851573, 'epsilon': 7.5535663577560995e-06, 'balanced_loss': False, 'epochs': 110, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 19:00:34,276] Trial 59 finished with value: 0.9305995359513685 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8392845845428321, 'batch_size': 33, 'attention_heads': 8, 'hidden_dimension': 107, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5244038074265746, 'global_pooling': 'sum', 'learning_rate': 0.00028906097924146873, 'weight_decay': 0.00021106620118368987, 'beta_0': 0.8347290976070803, 'beta_1': 0.9833586212904692, 'epsilon': 3.331700719231519e-05, 'balanced_loss': True, 'epochs': 130, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
CUDA out of memory. Tried to allocate 4.12 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.39 GiB is free. Including non-PyTorch memory, this process has 41.17 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 19:08:04,716] Trial 60 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8041659028352736, 'batch_size': 98, 'attention_heads': 11, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5105740329402804, 'global_pooling': 'sum', 'learning_rate': 0.001470006078760481, 'weight_decay': 0.0001346588546665144, 'beta_0': 0.824941488581141, 'beta_1': 0.9855769635412037, 'epsilon': 2.2999989149626103e-05, 'balanced_loss': False, 'epochs': 120, 'early_stopping_patience': 15, 'plateau_patience': 20, 'plateau_divider': 6}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 19:24:41,865] Trial 61 finished with value: 0.9277255059072675 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9246355643631643, 'batch_size': 77, 'attention_heads': 7, 'hidden_dimension': 166, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4857612592892607, 'global_pooling': 'sum', 'learning_rate': 0.002438799501500941, 'weight_decay': 5.063588190963826e-05, 'beta_0': 0.8193201182978026, 'beta_1': 0.9886032951095411, 'epsilon': 9.905023863188298e-05, 'balanced_loss': False, 'epochs': 107, 'early_stopping_patience': 21, 'plateau_patience': 23, 'plateau_divider': 7}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 19:35:41,817] Trial 62 finished with value: 0.9412632850058205 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9427774776346142, 'batch_size': 58, 'attention_heads': 5, 'hidden_dimension': 191, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5388102649462342, 'global_pooling': 'sum', 'learning_rate': 0.000350538577435504, 'weight_decay': 0.0007815158852651254, 'beta_0': 0.8090416383238914, 'beta_1': 0.9825538603811024, 'epsilon': 1.1488427461415811e-05, 'balanced_loss': False, 'epochs': 100, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 19:58:09,892] Trial 63 finished with value: 0.9448345792801447 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8898501904816691, 'batch_size': 54, 'attention_heads': 5, 'hidden_dimension': 189, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5362484664387822, 'global_pooling': 'sum', 'learning_rate': 6.867039840515462e-05, 'weight_decay': 0.0007322258175327676, 'beta_0': 0.8036402570554596, 'beta_1': 0.9824869873832115, 'epsilon': 1.1005618175278214e-05, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 20:16:23,633] Trial 64 finished with value: 0.9374154105238444 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8883398763422427, 'batch_size': 47, 'attention_heads': 6, 'hidden_dimension': 171, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5545156457152248, 'global_pooling': 'sum', 'learning_rate': 7.060266965753442e-05, 'weight_decay': 0.0004383874300247949, 'beta_0': 0.8041185639895513, 'beta_1': 0.9816413014364588, 'epsilon': 1.768098430713027e-05, 'balanced_loss': False, 'epochs': 111, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 4}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 20:34:47,528] Trial 65 finished with value: 0.9177847176793763 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8844752427181175, 'batch_size': 60, 'attention_heads': 4, 'hidden_dimension': 205, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5686849655734361, 'global_pooling': 'sum', 'learning_rate': 2.224946402485459e-05, 'weight_decay': 0.0005634502080141185, 'beta_0': 0.8172389043911711, 'beta_1': 0.9821561073117279, 'epsilon': 5.3768815575129005e-06, 'balanced_loss': False, 'epochs': 81, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 20:57:15,533] Trial 66 finished with value: 0.9308106705594505 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8668638922395205, 'batch_size': 91, 'attention_heads': 5, 'hidden_dimension': 155, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5311197524506982, 'global_pooling': 'mean', 'learning_rate': 2.9678008771490888e-05, 'weight_decay': 0.0003038533271670426, 'beta_0': 0.8382180812706241, 'beta_1': 0.9829327310775012, 'epsilon': 3.919743321706847e-05, 'balanced_loss': False, 'epochs': 115, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 21:17:50,198] Trial 67 finished with value: 0.9310514320357617 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9196659653099291, 'batch_size': 156, 'attention_heads': 6, 'hidden_dimension': 182, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3078022340859282, 'global_pooling': 'sum', 'learning_rate': 5.922189721465016e-05, 'weight_decay': 0.0002725921571741403, 'beta_0': 0.813433385333202, 'beta_1': 0.9844422175969012, 'epsilon': 2.7902502220905327e-05, 'balanced_loss': False, 'epochs': 98, 'early_stopping_patience': 21, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 21:40:40,465] Trial 68 finished with value: 0.8847220818970045 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9021779650111424, 'batch_size': 134, 'attention_heads': 5, 'hidden_dimension': 175, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5472230500934475, 'global_pooling': 'sum', 'learning_rate': 1.0942290680543605e-05, 'weight_decay': 2.2946983128139472e-05, 'beta_0': 0.8038674594077568, 'beta_1': 0.9944844767777836, 'epsilon': 7.068715804229441e-06, 'balanced_loss': False, 'epochs': 106, 'early_stopping_patience': 22, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 21:52:21,604] Trial 69 finished with value: 0.9225783524796345 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8435933040376218, 'batch_size': 113, 'attention_heads': 4, 'hidden_dimension': 189, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5231359383074152, 'global_pooling': 'sum', 'learning_rate': 0.00014255118701731457, 'weight_decay': 0.0004094725516389612, 'beta_0': 0.8434567217906421, 'beta_1': 0.9861703077383518, 'epsilon': 8.518249326863375e-06, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 17, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 37 with value: 0.9504238381020242.
[I 2025-02-18 22:02:38,052] Trial 70 finished with value: 0.9542719268220199 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8655463641290966, 'batch_size': 51, 'attention_heads': 6, 'hidden_dimension': 52, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4761900020800823, 'global_pooling': 'max', 'learning_rate': 0.0007075393612533177, 'weight_decay': 0.0005870623307841058, 'beta_0': 0.8318711087992383, 'beta_1': 0.985427932934715, 'epsilon': 1.3593814701133655e-05, 'balanced_loss': True, 'epochs': 87, 'early_stopping_patience': 20, 'plateau_patience': 22, 'plateau_divider': 4}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:10:33,752] Trial 71 finished with value: 0.9443870498120458 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8290798745011392, 'batch_size': 48, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5121993128128893, 'global_pooling': 'max', 'learning_rate': 0.0007496925654567013, 'weight_decay': 0.0006070098907068288, 'beta_0': 0.8303650280744491, 'beta_1': 0.985376813487471, 'epsilon': 1.2736473420530693e-05, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:18:08,541] Trial 72 finished with value: 0.9383612483951758 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8281751793673215, 'batch_size': 49, 'attention_heads': 5, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49581489065827333, 'global_pooling': 'max', 'learning_rate': 0.0005694884486286259, 'weight_decay': 0.0006053908118385141, 'beta_0': 0.8297203034013009, 'beta_1': 0.983497399934802, 'epsilon': 1.5362937504907486e-05, 'balanced_loss': True, 'epochs': 152, 'early_stopping_patience': 18, 'plateau_patience': 25, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:27:15,326] Trial 73 finished with value: 0.9453257973965036 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7865693705365568, 'batch_size': 66, 'attention_heads': 5, 'hidden_dimension': 59, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5128394208814063, 'global_pooling': 'max', 'learning_rate': 0.0007487453914733546, 'weight_decay': 0.0007188896368537902, 'beta_0': 0.8377489767812911, 'beta_1': 0.9853177664128239, 'epsilon': 1.2558447339568292e-05, 'balanced_loss': True, 'epochs': 136, 'early_stopping_patience': 20, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:36:26,617] Trial 74 finished with value: 0.9481732122717648 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7555545975154193, 'batch_size': 38, 'attention_heads': 5, 'hidden_dimension': 54, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4803504047675266, 'global_pooling': 'max', 'learning_rate': 0.0007594385504687234, 'weight_decay': 0.0007612656186622874, 'beta_0': 0.8324846040015416, 'beta_1': 0.9873241481353794, 'epsilon': 3.37205578378772e-06, 'balanced_loss': True, 'epochs': 147, 'early_stopping_patience': 20, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:46:24,908] Trial 75 finished with value: 0.936741834488722 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7513202766202266, 'batch_size': 37, 'attention_heads': 4, 'hidden_dimension': 55, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4801764097085605, 'global_pooling': 'max', 'learning_rate': 0.0008576512752361331, 'weight_decay': 0.0007823255495487219, 'beta_0': 0.8375657295404688, 'beta_1': 0.9852800118295549, 'epsilon': 3.3093484446724135e-06, 'balanced_loss': True, 'epochs': 162, 'early_stopping_patience': 20, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 22:56:14,322] Trial 76 finished with value: 0.951560229454514 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7671669553735414, 'batch_size': 54, 'attention_heads': 5, 'hidden_dimension': 54, 'number_of_hidden_layers': 1, 'dropout_rate': 0.46035811846177993, 'global_pooling': 'max', 'learning_rate': 0.0007435420590838287, 'weight_decay': 0.0005475652140468104, 'beta_0': 0.8413625609482512, 'beta_1': 0.9869735096770348, 'epsilon': 1.12153124143778e-05, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-18 22:56:16,176] Trial 77 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7596859602613977, 'batch_size': 60, 'attention_heads': 6, 'hidden_dimension': 53, 'number_of_hidden_layers': 1, 'dropout_rate': 0.46653773982250485, 'global_pooling': 'max', 'learning_rate': 0.00017900331487337364, 'weight_decay': 0.0005089842943757528, 'beta_0': 0.8518651277653697, 'beta_1': 0.9869345173150321, 'epsilon': 1.0056917988657774e-05, 'balanced_loss': True, 'epochs': 143, 'early_stopping_patience': 20, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 23:04:48,846] Trial 78 finished with value: 0.9348697629958892 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7902233139876822, 'batch_size': 41, 'attention_heads': 4, 'hidden_dimension': 45, 'number_of_hidden_layers': 1, 'dropout_rate': 0.4517546351679527, 'global_pooling': 'max', 'learning_rate': 0.0005052512485152371, 'weight_decay': 0.0007111141997436088, 'beta_0': 0.8444187722040679, 'beta_1': 0.9865042109615821, 'epsilon': 6.161236816702491e-06, 'balanced_loss': True, 'epochs': 134, 'early_stopping_patience': 19, 'plateau_patience': 24, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
CUDA out of memory. Tried to allocate 980.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 962.69 MiB is free. Including non-PyTorch memory, this process has 43.61 GiB memory in use. Of the allocated memory 41.68 GiB is allocated by PyTorch, and 804.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 23:11:01,567] Trial 79 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7196123128310612, 'batch_size': 66, 'attention_heads': 5, 'hidden_dimension': 64, 'number_of_hidden_layers': 1, 'dropout_rate': 0.46062375480089146, 'global_pooling': 'max', 'learning_rate': 0.001186612792201424, 'weight_decay': 0.0007914615950443047, 'beta_0': 0.8392919109282178, 'beta_1': 0.9873448106571856, 'epsilon': 3.534191954469823e-06, 'balanced_loss': True, 'epochs': 150, 'early_stopping_patience': 19, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 70 with value: 0.9542719268220199.
CUDA out of memory. Tried to allocate 958.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 804.69 MiB is free. Including non-PyTorch memory, this process has 43.77 GiB memory in use. Of the allocated memory 41.86 GiB is allocated by PyTorch, and 777.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 23:17:12,380] Trial 80 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7101332180509776, 'batch_size': 52, 'attention_heads': 7, 'hidden_dimension': 59, 'number_of_hidden_layers': 2, 'dropout_rate': 0.4318240290666146, 'global_pooling': 'max', 'learning_rate': 0.0035729819982989377, 'weight_decay': 0.0005050815756518388, 'beta_0': 0.8947772966949517, 'beta_1': 0.9884262541190182, 'epsilon': 4.608342610096537e-06, 'balanced_loss': True, 'epochs': 183, 'early_stopping_patience': 11, 'plateau_patience': 25, 'plateau_divider': 4}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 23:26:14,671] Trial 81 finished with value: 0.9484620563812453 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.770828744509055, 'batch_size': 43, 'attention_heads': 10, 'hidden_dimension': 44, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4792295621810191, 'global_pooling': 'max', 'learning_rate': 0.0007553070618042778, 'weight_decay': 0.0009915655036284443, 'beta_0': 0.860345663916741, 'beta_1': 0.9896251064064084, 'epsilon': 1.8162424604015973e-06, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 20, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 23:35:00,627] Trial 82 finished with value: 0.9452501749709978 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7621817733537273, 'batch_size': 39, 'attention_heads': 10, 'hidden_dimension': 40, 'number_of_hidden_layers': 0, 'dropout_rate': 0.44326894164976216, 'global_pooling': 'max', 'learning_rate': 0.00067395482660811, 'weight_decay': 0.000856402137743213, 'beta_0': 0.8622231926204039, 'beta_1': 0.989952138726598, 'epsilon': 5.82153464904208e-06, 'balanced_loss': True, 'epochs': 170, 'early_stopping_patience': 20, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 23:44:15,039] Trial 83 finished with value: 0.9455003972731495 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7557666626096622, 'batch_size': 42, 'attention_heads': 10, 'hidden_dimension': 41, 'number_of_hidden_layers': 0, 'dropout_rate': 0.44161251053325234, 'global_pooling': 'max', 'learning_rate': 0.00037229141392010193, 'weight_decay': 0.0008800068378624371, 'beta_0': 0.8688144248719145, 'beta_1': 0.990342113630669, 'epsilon': 7.000570495973513e-07, 'balanced_loss': True, 'epochs': 167, 'early_stopping_patience': 20, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
[I 2025-02-18 23:54:15,890] Trial 84 finished with value: 0.9514556314792557 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7681906241265849, 'batch_size': 32, 'attention_heads': 10, 'hidden_dimension': 49, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4279283861129337, 'global_pooling': 'max', 'learning_rate': 0.00037817781216402076, 'weight_decay': 0.0004962943466069031, 'beta_0': 0.8703262368089273, 'beta_1': 0.9916210849065357, 'epsilon': 1.0244017024653124e-06, 'balanced_loss': True, 'epochs': 178, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
CUDA out of memory. Tried to allocate 1.31 GiB. GPU 0 has a total capacity of 44.56 GiB of which 646.69 MiB is free. Including non-PyTorch memory, this process has 43.92 GiB memory in use. Of the allocated memory 41.23 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 00:01:42,670] Trial 85 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7486433423701739, 'batch_size': 32, 'attention_heads': 10, 'hidden_dimension': 43, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4135527203011969, 'global_pooling': 'max', 'learning_rate': 0.0003618517615682481, 'weight_decay': 0.000976589230246074, 'beta_0': 0.8718199812321322, 'beta_1': 0.9915253586657802, 'epsilon': 7.323153796312069e-07, 'balanced_loss': True, 'epochs': 177, 'early_stopping_patience': 18, 'plateau_patience': 23, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 44.56 GiB of which 938.69 MiB is free. Including non-PyTorch memory, this process has 43.64 GiB memory in use. Of the allocated memory 41.64 GiB is allocated by PyTorch, and 867.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-19 00:07:57,406] Trial 86 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7391364483494484, 'batch_size': 44, 'attention_heads': 12, 'hidden_dimension': 51, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3883564168101528, 'global_pooling': 'max', 'learning_rate': 0.0005103810676170373, 'weight_decay': 0.0004733552320482907, 'beta_0': 0.8803929293597489, 'beta_1': 0.9908342850138576, 'epsilon': 1.5514327115846012e-06, 'balanced_loss': True, 'epochs': 166, 'early_stopping_patience': 19, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 70 with value: 0.9542719268220199.
slurmstepd: error: *** JOB 14932823 ON gpu052 CANCELLED AT 2025-02-19T00:08:02 DUE TO TIME LIMIT ***
