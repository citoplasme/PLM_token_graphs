[I 2025-02-18 09:53:09,763] A new study created in RDB with name: R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-Single_Unitary_Weight-0.0-0.0
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 42.23 GiB memory in use. Of the allocated memory 39.90 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:00:29,551] Trial 0 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.7564817426134086, 'batch_size': 150, 'attention_heads': 9, 'hidden_dimension': 97, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34184815819561254, 'global_pooling': 'max', 'learning_rate': 0.013826232179369865, 'weight_decay': 3.972110727381911e-06, 'beta_0': 0.849951952030185, 'beta_1': 0.9912118037965686, 'epsilon': 1.5339162591163588e-08, 'balanced_loss': True, 'epochs': 59, 'early_stopping_patience': 25, 'plateau_patience': 25, 'plateau_divider': 9}. Best is trial 0 with value: -1.0.
[I 2025-02-18 10:09:46,691] Trial 1 finished with value: 0.936619760277766 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9788152345580505, 'batch_size': 233, 'attention_heads': 11, 'hidden_dimension': 239, 'number_of_hidden_layers': 0, 'dropout_rate': 0.35879485872574357, 'global_pooling': 'max', 'learning_rate': 0.00012172958098369953, 'weight_decay': 0.0003063462210622083, 'beta_0': 0.834331843801655, 'beta_1': 0.9853009566677808, 'epsilon': 1.4817820606039087e-06, 'balanced_loss': False, 'epochs': 61, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 1 with value: 0.936619760277766.
[I 2025-02-18 10:19:00,031] Trial 2 finished with value: 0.9316219649640507 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9605155877742886, 'batch_size': 138, 'attention_heads': 5, 'hidden_dimension': 192, 'number_of_hidden_layers': 3, 'dropout_rate': 0.46838315927084884, 'global_pooling': 'mean', 'learning_rate': 0.0005130551760589835, 'weight_decay': 1.1919481947918734e-06, 'beta_0': 0.8102310933924105, 'beta_1': 0.98059161803998, 'epsilon': 3.512704726270843e-06, 'balanced_loss': False, 'epochs': 187, 'early_stopping_patience': 13, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 12.53 GiB. GPU 0 has a total capacity of 44.56 GiB of which 12.26 GiB is free. Including non-PyTorch memory, this process has 32.30 GiB memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:24:59,001] Trial 3 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7297750275380542, 'batch_size': 128, 'attention_heads': 14, 'hidden_dimension': 225, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4532241907732697, 'global_pooling': 'mean', 'learning_rate': 0.00022410971619109496, 'weight_decay': 0.0006741074265640696, 'beta_0': 0.8310413476654125, 'beta_1': 0.9898114758541204, 'epsilon': 6.487477066058673e-06, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 14, 'plateau_patience': 17, 'plateau_divider': 4}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacity of 44.56 GiB of which 6.27 GiB is free. Including non-PyTorch memory, this process has 38.29 GiB memory in use. Of the allocated memory 36.44 GiB is allocated by PyTorch, and 713.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:30:50,704] Trial 4 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7787204186204115, 'batch_size': 174, 'attention_heads': 12, 'hidden_dimension': 152, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5505907486767714, 'global_pooling': 'mean', 'learning_rate': 0.002309786149269356, 'weight_decay': 0.00010781845035122267, 'beta_0': 0.8015645397505602, 'beta_1': 0.9896841863656863, 'epsilon': 8.053471030316087e-08, 'balanced_loss': True, 'epochs': 154, 'early_stopping_patience': 16, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 7.61 GiB. GPU 0 has a total capacity of 44.56 GiB of which 104.69 MiB is free. Including non-PyTorch memory, this process has 44.45 GiB memory in use. Of the allocated memory 42.67 GiB is allocated by PyTorch, and 651.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:36:42,625] Trial 5 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9040772280477951, 'batch_size': 233, 'attention_heads': 15, 'hidden_dimension': 207, 'number_of_hidden_layers': 3, 'dropout_rate': 0.3252419894985146, 'global_pooling': 'sum', 'learning_rate': 1.0883991813938131e-05, 'weight_decay': 2.015647705936503e-06, 'beta_0': 0.8650272248026284, 'beta_1': 0.9800952543380481, 'epsilon': 4.397766894483953e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 4}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 4.51 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.69 GiB is free. Including non-PyTorch memory, this process has 41.86 GiB memory in use. Of the allocated memory 40.50 GiB is allocated by PyTorch, and 219.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:43:22,699] Trial 6 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8519105905803794, 'batch_size': 142, 'attention_heads': 6, 'hidden_dimension': 194, 'number_of_hidden_layers': 1, 'dropout_rate': 0.30729478992943615, 'global_pooling': 'max', 'learning_rate': 0.06542056762893128, 'weight_decay': 0.0005553837526912237, 'beta_0': 0.8356502322469728, 'beta_1': 0.9802909082956842, 'epsilon': 5.167425813322413e-05, 'balanced_loss': False, 'epochs': 195, 'early_stopping_patience': 23, 'plateau_patience': 14, 'plateau_divider': 5}. Best is trial 1 with value: 0.936619760277766.
The selected strides are greater or equal to the total chunk size.
[I 2025-02-18 10:43:24,313] Trial 7 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7758183080154022, 'batch_size': 98, 'attention_heads': 14, 'hidden_dimension': 214, 'number_of_hidden_layers': 4, 'dropout_rate': 0.5739721657669414, 'global_pooling': 'max', 'learning_rate': 0.0039797493741031125, 'weight_decay': 0.0001276146788173022, 'beta_0': 0.8786113098385785, 'beta_1': 0.996892198716152, 'epsilon': 2.248954284391446e-07, 'balanced_loss': True, 'epochs': 137, 'early_stopping_patience': 10, 'plateau_patience': 17, 'plateau_divider': 6}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 2.82 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.40 GiB is free. Including non-PyTorch memory, this process has 43.16 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 385.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 10:49:25,554] Trial 8 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 128, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.9041247058895944, 'batch_size': 251, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 3, 'dropout_rate': 0.38124967537862225, 'global_pooling': 'mean', 'learning_rate': 0.07089141723796885, 'weight_decay': 0.0003220626495993124, 'beta_0': 0.8683420313684149, 'beta_1': 0.9877260389162159, 'epsilon': 4.933751600448336e-08, 'balanced_loss': False, 'epochs': 132, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 4}. Best is trial 1 with value: 0.936619760277766.
[I 2025-02-18 11:00:51,279] Trial 9 finished with value: 0.9332598431713155 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8220606401321402, 'batch_size': 138, 'attention_heads': 6, 'hidden_dimension': 129, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48475502941566495, 'global_pooling': 'mean', 'learning_rate': 0.003187422711813414, 'weight_decay': 3.2315343430749745e-05, 'beta_0': 0.8849150937783302, 'beta_1': 0.9924741264147013, 'epsilon': 4.484744524732786e-08, 'balanced_loss': False, 'epochs': 54, 'early_stopping_patience': 19, 'plateau_patience': 25, 'plateau_divider': 7}. Best is trial 1 with value: 0.936619760277766.
[I 2025-02-18 11:09:41,144] Trial 10 finished with value: 0.7892324562173884 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9943461369090066, 'batch_size': 40, 'attention_heads': 9, 'hidden_dimension': 48, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3977762266143904, 'global_pooling': 'sum', 'learning_rate': 4.519442811197253e-05, 'weight_decay': 1.391414427795109e-05, 'beta_0': 0.8215105131549126, 'beta_1': 0.9849479143747095, 'epsilon': 5.948652419846486e-07, 'balanced_loss': True, 'epochs': 91, 'early_stopping_patience': 25, 'plateau_patience': 10, 'plateau_divider': 2}. Best is trial 1 with value: 0.936619760277766.
CUDA out of memory. Tried to allocate 7.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 404.69 MiB is free. Including non-PyTorch memory, this process has 44.16 GiB memory in use. Of the allocated memory 42.02 GiB is allocated by PyTorch, and 1011.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 11:15:42,091] Trial 11 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.6617547238682159, 'batch_size': 205, 'attention_heads': 4, 'hidden_dimension': 252, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5093183654188389, 'global_pooling': 'max', 'learning_rate': 0.00012386356736714718, 'weight_decay': 2.511405901881224e-05, 'beta_0': 0.8994794194881102, 'beta_1': 0.9951767732392878, 'epsilon': 1.5282726063969226e-06, 'balanced_loss': False, 'epochs': 52, 'early_stopping_patience': 20, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 1 with value: 0.936619760277766.
[I 2025-02-18 11:24:43,409] Trial 12 finished with value: 0.9432612469931481 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.852842321652628, 'batch_size': 94, 'attention_heads': 7, 'hidden_dimension': 145, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4055270473193357, 'global_pooling': 'mean', 'learning_rate': 0.0010929014114300284, 'weight_decay': 9.852540188880756e-05, 'beta_0': 0.8514603055031648, 'beta_1': 0.9936937006888107, 'epsilon': 1.5709224758814656e-05, 'balanced_loss': False, 'epochs': 82, 'early_stopping_patience': 19, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 12 with value: 0.9432612469931481.
[I 2025-02-18 11:33:57,578] Trial 13 finished with value: 0.9492995410195262 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9141669233316818, 'batch_size': 68, 'attention_heads': 11, 'hidden_dimension': 161, 'number_of_hidden_layers': 0, 'dropout_rate': 0.40531227134494735, 'global_pooling': 'max', 'learning_rate': 0.0006995486940197135, 'weight_decay': 0.0001586041083025955, 'beta_0': 0.8542582721938591, 'beta_1': 0.9855151048251092, 'epsilon': 2.0410890027900124e-05, 'balanced_loss': False, 'epochs': 91, 'early_stopping_patience': 17, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 11:46:31,300] Trial 14 finished with value: 0.9351851781782358 and parameters: {'left_stride': 0, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.911884866290284, 'batch_size': 62, 'attention_heads': 8, 'hidden_dimension': 157, 'number_of_hidden_layers': 2, 'dropout_rate': 0.41478485428531203, 'global_pooling': 'sum', 'learning_rate': 0.0011066623705286787, 'weight_decay': 0.00010625488232933266, 'beta_0': 0.8515247141640885, 'beta_1': 0.998881893098348, 'epsilon': 5.636220165473943e-05, 'balanced_loss': False, 'epochs': 96, 'early_stopping_patience': 17, 'plateau_patience': 19, 'plateau_divider': 10}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 11:55:42,964] Trial 15 finished with value: 0.9334874542249982 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8535623637200487, 'batch_size': 90, 'attention_heads': 7, 'hidden_dimension': 162, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4247081024249904, 'global_pooling': 'mean', 'learning_rate': 0.0005550150273001859, 'weight_decay': 5.125266198304268e-05, 'beta_0': 0.8606876563219761, 'beta_1': 0.9936246423084829, 'epsilon': 1.4081206827145327e-05, 'balanced_loss': False, 'epochs': 97, 'early_stopping_patience': 16, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 12:08:16,343] Trial 16 finished with value: 0.6214950907312637 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9359465303139326, 'batch_size': 86, 'attention_heads': 12, 'hidden_dimension': 119, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3646656056046315, 'global_pooling': 'max', 'learning_rate': 0.008437826623666735, 'weight_decay': 1.131972459279219e-05, 'beta_0': 0.842576955656521, 'beta_1': 0.9858708325128167, 'epsilon': 1.1598630173023627e-05, 'balanced_loss': False, 'epochs': 85, 'early_stopping_patience': 19, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 12:15:58,232] Trial 17 finished with value: 0.926164352567618 and parameters: {'left_stride': 128, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.863459822052392, 'batch_size': 42, 'attention_heads': 8, 'hidden_dimension': 74, 'number_of_hidden_layers': 0, 'dropout_rate': 0.43195035576350244, 'global_pooling': 'max', 'learning_rate': 0.016498718119490358, 'weight_decay': 0.00019823235013558693, 'beta_0': 0.8567013535422588, 'beta_1': 0.9829469332485619, 'epsilon': 8.726317095919483e-05, 'balanced_loss': False, 'epochs': 112, 'early_stopping_patience': 21, 'plateau_patience': 13, 'plateau_divider': 9}. Best is trial 13 with value: 0.9492995410195262.
CUDA out of memory. Tried to allocate 4.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 236.69 MiB is free. Including non-PyTorch memory, this process has 44.32 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 193.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:22:18,724] Trial 18 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.8135779304753067, 'batch_size': 74, 'attention_heads': 12, 'hidden_dimension': 177, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5072209626369821, 'global_pooling': 'mean', 'learning_rate': 0.0008518018712075429, 'weight_decay': 5.79777479411322e-05, 'beta_0': 0.8770990442907146, 'beta_1': 0.9878892395496247, 'epsilon': 1.9669096550896284e-05, 'balanced_loss': True, 'epochs': 75, 'early_stopping_patience': 18, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 13 with value: 0.9492995410195262.
CUDA out of memory. Tried to allocate 8.87 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.25 GiB is free. Including non-PyTorch memory, this process has 42.30 GiB memory in use. Of the allocated memory 40.27 GiB is allocated by PyTorch, and 903.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 12:28:16,035] Trial 19 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.7011726127843165, 'batch_size': 119, 'attention_heads': 16, 'hidden_dimension': 138, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3983496372494011, 'global_pooling': 'sum', 'learning_rate': 2.6605497428391212e-05, 'weight_decay': 7.332112139154073e-06, 'beta_0': 0.820269387461979, 'beta_1': 0.9949434335242437, 'epsilon': 3.0153175473575412e-05, 'balanced_loss': False, 'epochs': 118, 'early_stopping_patience': 15, 'plateau_patience': 23, 'plateau_divider': 10}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 12:36:12,473] Trial 20 finished with value: 0.9380472265677493 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9377571618315975, 'batch_size': 106, 'attention_heads': 10, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4528479041085119, 'global_pooling': 'max', 'learning_rate': 0.00031119790800285263, 'weight_decay': 0.0008497307695257274, 'beta_0': 0.8438461240383416, 'beta_1': 0.9830190573582772, 'epsilon': 3.813505499149013e-06, 'balanced_loss': False, 'epochs': 73, 'early_stopping_patience': 11, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 12:43:58,963] Trial 21 finished with value: 0.9325058130893387 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9359192584077144, 'batch_size': 109, 'attention_heads': 10, 'hidden_dimension': 36, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4425772540258605, 'global_pooling': 'max', 'learning_rate': 0.0002735897033821394, 'weight_decay': 0.0009787479460993081, 'beta_0': 0.8432281587753271, 'beta_1': 0.9829850080017878, 'epsilon': 5.298561378787181e-06, 'balanced_loss': False, 'epochs': 77, 'early_stopping_patience': 10, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 12:52:13,877] Trial 22 finished with value: 0.948101541460628 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8935592760826828, 'batch_size': 60, 'attention_heads': 11, 'hidden_dimension': 71, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4843402343993413, 'global_pooling': 'max', 'learning_rate': 0.0015733386898416596, 'weight_decay': 0.000310478390396858, 'beta_0': 0.8450554749855028, 'beta_1': 0.9833764454182105, 'epsilon': 2.8120222182876615e-06, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 12, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 13:03:31,986] Trial 23 finished with value: 0.9476190970620121 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8862914380655081, 'batch_size': 63, 'attention_heads': 13, 'hidden_dimension': 90, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5267013377087576, 'global_pooling': 'max', 'learning_rate': 0.001422807355907181, 'weight_decay': 0.00030245903396143115, 'beta_0': 0.8551013458018368, 'beta_1': 0.9871663065890943, 'epsilon': 9.284196393042104e-06, 'balanced_loss': False, 'epochs': 108, 'early_stopping_patience': 12, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 13 with value: 0.9492995410195262.
[I 2025-02-18 13:13:15,579] Trial 24 finished with value: 0.9502522775403794 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8807395655660483, 'batch_size': 59, 'attention_heads': 13, 'hidden_dimension': 66, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5249552574174107, 'global_pooling': 'max', 'learning_rate': 0.0020155334283207763, 'weight_decay': 0.00027043612392828114, 'beta_0': 0.8732772690247655, 'beta_1': 0.9871680263984631, 'epsilon': 5.58158341809808e-07, 'balanced_loss': False, 'epochs': 109, 'early_stopping_patience': 12, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 24 with value: 0.9502522775403794.
[I 2025-02-18 13:22:54,701] Trial 25 finished with value: 0.8640213300550155 and parameters: {'left_stride': 128, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8851818864688522, 'batch_size': 54, 'attention_heads': 13, 'hidden_dimension': 70, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5814380479649938, 'global_pooling': 'max', 'learning_rate': 0.005544662430189828, 'weight_decay': 0.0004365401249446073, 'beta_0': 0.8727151585550317, 'beta_1': 0.9847523713727196, 'epsilon': 4.439089225185599e-07, 'balanced_loss': False, 'epochs': 105, 'early_stopping_patience': 12, 'plateau_patience': 15, 'plateau_divider': 7}. Best is trial 24 with value: 0.9502522775403794.
[I 2025-02-18 13:33:09,382] Trial 26 finished with value: 0.3367695833446559 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8115394701978963, 'batch_size': 38, 'attention_heads': 11, 'hidden_dimension': 64, 'number_of_hidden_layers': 1, 'dropout_rate': 0.48303302210213694, 'global_pooling': 'max', 'learning_rate': 0.02517772589853204, 'weight_decay': 0.0001825592273316722, 'beta_0': 0.8866224839484564, 'beta_1': 0.982129946011773, 'epsilon': 1.821374781770293e-06, 'balanced_loss': True, 'epochs': 122, 'early_stopping_patience': 14, 'plateau_patience': 21, 'plateau_divider': 6}. Best is trial 24 with value: 0.9502522775403794.
[I 2025-02-18 13:41:10,268] Trial 27 finished with value: 0.9406765765819229 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9542836922288893, 'batch_size': 74, 'attention_heads': 13, 'hidden_dimension': 113, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5383486212560882, 'global_pooling': 'max', 'learning_rate': 0.0027897901836989804, 'weight_decay': 5.5440725912458336e-05, 'beta_0': 0.8967420519905267, 'beta_1': 0.9869618201389795, 'epsilon': 2.13342907289084e-07, 'balanced_loss': False, 'epochs': 169, 'early_stopping_patience': 11, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 24 with value: 0.9502522775403794.
CUDA out of memory. Tried to allocate 2.98 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 41.65 GiB memory in use. Of the allocated memory 37.44 GiB is allocated by PyTorch, and 3.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 13:48:49,774] Trial 28 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8747993628218155, 'batch_size': 165, 'attention_heads': 11, 'hidden_dimension': 86, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5035779676195437, 'global_pooling': 'max', 'learning_rate': 0.0018418137913473092, 'weight_decay': 0.00019579994129138224, 'beta_0': 0.8600525540500152, 'beta_1': 0.9838189907248829, 'epsilon': 7.230435455975362e-07, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 15, 'plateau_patience': 22, 'plateau_divider': 7}. Best is trial 24 with value: 0.9502522775403794.
[I 2025-02-18 13:57:20,592] Trial 29 finished with value: 0.9066925333272536 and parameters: {'left_stride': 32, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.919445878552545, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 62, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5606508225128163, 'global_pooling': 'max', 'learning_rate': 0.008205183493069592, 'weight_decay': 0.00041604000311308304, 'beta_0': 0.8286985814054229, 'beta_1': 0.9887670620073399, 'epsilon': 2.95156915897182e-07, 'balanced_loss': True, 'epochs': 129, 'early_stopping_patience': 13, 'plateau_patience': 24, 'plateau_divider': 9}. Best is trial 24 with value: 0.9502522775403794.
[I 2025-02-18 14:12:35,721] Trial 30 finished with value: 0.9567749397755236 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8386015812836483, 'batch_size': 76, 'attention_heads': 14, 'hidden_dimension': 95, 'number_of_hidden_layers': 1, 'dropout_rate': 0.594009435123774, 'global_pooling': 'max', 'learning_rate': 0.0005657975578611044, 'weight_decay': 0.0001745669575257965, 'beta_0': 0.8670253445746536, 'beta_1': 0.9913225020999681, 'epsilon': 3.230843142659727e-06, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 30 with value: 0.9567749397755236.
[I 2025-02-18 14:27:55,537] Trial 31 finished with value: 0.948773603502435 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8346227986603971, 'batch_size': 74, 'attention_heads': 14, 'hidden_dimension': 97, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5889296528350049, 'global_pooling': 'max', 'learning_rate': 0.0006836845733389122, 'weight_decay': 0.00019414833714767965, 'beta_0': 0.8670189180100645, 'beta_1': 0.9910583100268953, 'epsilon': 2.3758258464322237e-06, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 9}. Best is trial 30 with value: 0.9567749397755236.
CUDA out of memory. Tried to allocate 3.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 41.78 GiB memory in use. Of the allocated memory 40.01 GiB is allocated by PyTorch, and 630.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 14:36:07,711] Trial 32 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.837798996343291, 'batch_size': 76, 'attention_heads': 15, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5989984000596156, 'global_pooling': 'max', 'learning_rate': 0.000514735358155906, 'weight_decay': 0.00018182898402034452, 'beta_0': 0.8677268261174456, 'beta_1': 0.992272253402632, 'epsilon': 1.0620557053711835e-06, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 30 with value: 0.9567749397755236.
[I 2025-02-18 14:56:12,506] Trial 33 finished with value: 0.944604047967738 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7956188350457123, 'batch_size': 52, 'attention_heads': 14, 'hidden_dimension': 85, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5962282784085414, 'global_pooling': 'max', 'learning_rate': 0.00013694127525373256, 'weight_decay': 7.69031124394811e-05, 'beta_0': 0.8824829755716559, 'beta_1': 0.990742495527968, 'epsilon': 1.914240637253382e-06, 'balanced_loss': False, 'epochs': 138, 'early_stopping_patience': 16, 'plateau_patience': 16, 'plateau_divider': 10}. Best is trial 30 with value: 0.9567749397755236.
CUDA out of memory. Tried to allocate 3.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 548.69 MiB is free. Including non-PyTorch memory, this process has 44.02 GiB memory in use. Of the allocated memory 41.86 GiB is allocated by PyTorch, and 1.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:03:36,081] Trial 34 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8356821176252915, 'batch_size': 81, 'attention_heads': 15, 'hidden_dimension': 125, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5690292707243604, 'global_pooling': 'max', 'learning_rate': 0.0004919546954229842, 'weight_decay': 0.00015247925590225884, 'beta_0': 0.8723325988801616, 'beta_1': 0.9909377429080215, 'epsilon': 7.192507822555933e-06, 'balanced_loss': False, 'epochs': 167, 'early_stopping_patience': 18, 'plateau_patience': 16, 'plateau_divider': 8}. Best is trial 30 with value: 0.9567749397755236.
CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.92 GiB is free. Including non-PyTorch memory, this process has 39.63 GiB memory in use. Of the allocated memory 36.18 GiB is allocated by PyTorch, and 2.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:11:01,887] Trial 35 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7419150381817992, 'batch_size': 112, 'attention_heads': 13, 'hidden_dimension': 170, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5423435951808213, 'global_pooling': 'max', 'learning_rate': 6.490314495042751e-05, 'weight_decay': 0.00025431337344168257, 'beta_0': 0.8918023364867952, 'beta_1': 0.9859759505080253, 'epsilon': 2.5001770674172477e-06, 'balanced_loss': False, 'epochs': 63, 'early_stopping_patience': 15, 'plateau_patience': 17, 'plateau_divider': 9}. Best is trial 30 with value: 0.9567749397755236.
[I 2025-02-18 15:25:38,481] Trial 36 finished with value: 0.966858574697526 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7842212765743521, 'batch_size': 125, 'attention_heads': 12, 'hidden_dimension': 53, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5893380437747775, 'global_pooling': 'max', 'learning_rate': 0.0007299597388083921, 'weight_decay': 3.292119017465294e-05, 'beta_0': 0.8663164725328848, 'beta_1': 0.9897150423688409, 'epsilon': 2.7445333291108016e-05, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 15:44:23,922] Trial 37 finished with value: 0.9389807865090536 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7579538897344152, 'batch_size': 167, 'attention_heads': 12, 'hidden_dimension': 45, 'number_of_hidden_layers': 1, 'dropout_rate': 0.52749990967059, 'global_pooling': 'sum', 'learning_rate': 0.00019355890072611807, 'weight_decay': 3.1148033113508996e-06, 'beta_0': 0.8624431004890354, 'beta_1': 0.9887543425707258, 'epsilon': 2.7484002817935233e-05, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 2.43 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 42.78 GiB memory in use. Of the allocated memory 39.24 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:50:33,010] Trial 38 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7837715739844693, 'batch_size': 124, 'attention_heads': 12, 'hidden_dimension': 55, 'number_of_hidden_layers': 4, 'dropout_rate': 0.559140158324777, 'global_pooling': 'max', 'learning_rate': 0.000391929134116318, 'weight_decay': 3.232687713669608e-05, 'beta_0': 0.875841147813956, 'beta_1': 0.9890341892575136, 'epsilon': 9.708362333384232e-08, 'balanced_loss': True, 'epochs': 158, 'early_stopping_patience': 23, 'plateau_patience': 20, 'plateau_divider': 5}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 6.94 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.27 GiB is free. Including non-PyTorch memory, this process has 41.28 GiB memory in use. Of the allocated memory 39.14 GiB is allocated by PyTorch, and 1017.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 15:56:30,794] Trial 39 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8047210989145969, 'batch_size': 183, 'attention_heads': 9, 'hidden_dimension': 188, 'number_of_hidden_layers': 3, 'dropout_rate': 0.34383661467414833, 'global_pooling': 'max', 'learning_rate': 0.004932143176858197, 'weight_decay': 0.0005956768030091662, 'beta_0': 0.8564007851747906, 'beta_1': 0.9901679140153479, 'epsilon': 1.213965801256414e-08, 'balanced_loss': False, 'epochs': 175, 'early_stopping_patience': 24, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 16:06:38,981] Trial 40 finished with value: 0.9379157132306966 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.9694759012499682, 'batch_size': 100, 'attention_heads': 13, 'hidden_dimension': 110, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5686538013817648, 'global_pooling': 'max', 'learning_rate': 6.71544182839888e-05, 'weight_decay': 1.0005072453856697e-06, 'beta_0': 0.8721327148575765, 'beta_1': 0.9921724974987148, 'epsilon': 3.8997420398683326e-05, 'balanced_loss': False, 'epochs': 186, 'early_stopping_patience': 21, 'plateau_patience': 15, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 16:19:52,160] Trial 41 finished with value: 0.9386064440718093 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8316181758188884, 'batch_size': 67, 'attention_heads': 14, 'hidden_dimension': 80, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5833318052446711, 'global_pooling': 'max', 'learning_rate': 0.0007959443434858858, 'weight_decay': 7.473674556973362e-05, 'beta_0': 0.8686512385711346, 'beta_1': 0.9914528147114205, 'epsilon': 4.817856728738722e-06, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 16:37:00,654] Trial 42 finished with value: 0.9409447171895212 and parameters: {'left_stride': 256, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8692208786531783, 'batch_size': 84, 'attention_heads': 14, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5854583467757848, 'global_pooling': 'max', 'learning_rate': 0.0007223820310913726, 'weight_decay': 2.0379084097901182e-05, 'beta_0': 0.8648503060925946, 'beta_1': 0.9898989619105383, 'epsilon': 9.103167307724127e-07, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 5}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 5.27 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 43.27 GiB memory in use. Of the allocated memory 41.68 GiB is allocated by PyTorch, and 451.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 16:44:17,336] Trial 43 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7622360869529256, 'batch_size': 151, 'attention_heads': 14, 'hidden_dimension': 97, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5999451288547281, 'global_pooling': 'max', 'learning_rate': 0.00021751496906686666, 'weight_decay': 0.00014127875747389947, 'beta_0': 0.8804430320744713, 'beta_1': 0.9864724565184445, 'epsilon': 7.812825847698115e-06, 'balanced_loss': False, 'epochs': 138, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 7}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 17:06:32,253] Trial 44 finished with value: 0.9494676207268025 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7945072889299677, 'batch_size': 43, 'attention_heads': 15, 'hidden_dimension': 133, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5500344907807501, 'global_pooling': 'max', 'learning_rate': 0.002576601578780892, 'weight_decay': 0.00041210908527122207, 'beta_0': 0.8478847245376122, 'beta_1': 0.9887196949521221, 'epsilon': 8.700034786779272e-05, 'balanced_loss': False, 'epochs': 156, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 4.65 GiB. GPU 0 has a total capacity of 44.56 GiB of which 798.69 MiB is free. Including non-PyTorch memory, this process has 43.77 GiB memory in use. Of the allocated memory 41.97 GiB is allocated by PyTorch, and 669.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 17:12:39,131] Trial 45 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.7191486605859452, 'batch_size': 47, 'attention_heads': 15, 'hidden_dimension': 132, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5479833167746206, 'global_pooling': 'sum', 'learning_rate': 0.0028448787289503245, 'weight_decay': 0.0005466599291220057, 'beta_0': 0.8374269433678311, 'beta_1': 0.9879362509701143, 'epsilon': 9.884657933824045e-05, 'balanced_loss': False, 'epochs': 180, 'early_stopping_patience': 22, 'plateau_patience': 19, 'plateau_divider': 7}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 17:29:37,129] Trial 46 finished with value: 0.9464832142825668 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7945507777686137, 'batch_size': 32, 'attention_heads': 16, 'hidden_dimension': 48, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5241970839035388, 'global_pooling': 'max', 'learning_rate': 0.0021137744029534554, 'weight_decay': 0.0003711058300764837, 'beta_0': 0.8519423681858876, 'beta_1': 0.9882949027012499, 'epsilon': 6.010439791111286e-05, 'balanced_loss': False, 'epochs': 161, 'early_stopping_patience': 24, 'plateau_patience': 21, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 17:45:32,255] Trial 47 finished with value: 0.9445944147493543 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'max', 'threshold': 0.7683544855377005, 'batch_size': 47, 'attention_heads': 15, 'hidden_dimension': 146, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4673396423307196, 'global_pooling': 'max', 'learning_rate': 0.0012835025344887588, 'weight_decay': 4.1690276633240866e-05, 'beta_0': 0.8480610103903565, 'beta_1': 0.9894740556621824, 'epsilon': 2.2540524854784306e-05, 'balanced_loss': True, 'epochs': 153, 'early_stopping_patience': 22, 'plateau_patience': 11, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 7.02 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.61 GiB is free. Including non-PyTorch memory, this process has 41.94 GiB memory in use. Of the allocated memory 39.53 GiB is allocated by PyTorch, and 1.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 17:51:39,389] Trial 48 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7860941967644666, 'batch_size': 129, 'attention_heads': 11, 'hidden_dimension': 201, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5542150530999, 'global_pooling': 'mean', 'learning_rate': 0.004154566082101357, 'weight_decay': 0.0002529220569452805, 'beta_0': 0.8007159752103736, 'beta_1': 0.9844150021233764, 'epsilon': 4.690717222118266e-05, 'balanced_loss': False, 'epochs': 116, 'early_stopping_patience': 20, 'plateau_patience': 17, 'plateau_divider': 7}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 6.80 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.27 GiB is free. Including non-PyTorch memory, this process has 40.28 GiB memory in use. Of the allocated memory 38.39 GiB is allocated by PyTorch, and 762.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 17:57:37,935] Trial 49 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8534946872886106, 'batch_size': 202, 'attention_heads': 12, 'hidden_dimension': 173, 'number_of_hidden_layers': 3, 'dropout_rate': 0.5715388723275862, 'global_pooling': 'max', 'learning_rate': 0.008120941951039492, 'weight_decay': 8.668813449377333e-05, 'beta_0': 0.8391389504004363, 'beta_1': 0.9856478018368847, 'epsilon': 6.787223938367024e-05, 'balanced_loss': False, 'epochs': 200, 'early_stopping_patience': 24, 'plateau_patience': 21, 'plateau_divider': 5}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.48 GiB is free. Including non-PyTorch memory, this process has 43.08 GiB memory in use. Of the allocated memory 41.37 GiB is allocated by PyTorch, and 570.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:03:37,223] Trial 50 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.7418030657435342, 'batch_size': 55, 'attention_heads': 13, 'hidden_dimension': 234, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5346812857373813, 'global_pooling': 'sum', 'learning_rate': 0.04404855920925648, 'weight_decay': 0.00011650861443470071, 'beta_0': 0.8605918242289197, 'beta_1': 0.9813864911517354, 'epsilon': 2.096250610248489e-08, 'balanced_loss': False, 'epochs': 128, 'early_stopping_patience': 25, 'plateau_patience': 22, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 18:18:29,639] Trial 51 finished with value: 0.9525430733162963 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8190152152776576, 'batch_size': 67, 'attention_heads': 14, 'hidden_dimension': 114, 'number_of_hidden_layers': 1, 'dropout_rate': 0.377930972364214, 'global_pooling': 'max', 'learning_rate': 0.0006940812181425195, 'weight_decay': 6.908836951116581e-06, 'beta_0': 0.866911121291967, 'beta_1': 0.9931177518034241, 'epsilon': 1.1784472406139276e-06, 'balanced_loss': False, 'epochs': 153, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 4.76 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 42.56 GiB is allocated by PyTorch, and 864.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:25:15,786] Trial 52 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8203062058112441, 'batch_size': 92, 'attention_heads': 15, 'hidden_dimension': 113, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3854596690769303, 'global_pooling': 'max', 'learning_rate': 0.0003621533365111747, 'weight_decay': 5.438724958259733e-06, 'beta_0': 0.8550578304018781, 'beta_1': 0.9934446859213637, 'epsilon': 4.3456772839409947e-07, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 41.30 GiB memory in use. Of the allocated memory 39.48 GiB is allocated by PyTorch, and 681.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:31:53,390] Trial 53 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.801778022106911, 'batch_size': 68, 'attention_heads': 12, 'hidden_dimension': 125, 'number_of_hidden_layers': 1, 'dropout_rate': 0.374357626630615, 'global_pooling': 'max', 'learning_rate': 0.0012086453202183386, 'weight_decay': 1.203723138584501e-05, 'beta_0': 0.8632805491345487, 'beta_1': 0.9950126543965875, 'epsilon': 1.2583244000275149e-06, 'balanced_loss': False, 'epochs': 171, 'early_stopping_patience': 22, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 18:40:51,832] Trial 54 finished with value: 0.9431923301790448 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9206132181209822, 'batch_size': 59, 'attention_heads': 16, 'hidden_dimension': 139, 'number_of_hidden_layers': 0, 'dropout_rate': 0.3354642586256782, 'global_pooling': 'max', 'learning_rate': 0.0019469929417200644, 'weight_decay': 1.9948934914231477e-05, 'beta_0': 0.8480350814239698, 'beta_1': 0.996046463701221, 'epsilon': 1.4292946028700306e-07, 'balanced_loss': False, 'epochs': 88, 'early_stopping_patience': 19, 'plateau_patience': 17, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 18:53:28,700] Trial 55 finished with value: 0.929314219535488 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.9005864809758828, 'batch_size': 44, 'attention_heads': 13, 'hidden_dimension': 154, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3531530105220296, 'global_pooling': 'mean', 'learning_rate': 0.0009427686742151156, 'weight_decay': 3.977871792502746e-06, 'beta_0': 0.8314454959380669, 'beta_1': 0.9931091094651652, 'epsilon': 1.2491919841634796e-05, 'balanced_loss': False, 'epochs': 136, 'early_stopping_patience': 16, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 5.13 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.76 GiB is free. Including non-PyTorch memory, this process has 42.80 GiB memory in use. Of the allocated memory 40.47 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 18:59:35,189] Trial 56 finished with value: -1.0 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8508295392573673, 'batch_size': 100, 'attention_heads': 15, 'hidden_dimension': 163, 'number_of_hidden_layers': 1, 'dropout_rate': 0.3087754452466251, 'global_pooling': 'max', 'learning_rate': 0.00046048236355071575, 'weight_decay': 6.650783507821186e-06, 'beta_0': 0.8582758249881663, 'beta_1': 0.9874465290368435, 'epsilon': 3.691249185762707e-05, 'balanced_loss': False, 'epochs': 94, 'early_stopping_patience': 21, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 19:10:49,730] Trial 57 finished with value: 0.947173145029885 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8219099343581658, 'batch_size': 82, 'attention_heads': 11, 'hidden_dimension': 118, 'number_of_hidden_layers': 0, 'dropout_rate': 0.4187441253310599, 'global_pooling': 'max', 'learning_rate': 0.0006295562518699576, 'weight_decay': 1.9020390258806214e-06, 'beta_0': 0.8747117785700236, 'beta_1': 0.9943190908510594, 'epsilon': 5.269126030485524e-07, 'balanced_loss': False, 'epochs': 154, 'early_stopping_patience': 23, 'plateau_patience': 23, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 19:17:32,829] Trial 58 finished with value: 0.8050606578197181 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'max', 'threshold': 0.9949926280392398, 'batch_size': 68, 'attention_heads': 9, 'hidden_dimension': 76, 'number_of_hidden_layers': 1, 'dropout_rate': 0.38815789649965443, 'global_pooling': 'max', 'learning_rate': 0.00018178744525336381, 'weight_decay': 0.0007557081671708065, 'beta_0': 0.869036425542072, 'beta_1': 0.9866044007961766, 'epsilon': 3.0402676144670035e-07, 'balanced_loss': True, 'epochs': 124, 'early_stopping_patience': 14, 'plateau_patience': 21, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 19:29:57,690] Trial 59 finished with value: 0.9387309022442463 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'mean', 'threshold': 0.8832795314955254, 'batch_size': 51, 'attention_heads': 10, 'hidden_dimension': 62, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5129296830158181, 'global_pooling': 'max', 'learning_rate': 0.00030729693578032517, 'weight_decay': 0.0002630290089038881, 'beta_0': 0.851372654584187, 'beta_1': 0.99186655958944, 'epsilon': 3.7710956796074496e-06, 'balanced_loss': False, 'epochs': 176, 'early_stopping_patience': 20, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 19:41:11,759] Trial 60 finished with value: 0.9376423568453004 and parameters: {'left_stride': 0, 'right_stride': 128, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8445124454926777, 'batch_size': 38, 'attention_heads': 14, 'hidden_dimension': 135, 'number_of_hidden_layers': 0, 'dropout_rate': 0.36634843518644433, 'global_pooling': 'mean', 'learning_rate': 0.002979374854249322, 'weight_decay': 0.0004832405960215776, 'beta_0': 0.8866111298602952, 'beta_1': 0.9927071638602257, 'epsilon': 2.184578825180828e-05, 'balanced_loss': False, 'epochs': 101, 'early_stopping_patience': 19, 'plateau_patience': 18, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.43 GiB is free. Including non-PyTorch memory, this process has 41.12 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 19:49:30,500] Trial 61 finished with value: -1.0 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7750053157308336, 'batch_size': 74, 'attention_heads': 14, 'hidden_dimension': 101, 'number_of_hidden_layers': 1, 'dropout_rate': 0.58535401211078, 'global_pooling': 'max', 'learning_rate': 0.0009625309566662753, 'weight_decay': 0.00022796255528724426, 'beta_0': 0.8667698383601353, 'beta_1': 0.9904348857297083, 'epsilon': 2.3958855007574123e-06, 'balanced_loss': False, 'epochs': 141, 'early_stopping_patience': 16, 'plateau_patience': 17, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 20:04:25,749] Trial 62 finished with value: 0.9375359399779924 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8129251292940719, 'batch_size': 62, 'attention_heads': 14, 'hidden_dimension': 88, 'number_of_hidden_layers': 1, 'dropout_rate': 0.40501012164760003, 'global_pooling': 'max', 'learning_rate': 0.0006836827774380333, 'weight_decay': 0.00013824472171908628, 'beta_0': 0.8778189909495755, 'beta_1': 0.9911462960497537, 'epsilon': 7.583738690273947e-07, 'balanced_loss': False, 'epochs': 158, 'early_stopping_patience': 17, 'plateau_patience': 18, 'plateau_divider': 10}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 20:18:42,658] Trial 63 finished with value: 0.9410304794049732 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8668178876109782, 'batch_size': 87, 'attention_heads': 13, 'hidden_dimension': 108, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5900252744784051, 'global_pooling': 'max', 'learning_rate': 0.0018268970261959576, 'weight_decay': 0.00033482185182397373, 'beta_0': 0.87038491358459, 'beta_1': 0.9897417095250515, 'epsilon': 1.6056939792185692e-06, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 18, 'plateau_patience': 19, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 20:29:49,668] Trial 64 finished with value: 0.9537047596397789 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8359411140795339, 'batch_size': 77, 'attention_heads': 12, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5790122693047153, 'global_pooling': 'max', 'learning_rate': 0.0013974722226182095, 'weight_decay': 0.00016793983863569944, 'beta_0': 0.8642862136123433, 'beta_1': 0.9915626566538382, 'epsilon': 9.709496856152438e-06, 'balanced_loss': False, 'epochs': 133, 'early_stopping_patience': 15, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 20:41:13,068] Trial 65 finished with value: 0.9525734267051443 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8242121966081472, 'batch_size': 96, 'attention_heads': 12, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5760787550581243, 'global_pooling': 'max', 'learning_rate': 0.0013638598296204832, 'weight_decay': 0.00011155279398786084, 'beta_0': 0.8642549052296603, 'beta_1': 0.994202013616797, 'epsilon': 1.6679002926757206e-05, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 15, 'plateau_patience': 20, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 20:51:27,262] Trial 66 finished with value: 0.9446751731123503 and parameters: {'left_stride': 0, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7906459330946993, 'batch_size': 114, 'attention_heads': 12, 'hidden_dimension': 38, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5763280249580904, 'global_pooling': 'max', 'learning_rate': 0.0013619207987535292, 'weight_decay': 0.00010573443892252356, 'beta_0': 0.8638969305537775, 'beta_1': 0.9942094595165788, 'epsilon': 1.6871522877241926e-05, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 13, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 21:00:07,345] Trial 67 finished with value: 0.9488392618633711 and parameters: {'left_stride': 0, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8258363432701044, 'batch_size': 95, 'attention_heads': 4, 'hidden_dimension': 52, 'number_of_hidden_layers': 1, 'dropout_rate': 0.562002982156693, 'global_pooling': 'max', 'learning_rate': 0.0024418503075921845, 'weight_decay': 4.1272606582940805e-05, 'beta_0': 0.8593464637843239, 'beta_1': 0.9963547733430304, 'epsilon': 1.0886161472001343e-05, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 15, 'plateau_patience': 19, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 21:10:22,597] Trial 68 finished with value: 0.9306151919755193 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8053536330863081, 'batch_size': 105, 'attention_heads': 12, 'hidden_dimension': 37, 'number_of_hidden_layers': 1, 'dropout_rate': 0.546920575285164, 'global_pooling': 'sum', 'learning_rate': 0.0036986951859655573, 'weight_decay': 6.616568517594505e-05, 'beta_0': 0.8820571269005495, 'beta_1': 0.998139720192247, 'epsilon': 4.965520519867516e-06, 'balanced_loss': False, 'epochs': 134, 'early_stopping_patience': 11, 'plateau_patience': 22, 'plateau_divider': 8}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 21:20:01,645] Trial 69 finished with value: 0.9289257916207079 and parameters: {'left_stride': 128, 'right_stride': 256, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'max', 'threshold': 0.8445496805268354, 'batch_size': 138, 'attention_heads': 13, 'hidden_dimension': 32, 'number_of_hidden_layers': 1, 'dropout_rate': 0.49666447774251476, 'global_pooling': 'max', 'learning_rate': 0.005708000077656299, 'weight_decay': 1.6404726655582312e-05, 'beta_0': 0.8741116484693515, 'beta_1': 0.9928640414480671, 'epsilon': 7.423998022567075e-05, 'balanced_loss': True, 'epochs': 120, 'early_stopping_patience': 16, 'plateau_patience': 20, 'plateau_divider': 2}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.33 GiB is free. Including non-PyTorch memory, this process has 42.22 GiB memory in use. Of the allocated memory 37.64 GiB is allocated by PyTorch, and 3.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-18 21:27:44,664] Trial 70 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.7774186500343854, 'batch_size': 151, 'attention_heads': 15, 'hidden_dimension': 43, 'number_of_hidden_layers': 2, 'dropout_rate': 0.564932424855304, 'global_pooling': 'max', 'learning_rate': 0.0016458172860287705, 'weight_decay': 9.032842632346573e-05, 'beta_0': 0.8893156643726381, 'beta_1': 0.9917130453859353, 'epsilon': 3.052722612372945e-05, 'balanced_loss': False, 'epochs': 164, 'early_stopping_patience': 15, 'plateau_patience': 21, 'plateau_divider': 7}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 21:39:05,749] Trial 71 finished with value: 0.9335194482828956 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.8603743980491247, 'batch_size': 80, 'attention_heads': 11, 'hidden_dimension': 55, 'number_of_hidden_layers': 1, 'dropout_rate': 0.5754961985097616, 'global_pooling': 'max', 'learning_rate': 0.0010706978248917007, 'weight_decay': 0.000154900895679279, 'beta_0': 0.8528420933491925, 'beta_1': 0.9940997423892001, 'epsilon': 1.3994105621441935e-05, 'balanced_loss': False, 'epochs': 80, 'early_stopping_patience': 12, 'plateau_patience': 20, 'plateau_divider': 9}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-18 21:48:20,088] Trial 72 finished with value: 0.9285559121192097 and parameters: {'left_stride': 0, 'right_stride': 64, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'mean', 'threshold': 0.9119589802194477, 'batch_size': 60, 'attention_heads': 12, 'hidden_dimension': 61, 'number_of_hidden_layers': 1, 'dropout_rate': 0.44115014364783545, 'global_pooling': 'max', 'learning_rate': 0.0005090296677626434, 'weight_decay': 0.00022008769244756597, 'beta_0': 0.8459875355144784, 'beta_1': 0.988490941157137, 'epsilon': 8.91311237681966e-06, 'balanced_loss': False, 'epochs': 114, 'early_stopping_patience': 14, 'plateau_patience': 20, 'plateau_divider': 10}. Best is trial 36 with value: 0.966858574697526.
slurmstepd: error: *** JOB 14932825 ON gpu029 CANCELLED AT 2025-02-18T21:53:04 DUE TO TIME LIMIT ***
