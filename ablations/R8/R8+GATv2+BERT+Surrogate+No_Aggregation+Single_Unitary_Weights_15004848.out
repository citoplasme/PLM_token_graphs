[I 2025-02-23 04:29:31,250] Using an existing study with name 'R8-GATv2-google-bert-bert-base-uncased-Surrogate-No_Aggregation-Single_Unitary_Weight-0.0-0.0' instead of creating a new one.
Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors
CUDA out of memory. Tried to allocate 4.62 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.12 GiB is free. Including non-PyTorch memory, this process has 43.43 GiB memory in use. Of the allocated memory 41.60 GiB is allocated by PyTorch, and 703.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 04:35:37,928] Trial 279 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8001456383247375, 'batch_size': 225, 'attention_heads': 10, 'hidden_dimension': 100, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5809465067915401, 'global_pooling': 'max', 'learning_rate': 4.576754022750641e-05, 'weight_decay': 7.038077824459506e-05, 'beta_0': 0.8528490223042429, 'beta_1': 0.9807877970459647, 'epsilon': 7.580130328136675e-08, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 24, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 04:53:15,703] Trial 280 finished with value: 0.9536383993960427 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8276224813268617, 'batch_size': 60, 'attention_heads': 8, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5915887123968793, 'global_pooling': 'max', 'learning_rate': 0.00033493482741201185, 'weight_decay': 4.083508722969738e-05, 'beta_0': 0.8499545785172807, 'beta_1': 0.9905682273430942, 'epsilon': 1.8558163490295888e-06, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 25, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.04 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.04 GiB is free. Including non-PyTorch memory, this process has 42.51 GiB memory in use. Of the allocated memory 38.02 GiB is allocated by PyTorch, and 3.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 05:04:29,295] Trial 281 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.835806542579417, 'batch_size': 71, 'attention_heads': 11, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5932363025186727, 'global_pooling': 'max', 'learning_rate': 0.00044320409202714843, 'weight_decay': 7.77013763334686e-05, 'beta_0': 0.849433508099493, 'beta_1': 0.9912388790393043, 'epsilon': 4.158120608425374e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 21, 'plateau_patience': 19, 'plateau_divider': 2}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.23 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.59 GiB is free. Including non-PyTorch memory, this process has 41.96 GiB memory in use. Of the allocated memory 40.09 GiB is allocated by PyTorch, and 739.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 05:15:58,768] Trial 282 finished with value: -1.0 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8085478440872376, 'batch_size': 64, 'attention_heads': 11, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5859961281358707, 'global_pooling': 'mean', 'learning_rate': 0.0006567118934690313, 'weight_decay': 5.05969874776987e-05, 'beta_0': 0.8477533943436479, 'beta_1': 0.9919686885018305, 'epsilon': 1.360229798221236e-06, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 24, 'plateau_patience': 14, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 05:35:47,732] Trial 283 finished with value: 0.9499041907481243 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8457406018758383, 'batch_size': 54, 'attention_heads': 10, 'hidden_dimension': 114, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5791673463544057, 'global_pooling': 'max', 'learning_rate': 0.0003671327342458381, 'weight_decay': 6.075888780164327e-05, 'beta_0': 0.8452091128810536, 'beta_1': 0.9902627315767202, 'epsilon': 1.859897588639842e-07, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 25, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 05:53:05,469] Trial 284 finished with value: 0.9469603331302852 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8193417517582999, 'batch_size': 58, 'attention_heads': 9, 'hidden_dimension': 95, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5944396726866713, 'global_pooling': 'max', 'learning_rate': 0.00082214842723883, 'weight_decay': 0.0001006122264125751, 'beta_0': 0.8503512883568357, 'beta_1': 0.9915180915722694, 'epsilon': 1.6047848884919935e-06, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 22, 'plateau_patience': 18, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 06:14:21,324] Trial 285 finished with value: 0.9417690538251613 and parameters: {'left_stride': 32, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8284913234114745, 'batch_size': 73, 'attention_heads': 9, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5880510140671045, 'global_pooling': 'sum', 'learning_rate': 0.0005248294547309317, 'weight_decay': 7.615502710660462e-05, 'beta_0': 0.8719578894985973, 'beta_1': 0.9909736347883954, 'epsilon': 8.11437380215421e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 6}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 06:34:45,087] Trial 286 finished with value: 0.9651474054862577 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8340456685379937, 'batch_size': 62, 'attention_heads': 10, 'hidden_dimension': 103, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5802340070142271, 'global_pooling': 'max', 'learning_rate': 0.00044342703589779877, 'weight_decay': 4.16378729940875e-05, 'beta_0': 0.8544314269962864, 'beta_1': 0.9912006853456066, 'epsilon': 1.5040400390103922e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.53 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.09 GiB is free. Including non-PyTorch memory, this process has 41.47 GiB memory in use. Of the allocated memory 40.03 GiB is allocated by PyTorch, and 297.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 06:40:53,988] Trial 287 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.838939303957634, 'batch_size': 157, 'attention_heads': 10, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5793902175041962, 'global_pooling': 'max', 'learning_rate': 0.0004068778166317582, 'weight_decay': 4.2390833724656864e-05, 'beta_0': 0.8541602103016073, 'beta_1': 0.9905096863350911, 'epsilon': 3.007091200981574e-06, 'balanced_loss': True, 'epochs': 148, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 4}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 07:03:05,543] Trial 288 finished with value: 0.9479743086922994 and parameters: {'left_stride': 64, 'right_stride': 256, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8498261767874616, 'batch_size': 57, 'attention_heads': 10, 'hidden_dimension': 115, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5830090230992229, 'global_pooling': 'max', 'learning_rate': 0.00031262377537020103, 'weight_decay': 3.433455380824881e-05, 'beta_0': 0.8513909692954518, 'beta_1': 0.9817872127587747, 'epsilon': 1.654338688016816e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 24, 'plateau_patience': 25, 'plateau_divider': 4}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 07:23:12,631] Trial 289 finished with value: 0.934598175429791 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8149540867184609, 'batch_size': 69, 'attention_heads': 10, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5898218139409557, 'global_pooling': 'max', 'learning_rate': 0.00047169054470275895, 'weight_decay': 0.00011420747194714189, 'beta_0': 0.8531669522066445, 'beta_1': 0.9897585384793858, 'epsilon': 4.796040228361861e-08, 'balanced_loss': False, 'epochs': 150, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 07:44:20,906] Trial 290 finished with value: 0.9576335268007857 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8216732382869167, 'batch_size': 51, 'attention_heads': 11, 'hidden_dimension': 98, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5718119938834735, 'global_pooling': 'max', 'learning_rate': 0.00035629117585183484, 'weight_decay': 3.954629321374678e-05, 'beta_0': 0.8559834028269787, 'beta_1': 0.9924328829941088, 'epsilon': 1.2912458642257596e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 08:03:13,299] Trial 291 finished with value: 0.9625941514241132 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8234376188325893, 'batch_size': 48, 'attention_heads': 11, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5728736334671682, 'global_pooling': 'max', 'learning_rate': 0.00034741962127951776, 'weight_decay': 3.716025261092876e-05, 'beta_0': 0.8552286023862653, 'beta_1': 0.9927312047085852, 'epsilon': 1.0709351668048542e-08, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.48 GiB. GPU 0 has a total capacity of 44.56 GiB of which 6.69 MiB is free. Including non-PyTorch memory, this process has 44.55 GiB memory in use. Of the allocated memory 42.91 GiB is allocated by PyTorch, and 501.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 08:10:16,630] Trial 292 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8039583428815971, 'batch_size': 48, 'attention_heads': 15, 'hidden_dimension': 99, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5730682478287786, 'global_pooling': 'max', 'learning_rate': 0.00042102692206495154, 'weight_decay': 4.0379006931163645e-05, 'beta_0': 0.8551013458018368, 'beta_1': 0.9918171828833968, 'epsilon': 2.5621321495165936e-06, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 43.21 GiB memory in use. Of the allocated memory 38.52 GiB is allocated by PyTorch, and 3.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 08:16:53,745] Trial 293 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8202248425625939, 'batch_size': 110, 'attention_heads': 11, 'hidden_dimension': 96, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5740330296209303, 'global_pooling': 'max', 'learning_rate': 0.00036388599837221594, 'weight_decay': 4.882068382964327e-05, 'beta_0': 0.857046590772558, 'beta_1': 0.9927448039994738, 'epsilon': 1.0921088730625003e-08, 'balanced_loss': False, 'epochs': 142, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.35 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 43.44 GiB memory in use. Of the allocated memory 41.48 GiB is allocated by PyTorch, and 828.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 08:23:36,330] Trial 294 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8104446415960427, 'batch_size': 75, 'attention_heads': 11, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5791255301931941, 'global_pooling': 'max', 'learning_rate': 0.00046720576073942794, 'weight_decay': 3.906973713827868e-05, 'beta_0': 0.8568998405793702, 'beta_1': 0.9930717989047445, 'epsilon': 1.0084200360250169e-08, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 23, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 41.84 GiB memory in use. Of the allocated memory 40.32 GiB is allocated by PyTorch, and 370.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 08:29:52,293] Trial 295 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8237107129432795, 'batch_size': 121, 'attention_heads': 11, 'hidden_dimension': 98, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5999743383291046, 'global_pooling': 'max', 'learning_rate': 0.0002845909911735124, 'weight_decay': 1.476624792687571e-06, 'beta_0': 0.8553762750387853, 'beta_1': 0.992465953116841, 'epsilon': 1.445563992489178e-08, 'balanced_loss': False, 'epochs': 65, 'early_stopping_patience': 24, 'plateau_patience': 19, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 08:52:59,270] Trial 296 finished with value: 0.9639241333502799 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8160113938230755, 'batch_size': 63, 'attention_heads': 11, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5716549947052384, 'global_pooling': 'max', 'learning_rate': 0.0005490736731500998, 'weight_decay': 5.24553466034071e-06, 'beta_0': 0.8594475347226679, 'beta_1': 0.9927513149573747, 'epsilon': 1.4528359422103528e-08, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 09:15:17,673] Trial 297 finished with value: 0.951161914036482 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8138943008788216, 'batch_size': 63, 'attention_heads': 11, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5693958865164335, 'global_pooling': 'max', 'learning_rate': 0.0006564983892394096, 'weight_decay': 3.152785349140387e-06, 'beta_0': 0.8579108748112249, 'beta_1': 0.9928780725353862, 'epsilon': 1.2767641201576213e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 09:29:54,084] Trial 298 finished with value: 0.9524025201748054 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.9001974940180065, 'batch_size': 67, 'attention_heads': 11, 'hidden_dimension': 90, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5718075584324974, 'global_pooling': 'max', 'learning_rate': 0.0008805938219117012, 'weight_decay': 4.8470663335103116e-05, 'beta_0': 0.8591396367855234, 'beta_1': 0.9925140546809037, 'epsilon': 1.2993998641081584e-08, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 20, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.34 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 42.84 GiB memory in use. Of the allocated memory 41.44 GiB is allocated by PyTorch, and 255.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 09:36:48,355] Trial 299 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.7950186473604686, 'batch_size': 78, 'attention_heads': 11, 'hidden_dimension': 108, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5778693289456123, 'global_pooling': 'sum', 'learning_rate': 0.0005727832589590144, 'weight_decay': 2.7037093377624727e-06, 'beta_0': 0.8547025906621644, 'beta_1': 0.9924994429703298, 'epsilon': 1.7133129911437322e-08, 'balanced_loss': False, 'epochs': 146, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 09:56:39,272] Trial 300 finished with value: 0.953955157306305 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.80599611983679, 'batch_size': 61, 'attention_heads': 11, 'hidden_dimension': 95, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5933656940458065, 'global_pooling': 'max', 'learning_rate': 0.0011162599744580157, 'weight_decay': 5.451005080347649e-05, 'beta_0': 0.8553281349861319, 'beta_1': 0.9931222084252145, 'epsilon': 2.2199605140410256e-08, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 10:21:11,403] Trial 301 finished with value: 0.9572680056183652 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.819270764304446, 'batch_size': 67, 'attention_heads': 11, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5861368936826903, 'global_pooling': 'max', 'learning_rate': 0.0007356983848341112, 'weight_decay': 8.600269979891797e-05, 'beta_0': 0.8526760547669414, 'beta_1': 0.9921366880688864, 'epsilon': 2.2647188384633818e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 4.40 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.78 GiB is free. Including non-PyTorch memory, this process has 41.78 GiB memory in use. Of the allocated memory 39.84 GiB is allocated by PyTorch, and 810.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 10:27:10,861] Trial 302 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8195469891014391, 'batch_size': 193, 'attention_heads': 11, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5878650456506214, 'global_pooling': 'max', 'learning_rate': 0.0008012135686550648, 'weight_decay': 5.401686135076023e-06, 'beta_0': 0.8533287486348649, 'beta_1': 0.9921893242351563, 'epsilon': 1.9680036278831973e-08, 'balanced_loss': False, 'epochs': 144, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 44.56 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 43.20 GiB memory in use. Of the allocated memory 41.83 GiB is allocated by PyTorch, and 216.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 10:33:55,969] Trial 303 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8090913434422295, 'batch_size': 67, 'attention_heads': 11, 'hidden_dimension': 109, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5998630601034026, 'global_pooling': 'max', 'learning_rate': 0.0007203691671702841, 'weight_decay': 8.220339807713128e-05, 'beta_0': 0.8585828217154721, 'beta_1': 0.9922288214085544, 'epsilon': 1.08357169601105e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 25, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.18 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.80 GiB is free. Including non-PyTorch memory, this process has 41.75 GiB memory in use. Of the allocated memory 39.70 GiB is allocated by PyTorch, and 926.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 10:41:08,905] Trial 304 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8157242418211399, 'batch_size': 134, 'attention_heads': 11, 'hidden_dimension': 104, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5851367614310491, 'global_pooling': 'max', 'learning_rate': 0.0005243976777392627, 'weight_decay': 8.226835002786609e-05, 'beta_0': 0.8525347347971545, 'beta_1': 0.992835399068582, 'epsilon': 1.525045537191355e-08, 'balanced_loss': False, 'epochs': 138, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.22 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.82 GiB is free. Including non-PyTorch memory, this process has 41.74 GiB memory in use. Of the allocated memory 40.04 GiB is allocated by PyTorch, and 558.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 10:48:17,918] Trial 305 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.800054148089391, 'batch_size': 71, 'attention_heads': 10, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5921451395219125, 'global_pooling': 'max', 'learning_rate': 0.0003602692014858494, 'weight_decay': 6.805871148703845e-05, 'beta_0': 0.860003066961451, 'beta_1': 0.9919405402900323, 'epsilon': 2.5406990464152036e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 41.83 GiB memory in use. Of the allocated memory 37.53 GiB is allocated by PyTorch, and 3.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 10:58:05,185] Trial 306 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.824550402215234, 'batch_size': 89, 'attention_heads': 10, 'hidden_dimension': 100, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5944953759382083, 'global_pooling': 'mean', 'learning_rate': 0.0010062325325155671, 'weight_decay': 9.48371852399786e-05, 'beta_0': 0.8550399042474341, 'beta_1': 0.9915520102815314, 'epsilon': 1.3893173564751513e-08, 'balanced_loss': False, 'epochs': 141, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 11:17:40,853] Trial 307 finished with value: 0.9466808116413805 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8288580531527914, 'batch_size': 58, 'attention_heads': 11, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.58248524994793, 'global_pooling': 'max', 'learning_rate': 0.0006192940876084337, 'weight_decay': 5.490943194338108e-06, 'beta_0': 0.851750026308912, 'beta_1': 0.9920475434312253, 'epsilon': 2.0451668038923394e-08, 'balanced_loss': False, 'epochs': 190, 'early_stopping_patience': 24, 'plateau_patience': 17, 'plateau_divider': 2}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.24 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.55 GiB is free. Including non-PyTorch memory, this process has 42.00 GiB memory in use. Of the allocated memory 40.27 GiB is allocated by PyTorch, and 597.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 11:24:29,685] Trial 308 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 32, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8126308985123569, 'batch_size': 64, 'attention_heads': 11, 'hidden_dimension': 111, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5765645470512814, 'global_pooling': 'max', 'learning_rate': 0.0004470165704865112, 'weight_decay': 5.868837581495365e-05, 'beta_0': 0.8558582611772877, 'beta_1': 0.9934029444358086, 'epsilon': 3.8315949235621535e-08, 'balanced_loss': True, 'epochs': 135, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 11:43:12,778] Trial 309 finished with value: 0.9588135204053078 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8213037912894638, 'batch_size': 74, 'attention_heads': 11, 'hidden_dimension': 100, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5885893068077356, 'global_pooling': 'max', 'learning_rate': 0.0002919867147455458, 'weight_decay': 4.547300192579648e-05, 'beta_0': 0.8572482084934192, 'beta_1': 0.9925997308696919, 'epsilon': 1.1401630144669356e-08, 'balanced_loss': False, 'epochs': 148, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 11:54:10,471] Trial 310 finished with value: 0.9458750751531602 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8251486537670709, 'batch_size': 83, 'attention_heads': 11, 'hidden_dimension': 98, 'number_of_hidden_layers': 0, 'dropout_rate': 0.5890106735558376, 'global_pooling': 'max', 'learning_rate': 0.0006968599863320463, 'weight_decay': 6.881247568953021e-05, 'beta_0': 0.8574716264883995, 'beta_1': 0.9925077870371481, 'epsilon': 1.1316660524245207e-08, 'balanced_loss': False, 'epochs': 149, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 12:13:37,011] Trial 311 finished with value: 0.9558279101682672 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8206818064982332, 'batch_size': 76, 'attention_heads': 11, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5953552223962219, 'global_pooling': 'max', 'learning_rate': 0.0005317898643988767, 'weight_decay': 8.903695659120072e-05, 'beta_0': 0.8588411896641195, 'beta_1': 0.9928697516258971, 'epsilon': 1.3452105236673773e-08, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 24, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 12:32:35,011] Trial 312 finished with value: 0.9483227546953417 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8320469137430213, 'batch_size': 77, 'attention_heads': 11, 'hidden_dimension': 98, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5998131658040519, 'global_pooling': 'max', 'learning_rate': 0.0003770552866851544, 'weight_decay': 7.905543774731563e-05, 'beta_0': 0.8594985622191522, 'beta_1': 0.9930599475515229, 'epsilon': 1.761840286976066e-08, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.15 GiB is free. Including non-PyTorch memory, this process has 41.40 GiB memory in use. Of the allocated memory 39.64 GiB is allocated by PyTorch, and 625.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 12:39:14,458] Trial 313 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8226797029556652, 'batch_size': 81, 'attention_heads': 11, 'hidden_dimension': 102, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5895953428693504, 'global_pooling': 'sum', 'learning_rate': 0.0005329603816870535, 'weight_decay': 5.2229034004476444e-05, 'beta_0': 0.8611859769772309, 'beta_1': 0.9927467426131383, 'epsilon': 1.3118889351376285e-08, 'balanced_loss': False, 'epochs': 139, 'early_stopping_patience': 22, 'plateau_patience': 22, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 12:58:09,986] Trial 314 finished with value: 0.951095056330135 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8317135673654532, 'batch_size': 71, 'attention_heads': 12, 'hidden_dimension': 93, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5942100253377184, 'global_pooling': 'max', 'learning_rate': 0.000481626531911128, 'weight_decay': 8.943144492665087e-05, 'beta_0': 0.8563198323747595, 'beta_1': 0.9933170129813842, 'epsilon': 1.176963187890799e-08, 'balanced_loss': False, 'epochs': 145, 'early_stopping_patience': 24, 'plateau_patience': 24, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 44.56 GiB of which 3.14 GiB is free. Including non-PyTorch memory, this process has 41.41 GiB memory in use. Of the allocated memory 39.61 GiB is allocated by PyTorch, and 670.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 13:04:52,026] Trial 315 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'min', 'embedding_pooling_operation': 'min', 'threshold': 0.8171722330843871, 'batch_size': 86, 'attention_heads': 11, 'hidden_dimension': 103, 'number_of_hidden_layers': 2, 'dropout_rate': 0.3556325156614351, 'global_pooling': 'max', 'learning_rate': 0.00032178539021473276, 'weight_decay': 4.6426726205989656e-05, 'beta_0': 0.853485957103326, 'beta_1': 0.9922641225627911, 'epsilon': 1.5623508942403967e-08, 'balanced_loss': False, 'epochs': 135, 'early_stopping_patience': 22, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 13:25:26,654] Trial 316 finished with value: 0.9509012589793224 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.82337849383552, 'batch_size': 77, 'attention_heads': 11, 'hidden_dimension': 94, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5855054589067981, 'global_pooling': 'max', 'learning_rate': 0.0003988575078401198, 'weight_decay': 6.401648975384462e-05, 'beta_0': 0.8580672117948858, 'beta_1': 0.9919060740682576, 'epsilon': 1.2899883629598904e-08, 'balanced_loss': False, 'epochs': 149, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 13:47:09,527] Trial 317 finished with value: 0.9508231485464889 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'mean', 'embedding_pooling_operation': 'min', 'threshold': 0.8349215109620133, 'batch_size': 75, 'attention_heads': 10, 'hidden_dimension': 88, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5776175977710419, 'global_pooling': 'max', 'learning_rate': 0.00026980468441531387, 'weight_decay': 7.509765499730612e-05, 'beta_0': 0.8598409603682262, 'beta_1': 0.9926331635084339, 'epsilon': 1.5179011909787775e-08, 'balanced_loss': False, 'epochs': 152, 'early_stopping_patience': 23, 'plateau_patience': 16, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 14:11:36,500] Trial 318 finished with value: 0.9500767882212603 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8270775290757337, 'batch_size': 71, 'attention_heads': 10, 'hidden_dimension': 117, 'number_of_hidden_layers': 2, 'dropout_rate': 0.594597908875934, 'global_pooling': 'max', 'learning_rate': 0.0005406594861370479, 'weight_decay': 5.5192470338668094e-05, 'beta_0': 0.8565316126005703, 'beta_1': 0.9915733817234957, 'epsilon': 2.6519875526138388e-08, 'balanced_loss': False, 'epochs': 140, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 14:29:44,589] Trial 319 finished with value: 0.9426677998648062 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8415241179890883, 'batch_size': 67, 'attention_heads': 11, 'hidden_dimension': 106, 'number_of_hidden_layers': 2, 'dropout_rate': 0.584212419896849, 'global_pooling': 'max', 'learning_rate': 0.00043275218187720606, 'weight_decay': 3.196554799415632e-05, 'beta_0': 0.8618763012629207, 'beta_1': 0.9923067399901461, 'epsilon': 6.291557731435621e-08, 'balanced_loss': False, 'epochs': 143, 'early_stopping_patience': 23, 'plateau_patience': 22, 'plateau_divider': 2}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.23 GiB. GPU 0 has a total capacity of 44.56 GiB of which 2.63 GiB is free. Including non-PyTorch memory, this process has 41.92 GiB memory in use. Of the allocated memory 40.19 GiB is allocated by PyTorch, and 596.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 14:36:59,433] Trial 320 finished with value: -1.0 and parameters: {'left_stride': 256, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8038656613675177, 'batch_size': 74, 'attention_heads': 11, 'hidden_dimension': 97, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5705898786736991, 'global_pooling': 'max', 'learning_rate': 0.0006123060596883008, 'weight_decay': 8.001823701360805e-06, 'beta_0': 0.8545251954616919, 'beta_1': 0.9912479790273058, 'epsilon': 1.161223319276473e-08, 'balanced_loss': False, 'epochs': 137, 'early_stopping_patience': 25, 'plateau_patience': 23, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
CUDA out of memory. Tried to allocate 3.39 GiB. GPU 0 has a total capacity of 44.56 GiB of which 654.69 MiB is free. Including non-PyTorch memory, this process has 43.91 GiB memory in use. Of the allocated memory 41.90 GiB is allocated by PyTorch, and 888.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[I 2025-02-23 14:43:53,259] Trial 321 finished with value: -1.0 and parameters: {'left_stride': 64, 'right_stride': 64, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'mean', 'threshold': 0.8162051515859154, 'batch_size': 66, 'attention_heads': 12, 'hidden_dimension': 113, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5893985800856151, 'global_pooling': 'max', 'learning_rate': 0.0003524767960007465, 'weight_decay': 4.4059157526421716e-05, 'beta_0': 0.85314412172137, 'beta_1': 0.9928550660532278, 'epsilon': 1.0585124663945148e-08, 'balanced_loss': False, 'epochs': 131, 'early_stopping_patience': 23, 'plateau_patience': 17, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.
[I 2025-02-23 15:03:38,846] Trial 322 finished with value: 0.9510975651245046 and parameters: {'left_stride': 64, 'right_stride': 0, 'attention_pooling_operation': 'max', 'embedding_pooling_operation': 'min', 'threshold': 0.8333670881214965, 'batch_size': 81, 'attention_heads': 10, 'hidden_dimension': 101, 'number_of_hidden_layers': 2, 'dropout_rate': 0.5998566017638592, 'global_pooling': 'max', 'learning_rate': 0.0008992149871600284, 'weight_decay': 8.626963465976662e-05, 'beta_0': 0.864011530883716, 'beta_1': 0.9914540009157515, 'epsilon': 2.0512397306533742e-08, 'balanced_loss': False, 'epochs': 147, 'early_stopping_patience': 23, 'plateau_patience': 18, 'plateau_divider': 3}. Best is trial 36 with value: 0.966858574697526.

[TRIAL] 36 [VALIDATION PERFORMANCE] 0.966858574697526 [TRAINING LOSS] 0.00923021720134582 [VALIDATION LOSS] 0.10200644160310428 

number                                      36
value                                 0.966859
params_threshold                      0.784221
params_attention_heads                      12
params_balanced_loss                     False
params_embedding_pooling_operation        mean
params_attention_pooling_operation         min
params_batch_size                          125
params_dropout_rate                   0.589338
params_early_stopping_patience              23
params_epochs                              161
params_global_pooling                      max
params_hidden_dimension                     53
params_learning_rate                   0.00073
params_number_of_hidden_layers               1
params_plateau_divider                       5
params_plateau_patience                     20
params_weight_decay                   0.000033
params_beta_0                         0.866316
params_beta_1                         0.989715
params_epsilon                        0.000027
user_attrs_epoch                          40.0
user_attrs_training_loss               0.00923
user_attrs_validation_loss            0.102006
params_left_stride                          64
params_right_stride                         32
Name: 36, dtype: object
37 Val: 0.9443999586143679 Test: 0.9452117793407238
38 Val: 0.9536603188838721 Test: 0.9398625011933768
39 Val: 0.9442013384637556 Test: 0.9233642435572791
40 Val: 0.949558111163791 Test: 0.9391107670234313
41 Val: 0.9525650501300333 Test: 0.9364905740968067
42 Val: 0.9579911371791188 Test: 0.9358157765615963
slurmstepd: error: *** JOB 15004848 ON gpu013 CANCELLED AT 2025-02-23T16:29:40 DUE TO TIME LIMIT ***
